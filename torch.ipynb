{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-07-15T09:44:03.725319Z",
     "start_time": "2025-07-15T09:44:03.710887Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sympy import false\n",
    "from sympy.abc import alpha\n",
    "from tensorflow.python.ops.linalg.linear_operator_algebra import inverse\n",
    "\n",
    "from datasetLoader import load_dataset\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "from inverse import fit_linear\n",
    "from kernel_cuttering import kernel\n",
    "from tools import model_tester\n",
    "from tools.reportParser import find_nearest\n",
    "%matplotlib qt"
   ],
   "outputs": [],
   "execution_count": 27
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-15T09:44:04.265670Z",
     "start_time": "2025-07-15T09:44:04.249883Z"
    }
   },
   "cell_type": "code",
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ],
   "id": "d322819b15356399",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "execution_count": 28
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-15T10:12:29.401462Z",
     "start_time": "2025-07-15T10:12:27.473618Z"
    }
   },
   "cell_type": "code",
   "source": [
    "pore_widths = np.load(\"data/initial kernels/new_kernel/pore_sizes.npy\")\n",
    "pressures = np.load(\"data/initial kernels/new_kernel/pressure.npy\")\n",
    "kernel = np.load(\"data/initial kernels/new_kernel/kernel.npy\")\n",
    "with open(\"data/initial kernels/new_kernel/kernel.npy\", 'rb') as f:\n",
    "    data_sorb = np.load(f)\n",
    "\n",
    "x, y = load_dataset('data/datasets/carbon_random_combined.npz')\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.15, random_state=1)\n",
    "\n",
    "x_exp, y_exp = load_dataset('data/datasets/exp.npz')\n",
    "\n",
    "x_train_exp, x_test_exp, y_train_exp, y_test_exp = train_test_split(x_exp, y_exp, test_size=0.15, random_state=1)"
   ],
   "id": "d29956229295484c",
   "outputs": [],
   "execution_count": 46
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-15T15:36:14.176249Z",
     "start_time": "2025-07-15T15:36:12.431783Z"
    }
   },
   "cell_type": "code",
   "source": [
    "figure, axis = plt.subplots(3, 4)\n",
    "for i in range(3):\n",
    "    for j in range(4):\n",
    "        k = np.random.randint(0, len(x_train_exp))\n",
    "        axis[i, j].plot(pressures[:], x_train_exp[k], marker=\".\")\n",
    "        #axis[i, j].set_xscale(\"log\")\n",
    "        ############################\n",
    "        start_i = 0\n",
    "        for l in range(len(x_train_exp[k])):\n",
    "            if x_train_exp[k][l] != 0:\n",
    "                start_i = l\n",
    "                break\n",
    "\n",
    "        end_i = len(x_train_exp[k])\n",
    "        for l in range(len(x_train_exp[k])-1, -1, -1):\n",
    "            if x_train_exp[k][l] != 1:\n",
    "                end_i = l+1\n",
    "                break\n",
    "\n",
    "        pd_test = fit_linear(x_train_exp[k][start_i:end_i], kernel[:, start_i:end_i], 0).x\n",
    "        tw = axis[i, j].twiny()\n",
    "        tw.plot(pore_widths, pd_test*100, color=\"r\", marker=\".\")\n",
    "\n",
    "        # dist = fit_linear(adsorption=isotherm_data[i][start_i:end_i], kernel=kernel[start_i:end_i], alpha=0).x\n",
    "        axis[i, j].plot(pressures[start_i:end_i], np.dot(pd_test, kernel)[start_i:end_i], marker=\".\")\n",
    "        ##############################\n",
    "        axis[i, j].grid()\n",
    "plt.show()"
   ],
   "id": "56cb2ebf581218f4",
   "outputs": [],
   "execution_count": 160
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-15T10:47:30.917725Z",
     "start_time": "2025-07-15T10:47:30.810385Z"
    }
   },
   "cell_type": "code",
   "source": [
    "figure, axis = plt.subplots(3, 4)\n",
    "for i in range(3):\n",
    "    for j in range(4):\n",
    "        k = np.random.randint(0, len(y_train_exp))\n",
    "        axis[i, j].plot(pore_widths, y_train_exp[k], marker=\".\")\n",
    "        axis[i, j].grid()\n",
    "plt.show()"
   ],
   "id": "b92642b436f09b6e",
   "outputs": [],
   "execution_count": 106
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-15T10:47:19.048033Z",
     "start_time": "2025-07-15T10:47:18.937665Z"
    }
   },
   "cell_type": "code",
   "source": [
    "figure, axis = plt.subplots(3, 4)\n",
    "for i in range(3):\n",
    "    for j in range(4):\n",
    "        k = np.random.randint(0, len(x_train))\n",
    "        axis[i, j].plot(pressures, x_train[k], marker=\".\")\n",
    "        axis[i, j].grid()\n",
    "plt.show()"
   ],
   "id": "56fc7f22da2e46d6",
   "outputs": [],
   "execution_count": 105
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-15T10:47:09.550934Z",
     "start_time": "2025-07-15T10:47:09.160936Z"
    }
   },
   "cell_type": "code",
   "source": [
    "figure, axis = plt.subplots(3, 4)\n",
    "for i in range(3):\n",
    "    for j in range(4):\n",
    "        k = np.random.randint(0, len(x_train))\n",
    "        axis[i, j].plot(pore_widths, y_train[k], marker=\".\")\n",
    "        axis[i, j].grid()\n",
    "plt.show()"
   ],
   "id": "15f9ff24438da3f5",
   "outputs": [],
   "execution_count": 104
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-15T10:17:12.325242Z",
     "start_time": "2025-07-15T10:17:12.237239Z"
    }
   },
   "cell_type": "code",
   "source": "plt.plot(pore_widths, sum(y_train), marker=\".\")",
   "id": "d204e5d61104e3ed",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x2d1b6ff3f10>]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 52
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-09T09:49:44.913859Z",
     "start_time": "2025-07-09T09:49:44.898572Z"
    }
   },
   "cell_type": "code",
   "source": [
    "i = np.random.randint(0, len(x_train))\n",
    "plt.plot(pressures[:-10], x_train[i], marker=\".\")\n",
    "plt.grid()\n",
    "plt.show()"
   ],
   "id": "3201b1f78762f4ff",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-15T10:17:30.579655Z",
     "start_time": "2025-07-15T10:17:30.437138Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class IsothermDataset(Dataset):\n",
    "    def __init__(self, isotherms, transform=None):\n",
    "        self.data = torch.tensor(isotherms, dtype=torch.float32).to(device)\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = self.data[idx]\n",
    "        if self.transform:\n",
    "            x = self.transform(x)\n",
    "        return x, x\n",
    "\n",
    "x_mixed_train = np.concatenate((x_train_exp, x_train))\n",
    "x_mixed_test = np.concatenate((x_test_exp, x_test))\n",
    "\n",
    "# dataset = IsothermDataset(np.concatenate((x_train_exp, x_train_exp)))\n",
    "# dataset_test = IsothermDataset(np.concatenate((x_test_exp, x_test_exp)))\n",
    "dataset = IsothermDataset(np.concatenate((x_mixed_train, x_mixed_train)))\n",
    "dataset_test = IsothermDataset(np.concatenate((x_mixed_test, x_mixed_test)))\n",
    "\n",
    "\n",
    "batch_size = 512\n",
    "loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "loader_test = DataLoader(dataset_test, batch_size=batch_size, shuffle=False)"
   ],
   "id": "5cefc1b65fc7fa63",
   "outputs": [],
   "execution_count": 54
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-15T10:18:15.209227Z",
     "start_time": "2025-07-15T10:18:15.193552Z"
    }
   },
   "cell_type": "code",
   "source": "y_exp[0].size",
   "id": "565056807e7f73b2",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "117"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 58
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-15T10:18:51.144820Z",
     "start_time": "2025-07-15T10:18:51.129068Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self, input_dim, latent_dim):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, latent_dim)\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(latent_dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, input_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        z = self.encoder(x)\n",
    "        x_recon = self.decoder(z)\n",
    "        return x_recon, z\n",
    "\n",
    "input_dim = 195\n",
    "latent_dim = 16\n",
    "epochs = 200\n",
    "learning_rate = 1e-3\n",
    "\n",
    "model = Autoencoder(input_dim=input_dim, latent_dim=latent_dim)\n",
    "model.to(device)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "def train_autoencoder(model, loader, loader_test):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    total_vloss = 0\n",
    "    for x, _ in loader:\n",
    "        optimizer.zero_grad()\n",
    "        x_recon, _ = model(x)\n",
    "        loss = criterion(x_recon, x)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for x, _ in loader_test:\n",
    "            x_recon, _  = model(x)\n",
    "            vloss = criterion(x_recon, x)\n",
    "            total_vloss += vloss.item()\n",
    "\n",
    "    return total_loss / len(loader.dataset), total_vloss / len(loader_test.dataset)\n",
    "\n",
    "\n",
    "# sample_z = model.encoder(torch.tensor(isotherms_np[0], dtype=torch.float32))"
   ],
   "id": "c960463d00766e7c",
   "outputs": [],
   "execution_count": 59
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-15T10:23:57.709201Z",
     "start_time": "2025-07-15T10:18:52.522872Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for epoch in range(1, epochs+1):\n",
    "    loss, vloss = train_autoencoder(model, loader,loader_test)\n",
    "    if epoch % 1 == 0:\n",
    "        print(f\"Epoch {epoch}/{epochs}, Loss: {loss*100:.8f} Test loss: {vloss*100:.8f}\")"
   ],
   "id": "6f625ed77c7b7164",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200, Loss: 0.00338765 Test loss: 0.00017241\n",
      "Epoch 2/200, Loss: 0.00010255 Test loss: 0.00007115\n",
      "Epoch 3/200, Loss: 0.00005622 Test loss: 0.00004804\n",
      "Epoch 4/200, Loss: 0.00004330 Test loss: 0.00004017\n",
      "Epoch 5/200, Loss: 0.00003756 Test loss: 0.00003666\n",
      "Epoch 6/200, Loss: 0.00003412 Test loss: 0.00003299\n",
      "Epoch 7/200, Loss: 0.00003211 Test loss: 0.00003704\n",
      "Epoch 8/200, Loss: 0.00002956 Test loss: 0.00002989\n",
      "Epoch 9/200, Loss: 0.00002824 Test loss: 0.00002636\n",
      "Epoch 10/200, Loss: 0.00002659 Test loss: 0.00002719\n",
      "Epoch 11/200, Loss: 0.00002394 Test loss: 0.00002247\n",
      "Epoch 12/200, Loss: 0.00002145 Test loss: 0.00002125\n",
      "Epoch 13/200, Loss: 0.00001937 Test loss: 0.00001698\n",
      "Epoch 14/200, Loss: 0.00001680 Test loss: 0.00001493\n",
      "Epoch 15/200, Loss: 0.00001626 Test loss: 0.00001651\n",
      "Epoch 16/200, Loss: 0.00001547 Test loss: 0.00001342\n",
      "Epoch 17/200, Loss: 0.00001456 Test loss: 0.00001364\n",
      "Epoch 18/200, Loss: 0.00001437 Test loss: 0.00001248\n",
      "Epoch 19/200, Loss: 0.00001371 Test loss: 0.00002652\n",
      "Epoch 20/200, Loss: 0.00001333 Test loss: 0.00002380\n",
      "Epoch 21/200, Loss: 0.00001366 Test loss: 0.00001358\n",
      "Epoch 22/200, Loss: 0.00001229 Test loss: 0.00001143\n",
      "Epoch 23/200, Loss: 0.00001177 Test loss: 0.00001651\n",
      "Epoch 24/200, Loss: 0.00001185 Test loss: 0.00001471\n",
      "Epoch 25/200, Loss: 0.00001110 Test loss: 0.00001088\n",
      "Epoch 26/200, Loss: 0.00001126 Test loss: 0.00001010\n",
      "Epoch 27/200, Loss: 0.00000994 Test loss: 0.00001009\n",
      "Epoch 28/200, Loss: 0.00000951 Test loss: 0.00000875\n",
      "Epoch 29/200, Loss: 0.00000971 Test loss: 0.00000834\n",
      "Epoch 30/200, Loss: 0.00000859 Test loss: 0.00000799\n",
      "Epoch 31/200, Loss: 0.00000866 Test loss: 0.00000842\n",
      "Epoch 32/200, Loss: 0.00000793 Test loss: 0.00000661\n",
      "Epoch 33/200, Loss: 0.00000812 Test loss: 0.00000612\n",
      "Epoch 34/200, Loss: 0.00000747 Test loss: 0.00000778\n",
      "Epoch 35/200, Loss: 0.00000751 Test loss: 0.00000797\n",
      "Epoch 36/200, Loss: 0.00000723 Test loss: 0.00000733\n",
      "Epoch 37/200, Loss: 0.00000706 Test loss: 0.00000751\n",
      "Epoch 38/200, Loss: 0.00000684 Test loss: 0.00000596\n",
      "Epoch 39/200, Loss: 0.00000690 Test loss: 0.00000532\n",
      "Epoch 40/200, Loss: 0.00000640 Test loss: 0.00000606\n",
      "Epoch 41/200, Loss: 0.00000632 Test loss: 0.00000551\n",
      "Epoch 42/200, Loss: 0.00000647 Test loss: 0.00000654\n",
      "Epoch 43/200, Loss: 0.00000615 Test loss: 0.00000573\n",
      "Epoch 44/200, Loss: 0.00000613 Test loss: 0.00000604\n",
      "Epoch 45/200, Loss: 0.00000581 Test loss: 0.00000501\n",
      "Epoch 46/200, Loss: 0.00000706 Test loss: 0.00000451\n",
      "Epoch 47/200, Loss: 0.00000562 Test loss: 0.00000860\n",
      "Epoch 48/200, Loss: 0.00000561 Test loss: 0.00000578\n",
      "Epoch 49/200, Loss: 0.00000579 Test loss: 0.00000546\n",
      "Epoch 50/200, Loss: 0.00000531 Test loss: 0.00000509\n",
      "Epoch 51/200, Loss: 0.00000572 Test loss: 0.00000617\n",
      "Epoch 52/200, Loss: 0.00000509 Test loss: 0.00000981\n",
      "Epoch 53/200, Loss: 0.00000589 Test loss: 0.00000367\n",
      "Epoch 54/200, Loss: 0.00000533 Test loss: 0.00000455\n",
      "Epoch 55/200, Loss: 0.00000477 Test loss: 0.00000720\n",
      "Epoch 56/200, Loss: 0.00000521 Test loss: 0.00000387\n",
      "Epoch 57/200, Loss: 0.00000506 Test loss: 0.00000391\n",
      "Epoch 58/200, Loss: 0.00000467 Test loss: 0.00000410\n",
      "Epoch 59/200, Loss: 0.00000490 Test loss: 0.00000382\n",
      "Epoch 60/200, Loss: 0.00000496 Test loss: 0.00000568\n",
      "Epoch 61/200, Loss: 0.00000469 Test loss: 0.00000337\n",
      "Epoch 62/200, Loss: 0.00000459 Test loss: 0.00000408\n",
      "Epoch 63/200, Loss: 0.00000452 Test loss: 0.00000508\n",
      "Epoch 64/200, Loss: 0.00000469 Test loss: 0.00000445\n",
      "Epoch 65/200, Loss: 0.00000437 Test loss: 0.00000375\n",
      "Epoch 66/200, Loss: 0.00000456 Test loss: 0.00000357\n",
      "Epoch 67/200, Loss: 0.00000437 Test loss: 0.00000320\n",
      "Epoch 68/200, Loss: 0.00000433 Test loss: 0.00000400\n",
      "Epoch 69/200, Loss: 0.00000467 Test loss: 0.00000910\n",
      "Epoch 70/200, Loss: 0.00000422 Test loss: 0.00000372\n",
      "Epoch 71/200, Loss: 0.00000412 Test loss: 0.00000347\n",
      "Epoch 72/200, Loss: 0.00000410 Test loss: 0.00000331\n",
      "Epoch 73/200, Loss: 0.00000407 Test loss: 0.00000361\n",
      "Epoch 74/200, Loss: 0.00000423 Test loss: 0.00000643\n",
      "Epoch 75/200, Loss: 0.00000403 Test loss: 0.00000367\n",
      "Epoch 76/200, Loss: 0.00000361 Test loss: 0.00000441\n",
      "Epoch 77/200, Loss: 0.00000419 Test loss: 0.00000491\n",
      "Epoch 78/200, Loss: 0.00000379 Test loss: 0.00000550\n",
      "Epoch 79/200, Loss: 0.00000399 Test loss: 0.00000459\n",
      "Epoch 80/200, Loss: 0.00000378 Test loss: 0.00000332\n",
      "Epoch 81/200, Loss: 0.00000378 Test loss: 0.00000435\n",
      "Epoch 82/200, Loss: 0.00000364 Test loss: 0.00000332\n",
      "Epoch 83/200, Loss: 0.00000387 Test loss: 0.00000267\n",
      "Epoch 84/200, Loss: 0.00000364 Test loss: 0.00000266\n",
      "Epoch 85/200, Loss: 0.00000368 Test loss: 0.00000269\n",
      "Epoch 86/200, Loss: 0.00000343 Test loss: 0.00000301\n",
      "Epoch 87/200, Loss: 0.00000347 Test loss: 0.00000502\n",
      "Epoch 88/200, Loss: 0.00000353 Test loss: 0.00000417\n",
      "Epoch 89/200, Loss: 0.00000343 Test loss: 0.00000444\n",
      "Epoch 90/200, Loss: 0.00000313 Test loss: 0.00000278\n",
      "Epoch 91/200, Loss: 0.00000364 Test loss: 0.00000441\n",
      "Epoch 92/200, Loss: 0.00000314 Test loss: 0.00000326\n",
      "Epoch 93/200, Loss: 0.00000321 Test loss: 0.00000260\n",
      "Epoch 94/200, Loss: 0.00000299 Test loss: 0.00000527\n",
      "Epoch 95/200, Loss: 0.00000328 Test loss: 0.00000287\n",
      "Epoch 96/200, Loss: 0.00000296 Test loss: 0.00000337\n",
      "Epoch 97/200, Loss: 0.00000323 Test loss: 0.00000244\n",
      "Epoch 98/200, Loss: 0.00000299 Test loss: 0.00000287\n",
      "Epoch 99/200, Loss: 0.00000310 Test loss: 0.00000328\n",
      "Epoch 100/200, Loss: 0.00000290 Test loss: 0.00000217\n",
      "Epoch 101/200, Loss: 0.00000338 Test loss: 0.00001037\n",
      "Epoch 102/200, Loss: 0.00000280 Test loss: 0.00000467\n",
      "Epoch 103/200, Loss: 0.00000290 Test loss: 0.00000330\n",
      "Epoch 104/200, Loss: 0.00000280 Test loss: 0.00000291\n",
      "Epoch 105/200, Loss: 0.00000286 Test loss: 0.00000186\n",
      "Epoch 106/200, Loss: 0.00000273 Test loss: 0.00000199\n",
      "Epoch 107/200, Loss: 0.00000291 Test loss: 0.00000355\n",
      "Epoch 108/200, Loss: 0.00000270 Test loss: 0.00000572\n",
      "Epoch 109/200, Loss: 0.00000284 Test loss: 0.00000218\n",
      "Epoch 110/200, Loss: 0.00000270 Test loss: 0.00000573\n",
      "Epoch 111/200, Loss: 0.00000285 Test loss: 0.00000532\n",
      "Epoch 112/200, Loss: 0.00000264 Test loss: 0.00001325\n",
      "Epoch 113/200, Loss: 0.00000268 Test loss: 0.00000363\n",
      "Epoch 114/200, Loss: 0.00000333 Test loss: 0.00000268\n",
      "Epoch 115/200, Loss: 0.00000211 Test loss: 0.00000252\n",
      "Epoch 116/200, Loss: 0.00000262 Test loss: 0.00000181\n",
      "Epoch 117/200, Loss: 0.00000264 Test loss: 0.00000560\n",
      "Epoch 118/200, Loss: 0.00000254 Test loss: 0.00000217\n",
      "Epoch 119/200, Loss: 0.00000255 Test loss: 0.00000385\n",
      "Epoch 120/200, Loss: 0.00000265 Test loss: 0.00000181\n",
      "Epoch 121/200, Loss: 0.00000246 Test loss: 0.00000213\n",
      "Epoch 122/200, Loss: 0.00000254 Test loss: 0.00000160\n",
      "Epoch 123/200, Loss: 0.00000247 Test loss: 0.00000233\n",
      "Epoch 124/200, Loss: 0.00000247 Test loss: 0.00000265\n",
      "Epoch 125/200, Loss: 0.00000245 Test loss: 0.00000207\n",
      "Epoch 126/200, Loss: 0.00000269 Test loss: 0.00000336\n",
      "Epoch 127/200, Loss: 0.00000252 Test loss: 0.00000263\n",
      "Epoch 128/200, Loss: 0.00000222 Test loss: 0.00000226\n",
      "Epoch 129/200, Loss: 0.00000235 Test loss: 0.00000254\n",
      "Epoch 130/200, Loss: 0.00000246 Test loss: 0.00000785\n",
      "Epoch 131/200, Loss: 0.00000257 Test loss: 0.00000183\n",
      "Epoch 132/200, Loss: 0.00000227 Test loss: 0.00000160\n",
      "Epoch 133/200, Loss: 0.00000230 Test loss: 0.00000221\n",
      "Epoch 134/200, Loss: 0.00000256 Test loss: 0.00000151\n",
      "Epoch 135/200, Loss: 0.00000220 Test loss: 0.00000275\n",
      "Epoch 136/200, Loss: 0.00000236 Test loss: 0.00000191\n",
      "Epoch 137/200, Loss: 0.00000228 Test loss: 0.00000300\n",
      "Epoch 138/200, Loss: 0.00000248 Test loss: 0.00000356\n",
      "Epoch 139/200, Loss: 0.00000222 Test loss: 0.00000741\n",
      "Epoch 140/200, Loss: 0.00000226 Test loss: 0.00000156\n",
      "Epoch 141/200, Loss: 0.00000228 Test loss: 0.00000171\n",
      "Epoch 142/200, Loss: 0.00000223 Test loss: 0.00000157\n",
      "Epoch 143/200, Loss: 0.00000229 Test loss: 0.00000156\n",
      "Epoch 144/200, Loss: 0.00000217 Test loss: 0.00000651\n",
      "Epoch 145/200, Loss: 0.00000225 Test loss: 0.00000183\n",
      "Epoch 146/200, Loss: 0.00000210 Test loss: 0.00000186\n",
      "Epoch 147/200, Loss: 0.00000224 Test loss: 0.00000437\n",
      "Epoch 148/200, Loss: 0.00000235 Test loss: 0.00000271\n",
      "Epoch 149/200, Loss: 0.00000218 Test loss: 0.00000679\n",
      "Epoch 150/200, Loss: 0.00000228 Test loss: 0.00000178\n",
      "Epoch 151/200, Loss: 0.00000222 Test loss: 0.00000187\n",
      "Epoch 152/200, Loss: 0.00000212 Test loss: 0.00000253\n",
      "Epoch 153/200, Loss: 0.00000215 Test loss: 0.00000170\n",
      "Epoch 154/200, Loss: 0.00000199 Test loss: 0.00000157\n",
      "Epoch 155/200, Loss: 0.00000212 Test loss: 0.00000175\n",
      "Epoch 156/200, Loss: 0.00000213 Test loss: 0.00000195\n",
      "Epoch 157/200, Loss: 0.00000208 Test loss: 0.00000165\n",
      "Epoch 158/200, Loss: 0.00000201 Test loss: 0.00000182\n",
      "Epoch 159/200, Loss: 0.00000211 Test loss: 0.00000263\n",
      "Epoch 160/200, Loss: 0.00000197 Test loss: 0.00000841\n",
      "Epoch 161/200, Loss: 0.00000366 Test loss: 0.00000135\n",
      "Epoch 162/200, Loss: 0.00000187 Test loss: 0.00000142\n",
      "Epoch 163/200, Loss: 0.00000217 Test loss: 0.00000169\n",
      "Epoch 164/200, Loss: 0.00000201 Test loss: 0.00000181\n",
      "Epoch 165/200, Loss: 0.00000192 Test loss: 0.00000235\n",
      "Epoch 166/200, Loss: 0.00000217 Test loss: 0.00000162\n",
      "Epoch 167/200, Loss: 0.00000184 Test loss: 0.00000167\n",
      "Epoch 168/200, Loss: 0.00000210 Test loss: 0.00000124\n",
      "Epoch 169/200, Loss: 0.00000203 Test loss: 0.00000123\n",
      "Epoch 170/200, Loss: 0.00000188 Test loss: 0.00000156\n",
      "Epoch 171/200, Loss: 0.00000200 Test loss: 0.00000179\n",
      "Epoch 172/200, Loss: 0.00000193 Test loss: 0.00000172\n",
      "Epoch 173/200, Loss: 0.00000216 Test loss: 0.00000182\n",
      "Epoch 174/200, Loss: 0.00000183 Test loss: 0.00000329\n",
      "Epoch 175/200, Loss: 0.00000196 Test loss: 0.00000146\n",
      "Epoch 176/200, Loss: 0.00000195 Test loss: 0.00000159\n",
      "Epoch 177/200, Loss: 0.00000175 Test loss: 0.00000127\n",
      "Epoch 178/200, Loss: 0.00000469 Test loss: 0.00000158\n",
      "Epoch 179/200, Loss: 0.00000130 Test loss: 0.00000151\n",
      "Epoch 180/200, Loss: 0.00000168 Test loss: 0.00000145\n",
      "Epoch 181/200, Loss: 0.00000191 Test loss: 0.00000125\n",
      "Epoch 182/200, Loss: 0.00000189 Test loss: 0.00000209\n",
      "Epoch 183/200, Loss: 0.00000179 Test loss: 0.00000136\n",
      "Epoch 184/200, Loss: 0.00000193 Test loss: 0.00000140\n",
      "Epoch 185/200, Loss: 0.00000197 Test loss: 0.00000156\n",
      "Epoch 186/200, Loss: 0.00000170 Test loss: 0.00000465\n",
      "Epoch 187/200, Loss: 0.00000195 Test loss: 0.00000193\n",
      "Epoch 188/200, Loss: 0.00000371 Test loss: 0.00000207\n",
      "Epoch 189/200, Loss: 0.00000136 Test loss: 0.00000127\n",
      "Epoch 190/200, Loss: 0.00000167 Test loss: 0.00000160\n",
      "Epoch 191/200, Loss: 0.00000180 Test loss: 0.00000159\n",
      "Epoch 192/200, Loss: 0.00000176 Test loss: 0.00000167\n",
      "Epoch 193/200, Loss: 0.00000182 Test loss: 0.00000295\n",
      "Epoch 194/200, Loss: 0.00000178 Test loss: 0.00000207\n",
      "Epoch 195/200, Loss: 0.00000177 Test loss: 0.00000213\n",
      "Epoch 196/200, Loss: 0.00000185 Test loss: 0.00000117\n",
      "Epoch 197/200, Loss: 0.00000182 Test loss: 0.00000231\n",
      "Epoch 198/200, Loss: 0.00000176 Test loss: 0.00000140\n",
      "Epoch 199/200, Loss: 0.00000173 Test loss: 0.00000153\n",
      "Epoch 200/200, Loss: 0.00000171 Test loss: 0.00000305\n"
     ]
    }
   ],
   "execution_count": 60
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-10T08:55:37.488845Z",
     "start_time": "2025-07-10T08:55:37.470852Z"
    }
   },
   "cell_type": "code",
   "source": "torch.save(model, \"data/models/torch/autoencoder_exp.pt\")",
   "id": "6e5a9e294045c74c",
   "outputs": [],
   "execution_count": 39
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-10T08:55:37.776910Z",
     "start_time": "2025-07-10T08:55:37.746207Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model = torch.load(\"data/models/torch/autoencoder_exp.pt\", weights_only=False)\n",
    "model.eval()"
   ],
   "id": "f8af0cb897395d7c",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Autoencoder(\n",
       "  (encoder): Sequential(\n",
       "    (0): Linear(in_features=448, out_features=128, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=128, out_features=64, bias=True)\n",
       "    (3): ReLU()\n",
       "    (4): Linear(in_features=64, out_features=16, bias=True)\n",
       "  )\n",
       "  (decoder): Sequential(\n",
       "    (0): Linear(in_features=16, out_features=64, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=64, out_features=128, bias=True)\n",
       "    (3): ReLU()\n",
       "    (4): Linear(in_features=128, out_features=448, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 40
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-15T10:38:29.723781Z",
     "start_time": "2025-07-15T10:38:29.660816Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model.eval()\n",
    "latent_vectors_train = model.encoder(torch.tensor(x_train, dtype=torch.float32).to(device)).detach().cpu().numpy()\n",
    "latent_vectors_test = model.encoder(torch.tensor(x_test, dtype=torch.float32).to(device)).detach().cpu().numpy()\n",
    "latent_vectors_test_exp = model.encoder(torch.tensor(x_test_exp, dtype=torch.float32).to(device)).detach().cpu().numpy()"
   ],
   "id": "59307777a8d5284",
   "outputs": [],
   "execution_count": 91
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-15T10:38:30.493166Z",
     "start_time": "2025-07-15T10:38:30.487168Z"
    }
   },
   "cell_type": "code",
   "source": "decoded = model.decoder(model.encoder(torch.tensor(x_test_exp, dtype=torch.float32).to(device))).detach().cpu().numpy()",
   "id": "a95fe3351e57df09",
   "outputs": [],
   "execution_count": 92
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-15T10:38:30.959736Z",
     "start_time": "2025-07-15T10:38:30.849674Z"
    }
   },
   "cell_type": "code",
   "source": [
    "figure, axis = plt.subplots(3, 3)\n",
    "for i in range(3):\n",
    "    for j in range(3):\n",
    "        k=np.random.randint(0, len(decoded))\n",
    "        axis[i, j].plot(pressures[:], x_test_exp[k], marker=\".\", label = \"origin\")\n",
    "        axis[i, j].plot(pressures[:], decoded[k], marker=\".\", label = \"decoded\")\n",
    "        axis[i, j].grid(True)\n",
    "axis[i, j].legend()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# k=np.random.randint(0, len(decoded))\n",
    "# plt.plot(pressures[:-10], x_test_exp[k], marker=\".\", label = \"origin\")\n",
    "# plt.plot(pressures[:-10], decoded[k], marker=\".\", label = \"decoded\")\n",
    "# plt.legend()\n",
    "# plt.grid(True)\n",
    "# plt.show()"
   ],
   "id": "dbe5444885056917",
   "outputs": [],
   "execution_count": 93
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-09T09:49:59.729171Z",
     "start_time": "2025-07-09T09:49:59.069100Z"
    }
   },
   "cell_type": "code",
   "source": [
    "np.random.seed(0)\n",
    "labels = None \n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "latent_pca = pca.fit_transform(latent_vectors_train[:100])\n",
    "latent_pca_exp = pca.fit_transform(latent_vectors_test[:100])\n",
    "\n",
    "tsne = TSNE(n_components=2, init='pca', random_state=0)\n",
    "latent_tsne = tsne.fit_transform(latent_vectors_train[:100])\n",
    "latent_tsne_exp = tsne.fit_transform(latent_vectors_test[:100])\n",
    "\n",
    "plt.figure()\n",
    "plt.scatter(latent_pca[:, 0], latent_pca[:, 1], label=\"train\")\n",
    "plt.scatter(latent_pca_exp[:, 0], latent_pca_exp[:, 1], label=\"exp\")\n",
    "plt.title(\"PCA of Latent Space\")\n",
    "plt.xlabel(\"PC1\")\n",
    "plt.ylabel(\"PC2\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "plt.scatter(latent_tsne[:, 0], latent_tsne[:, 1], label=\"train\")\n",
    "plt.scatter(latent_tsne_exp[:, 0], latent_tsne_exp[:, 1], label=\"exp\")\n",
    "for i in range(latent_tsne_exp.shape[0]):\n",
    "        plt.text(latent_tsne_exp[i, 0], latent_tsne_exp[i, 1], str(i), fontsize=8, ha='center', va='center')\n",
    "plt.title(\"t-SNE of Latent Space\")\n",
    "plt.xlabel(\"Dim 1\")\n",
    "plt.ylabel(\"Dim 2\")\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "id": "f578073be61beebe",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\anton\\PycharmProjects\\isotherm\\.venv\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:810: FutureWarning: The default learning rate in TSNE will change from 200.0 to 'auto' in 1.2.\n",
      "  warnings.warn(\n",
      "C:\\Users\\anton\\PycharmProjects\\isotherm\\.venv\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:996: FutureWarning: The PCA initialization in TSNE will change to have the standard deviation of PC1 equal to 1e-4 in 1.2. This will ensure better convergence.\n",
      "  warnings.warn(\n",
      "C:\\Users\\anton\\PycharmProjects\\isotherm\\.venv\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:810: FutureWarning: The default learning rate in TSNE will change from 200.0 to 'auto' in 1.2.\n",
      "  warnings.warn(\n",
      "C:\\Users\\anton\\PycharmProjects\\isotherm\\.venv\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:996: FutureWarning: The PCA initialization in TSNE will change to have the standard deviation of PC1 equal to 1e-4 in 1.2. This will ensure better convergence.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-15T14:47:37.931399Z",
     "start_time": "2025-07-15T14:47:37.911440Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def find_exact_idx_last(array, value):\n",
    "    flag = False\n",
    "    for i in range(len(array)):\n",
    "        if array[i] == value:\n",
    "            flag = True\n",
    "        if flag and array[i] != value:\n",
    "            return i\n",
    "def find_alpha(x_exp):\n",
    "    def triangle_method(log_residuals, log_solution_norms):\n",
    "        points = np.column_stack((log_residuals, log_solution_norms))\n",
    "        A = points[0]\n",
    "        B = points[-1]\n",
    "        AB = B - A\n",
    "        AB_norm = np.linalg.norm(AB)\n",
    "        dist = []\n",
    "        for i in range(0, len(points)):\n",
    "            P = points[i]\n",
    "            AP = P - A\n",
    "            dist.append(np.linalg.norm(np.cross(AB, AP)) / AB_norm)\n",
    "    \n",
    "        return dist\n",
    "    def calculate_roughness(psd, ord=2):\n",
    "        return np.linalg.norm(psd, ord=ord)\n",
    "\n",
    "    alpha_list = np.logspace(-3, 4, 40)\n",
    "    error_lst = []\n",
    "    roughness_lst = []\n",
    "    restored_isotherms = []\n",
    "    for alpha in alpha_list:\n",
    "        start_idx = find_exact_idx_last(x_exp, 0)\n",
    "        y = fit_linear(x_exp[start_idx:], kernel=data_sorb[:, start_idx:], alpha=alpha).x\n",
    "        restored_isotherm = np.dot(y, data_sorb)\n",
    "        restored_isotherms.append(restored_isotherm)\n",
    "        error_lst.append(np.linalg.norm((x_exp[start_idx:] - restored_isotherm[start_idx:]), ord=2))\n",
    "        roughness_lst.append(calculate_roughness(y))\n",
    "\n",
    "    dist = triangle_method(np.log(error_lst), np.log(roughness_lst))\n",
    "    alpha = alpha_list[np.argmax(dist)]\n",
    "    return alpha\n",
    "\n",
    "\n",
    "\n",
    "def plot_preds(x, y, preds): \n",
    "    NX, NY = 3, 4\n",
    "    figure, axis = plt.subplots(NX, NY)\n",
    "    for i in range(NX):\n",
    "        for j in range(NY):\n",
    "            \n",
    "            low_p = False\n",
    "            while low_p == False:\n",
    "                k = np.random.randint(0, len(preds))\n",
    "                for l in range(len(x[k])):\n",
    "                    if x[k][l]!=0:\n",
    "                        if pressures[l] < 1e-2:\n",
    "                            low_p = True\n",
    "                        break\n",
    "            \n",
    "            iso_axis = axis[i, j].twiny()\n",
    "            iso_axis.set_xlabel(\"P/P$^0$\",fontsize=8)\n",
    "            iso_axis.plot(pressures[:], x[k], label=\"Isotherm\", color = 'green')\n",
    "            kernel = (data_sorb.T[:])\n",
    "            iso_axis.plot(pressures[:], np.dot(kernel, preds[k][:]), label=\"Isotherm by model\", color=\"red\")\n",
    "            axis[i, j].set_title(f\"№ {k}\")\n",
    "            axis[i, j].title.set_size(10)\n",
    "            axis[i, j].grid()\n",
    "            axis[i, j].set_xlabel(\"nm\",fontsize=8)\n",
    "            axis[i, j].plot(pore_widths, (preds[k]), marker=\".\", label=f\"Model PSD\")\n",
    "            #axis[i, j].plot(pore_widths, y[k], marker=\".\", label=\"PSD\")\n",
    "            alpha = find_alpha(x[k])\n",
    "            start_idx = find_exact_idx_last(x[k], 0)\n",
    "            L_curve = fit_linear(x[k][start_idx:], kernel=data_sorb[:, start_idx:], alpha=alpha).x\n",
    "            axis[i, j].plot(pore_widths, L_curve, marker=\".\", label=\"L_curve\")\n",
    "            iso_axis.plot(pressures[:], np.dot(kernel, L_curve), label=\"Isotherm by L_curve\", color=\"yellow\")\n",
    "    plt.subplots_adjust(hspace=0.6, right=0.95, left=0.05, bottom=0.05, top=0.9)\n",
    "    plt.legend()\n",
    "    axis[0, 0].legend()\n",
    "    plt.show()"
   ],
   "id": "a07cbe66d234c80f",
   "outputs": [],
   "execution_count": 154
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-11T07:54:34.696430Z",
     "start_time": "2025-06-11T07:54:34.349935Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from tools import model_tester\n",
    "from inverse import fit_linear\n",
    "\n",
    "error_lst, roughness_lst = model_tester.test_model_predictions(preds, x_test_exp, kernel=data_sorb[:, :-10])\n",
    "kde_x, kde_error, kde_fun = model_tester.calculate_kde_data(error_lst, stop=150)\n",
    "print(\"average error:\", np.mean(error_lst))\n",
    "plt.plot(kde_x, kde_error, label=model_name)\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.plot()"
   ],
   "id": "1316a794fbe60afa",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average error: 19.576227837865584\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 56
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-15T10:24:59.578539Z",
     "start_time": "2025-07-15T10:24:59.572538Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class DynamicWeightAveraging:\n",
    "    def __init__(self, num_tasks, T=2.0):\n",
    "        self.num_tasks = num_tasks\n",
    "        self.T = T\n",
    "        self.loss_history = []  # список списков: [ [L1_1, L1_2, ...], [L2_1, L2_2, ...], ... ]\n",
    "\n",
    "    def update_weights(self):\n",
    "        if len(self.loss_history[0]) < 2:\n",
    "            return np.ones(self.num_tasks) / self.num_tasks\n",
    "\n",
    "        r = []\n",
    "        for i in range(self.num_tasks):\n",
    "            li = self.loss_history[i]\n",
    "            r_i = li[-1] / (li[-2] + 1e-8)\n",
    "            r.append(r_i)\n",
    "\n",
    "        r = np.array(r)\n",
    "        weights = self.T * np.exp(r / self.T)\n",
    "        weights /= weights.sum()\n",
    "        return weights\n",
    "\n",
    "    def append_losses(self, losses):  # losses — список текущих значений потерь [L1, L2, ...]\n",
    "        if not self.loss_history:\n",
    "            self.loss_history = [[] for _ in range(len(losses))]\n",
    "        for i, l in enumerate(losses):\n",
    "            self.loss_history[i].append(l)\n",
    "dwa = DynamicWeightAveraging(num_tasks=2)"
   ],
   "id": "8a3f50c19ae75247",
   "outputs": [],
   "execution_count": 65
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-15T10:25:00.389497Z",
     "start_time": "2025-07-15T10:25:00.356974Z"
    }
   },
   "cell_type": "code",
   "source": "dwa.update_weights()",
   "id": "64fc19dc8691f1e",
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mIndexError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[66], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m \u001B[43mdwa\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mupdate_weights\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[1;32mIn[65], line 8\u001B[0m, in \u001B[0;36mDynamicWeightAveraging.update_weights\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m      7\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21mupdate_weights\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[1;32m----> 8\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mloss_history\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m]\u001B[49m) \u001B[38;5;241m<\u001B[39m \u001B[38;5;241m2\u001B[39m:\n\u001B[0;32m      9\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m np\u001B[38;5;241m.\u001B[39mones(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mnum_tasks) \u001B[38;5;241m/\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mnum_tasks\n\u001B[0;32m     11\u001B[0m     r \u001B[38;5;241m=\u001B[39m []\n",
      "\u001B[1;31mIndexError\u001B[0m: list index out of range"
     ]
    }
   ],
   "execution_count": 66
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-15T10:26:00.957383Z",
     "start_time": "2025-07-15T10:26:00.952864Z"
    }
   },
   "cell_type": "code",
   "source": "x_exp[0].size, y_exp[0].size",
   "id": "ad563994bc77112b",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(195, 117)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 70
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-15T10:40:55.582663Z",
     "start_time": "2025-07-15T10:40:55.554346Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class PSD_model(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(PSD_model, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_dim, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(32, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(64, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(128, output_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        psd = self.model(x)\n",
    "        return psd\n",
    "\n",
    "class Isotherm_PSD_Dataset(Dataset):\n",
    "    def __init__(self, x, y, original_x, transform=None):\n",
    "        self.x = torch.tensor(x, dtype=torch.float32).to(device)\n",
    "        self.y = torch.tensor(y, dtype=torch.float32).to(device)\n",
    "        self.original_x = torch.tensor(original_x, dtype=torch.float32).to(device)\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = self.x[idx]\n",
    "        y = self.y[idx]\n",
    "        original_x = self.original_x[idx]\n",
    "        if self.transform:\n",
    "            x = self.transform(x)\n",
    "        return x, y, original_x\n",
    "\n",
    "train_PSD = Isotherm_PSD_Dataset(latent_vectors_train, (y_train), x_train)\n",
    "test_PSD = Isotherm_PSD_Dataset(latent_vectors_test, (y_test), x_test)\n",
    "\n",
    "batch_size = 512\n",
    "PSD_loader = DataLoader(train_PSD, batch_size=batch_size, shuffle=True)\n",
    "PSD_loader_test = DataLoader(test_PSD, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "\n",
    "model_PSD = PSD_model(input_dim=latent_dim, output_dim=117)\n",
    "model_PSD.to(device)\n",
    "\n",
    "optimizer = optim.Adam(model_PSD.parameters(), lr=learning_rate)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "\n",
    "with open(\"data/initial kernels/new_kernel/kernel.npy\", 'rb') as f:\n",
    "    data_sorb_torch = torch.tensor(np.load(f))\n",
    "    data_sorb_torch = data_sorb_torch.to(torch.float32).to(device)\n",
    "\n",
    "def isoterm_loss(predicted_y, x):\n",
    "    restored_isotherm = torch.matmul(predicted_y, data_sorb_torch)\n",
    "    loss = torch.mean((x - restored_isotherm) ** 2)\n",
    "    return loss\n",
    "\n",
    "def train_PSD_model(model, loader, loader_test):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    total_vloss = 0\n",
    "    for x, y, original_x in loader:\n",
    "        optimizer.zero_grad()\n",
    "        y_recon = model(x)\n",
    "        loss = criterion(y_recon, y)\n",
    "        # iso_loss = isoterm_loss(y_recon, original_x)\n",
    "        # dwa.append_losses([loss.item(), iso_loss.item()])\n",
    "        # weights = dwa.update_weights()\n",
    "        # loss = weights[0] * loss + weights[1] * iso_loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for x, y, original_x in loader_test:\n",
    "            y_recon  = model(x)\n",
    "            vloss = criterion(y_recon, y)\n",
    "            # iso_loss = isoterm_loss(y_recon, original_x)\n",
    "            # vloss = weights[0] * vloss + weights[1] * iso_loss\n",
    "            total_vloss += vloss.item()\n",
    "\n",
    "    return total_loss / len(loader.dataset), total_vloss / len(loader_test.dataset)\n"
   ],
   "id": "e5e1fac9c7ab87a1",
   "outputs": [],
   "execution_count": 98
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-15T10:41:05.816597Z",
     "start_time": "2025-07-15T10:40:56.081781Z"
    }
   },
   "cell_type": "code",
   "source": [
    "epochs = 100\n",
    "loss_lst = []\n",
    "vloss_lst = []\n",
    "for epoch in range(1, epochs+1):\n",
    "    loss, vloss = train_PSD_model(model_PSD, PSD_loader, PSD_loader_test)\n",
    "    loss_lst.append(loss)\n",
    "    vloss_lst.append(vloss)\n",
    "    if epoch % 1 == 0:\n",
    "        print(f\"Epoch {epoch}/{epochs}, Loss: {loss*100:.8f} Test loss: {vloss*100:.8f}\")"
   ],
   "id": "2fd72aae7361b860",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100, Loss: 0.00001148 Test loss: 0.00000008\n",
      "Epoch 2/100, Loss: 0.00000008 Test loss: 0.00000008\n",
      "Epoch 3/100, Loss: 0.00000008 Test loss: 0.00000008\n",
      "Epoch 4/100, Loss: 0.00000008 Test loss: 0.00000008\n",
      "Epoch 5/100, Loss: 0.00000008 Test loss: 0.00000008\n",
      "Epoch 6/100, Loss: 0.00000008 Test loss: 0.00000008\n",
      "Epoch 7/100, Loss: 0.00000008 Test loss: 0.00000008\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[99], line 5\u001B[0m\n\u001B[0;32m      3\u001B[0m vloss_lst \u001B[38;5;241m=\u001B[39m []\n\u001B[0;32m      4\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m epoch \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;241m1\u001B[39m, epochs\u001B[38;5;241m+\u001B[39m\u001B[38;5;241m1\u001B[39m):\n\u001B[1;32m----> 5\u001B[0m     loss, vloss \u001B[38;5;241m=\u001B[39m \u001B[43mtrain_PSD_model\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel_PSD\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mPSD_loader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mPSD_loader_test\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m      6\u001B[0m     loss_lst\u001B[38;5;241m.\u001B[39mappend(loss)\n\u001B[0;32m      7\u001B[0m     vloss_lst\u001B[38;5;241m.\u001B[39mappend(vloss)\n",
      "Cell \u001B[1;32mIn[98], line 71\u001B[0m, in \u001B[0;36mtrain_PSD_model\u001B[1;34m(model, loader, loader_test)\u001B[0m\n\u001B[0;32m     69\u001B[0m total_loss \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0\u001B[39m\n\u001B[0;32m     70\u001B[0m total_vloss \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0\u001B[39m\n\u001B[1;32m---> 71\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m x, y, original_x \u001B[38;5;129;01min\u001B[39;00m loader:\n\u001B[0;32m     72\u001B[0m     optimizer\u001B[38;5;241m.\u001B[39mzero_grad()\n\u001B[0;32m     73\u001B[0m     y_recon \u001B[38;5;241m=\u001B[39m model(x)\n",
      "File \u001B[1;32m~\\PycharmProjects\\isotherm\\.venv\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:733\u001B[0m, in \u001B[0;36m_BaseDataLoaderIter.__next__\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    730\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_sampler_iter \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    731\u001B[0m     \u001B[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001B[39;00m\n\u001B[0;32m    732\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_reset()  \u001B[38;5;66;03m# type: ignore[call-arg]\u001B[39;00m\n\u001B[1;32m--> 733\u001B[0m data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_next_data\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    734\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_yielded \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[0;32m    735\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m (\n\u001B[0;32m    736\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_dataset_kind \u001B[38;5;241m==\u001B[39m _DatasetKind\u001B[38;5;241m.\u001B[39mIterable\n\u001B[0;32m    737\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_IterableDataset_len_called \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m    738\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_yielded \u001B[38;5;241m>\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_IterableDataset_len_called\n\u001B[0;32m    739\u001B[0m ):\n",
      "File \u001B[1;32m~\\PycharmProjects\\isotherm\\.venv\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:789\u001B[0m, in \u001B[0;36m_SingleProcessDataLoaderIter._next_data\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    787\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21m_next_data\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[0;32m    788\u001B[0m     index \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_next_index()  \u001B[38;5;66;03m# may raise StopIteration\u001B[39;00m\n\u001B[1;32m--> 789\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_dataset_fetcher\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfetch\u001B[49m\u001B[43m(\u001B[49m\u001B[43mindex\u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# may raise StopIteration\u001B[39;00m\n\u001B[0;32m    790\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_pin_memory:\n\u001B[0;32m    791\u001B[0m         data \u001B[38;5;241m=\u001B[39m _utils\u001B[38;5;241m.\u001B[39mpin_memory\u001B[38;5;241m.\u001B[39mpin_memory(data, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_pin_memory_device)\n",
      "File \u001B[1;32m~\\PycharmProjects\\isotherm\\.venv\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:55\u001B[0m, in \u001B[0;36m_MapDatasetFetcher.fetch\u001B[1;34m(self, possibly_batched_index)\u001B[0m\n\u001B[0;32m     53\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m     54\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset[possibly_batched_index]\n\u001B[1;32m---> 55\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcollate_fn\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdata\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\PycharmProjects\\isotherm\\.venv\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:398\u001B[0m, in \u001B[0;36mdefault_collate\u001B[1;34m(batch)\u001B[0m\n\u001B[0;32m    337\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21mdefault_collate\u001B[39m(batch):\n\u001B[0;32m    338\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124mr\u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    339\u001B[0m \u001B[38;5;124;03m    Take in a batch of data and put the elements within the batch into a tensor with an additional outer dimension - batch size.\u001B[39;00m\n\u001B[0;32m    340\u001B[0m \n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    396\u001B[0m \u001B[38;5;124;03m        >>> default_collate(batch)  # Handle `CustomType` automatically\u001B[39;00m\n\u001B[0;32m    397\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m--> 398\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mcollate\u001B[49m\u001B[43m(\u001B[49m\u001B[43mbatch\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcollate_fn_map\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdefault_collate_fn_map\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\PycharmProjects\\isotherm\\.venv\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:211\u001B[0m, in \u001B[0;36mcollate\u001B[1;34m(batch, collate_fn_map)\u001B[0m\n\u001B[0;32m    208\u001B[0m transposed \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlist\u001B[39m(\u001B[38;5;28mzip\u001B[39m(\u001B[38;5;241m*\u001B[39mbatch))  \u001B[38;5;66;03m# It may be accessed twice, so we use a list.\u001B[39;00m\n\u001B[0;32m    210\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(elem, \u001B[38;5;28mtuple\u001B[39m):\n\u001B[1;32m--> 211\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m [\n\u001B[0;32m    212\u001B[0m         collate(samples, collate_fn_map\u001B[38;5;241m=\u001B[39mcollate_fn_map)\n\u001B[0;32m    213\u001B[0m         \u001B[38;5;28;01mfor\u001B[39;00m samples \u001B[38;5;129;01min\u001B[39;00m transposed\n\u001B[0;32m    214\u001B[0m     ]  \u001B[38;5;66;03m# Backwards compatibility.\u001B[39;00m\n\u001B[0;32m    215\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    216\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n",
      "File \u001B[1;32m~\\PycharmProjects\\isotherm\\.venv\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:212\u001B[0m, in \u001B[0;36m<listcomp>\u001B[1;34m(.0)\u001B[0m\n\u001B[0;32m    208\u001B[0m transposed \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlist\u001B[39m(\u001B[38;5;28mzip\u001B[39m(\u001B[38;5;241m*\u001B[39mbatch))  \u001B[38;5;66;03m# It may be accessed twice, so we use a list.\u001B[39;00m\n\u001B[0;32m    210\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(elem, \u001B[38;5;28mtuple\u001B[39m):\n\u001B[0;32m    211\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m [\n\u001B[1;32m--> 212\u001B[0m         \u001B[43mcollate\u001B[49m\u001B[43m(\u001B[49m\u001B[43msamples\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcollate_fn_map\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcollate_fn_map\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    213\u001B[0m         \u001B[38;5;28;01mfor\u001B[39;00m samples \u001B[38;5;129;01min\u001B[39;00m transposed\n\u001B[0;32m    214\u001B[0m     ]  \u001B[38;5;66;03m# Backwards compatibility.\u001B[39;00m\n\u001B[0;32m    215\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    216\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n",
      "File \u001B[1;32m~\\PycharmProjects\\isotherm\\.venv\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:155\u001B[0m, in \u001B[0;36mcollate\u001B[1;34m(batch, collate_fn_map)\u001B[0m\n\u001B[0;32m    153\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m collate_fn_map \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    154\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m elem_type \u001B[38;5;129;01min\u001B[39;00m collate_fn_map:\n\u001B[1;32m--> 155\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mcollate_fn_map\u001B[49m\u001B[43m[\u001B[49m\u001B[43melem_type\u001B[49m\u001B[43m]\u001B[49m\u001B[43m(\u001B[49m\u001B[43mbatch\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcollate_fn_map\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcollate_fn_map\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    157\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m collate_type \u001B[38;5;129;01min\u001B[39;00m collate_fn_map:\n\u001B[0;32m    158\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(elem, collate_type):\n",
      "File \u001B[1;32m~\\PycharmProjects\\isotherm\\.venv\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:272\u001B[0m, in \u001B[0;36mcollate_tensor_fn\u001B[1;34m(batch, collate_fn_map)\u001B[0m\n\u001B[0;32m    270\u001B[0m     storage \u001B[38;5;241m=\u001B[39m elem\u001B[38;5;241m.\u001B[39m_typed_storage()\u001B[38;5;241m.\u001B[39m_new_shared(numel, device\u001B[38;5;241m=\u001B[39melem\u001B[38;5;241m.\u001B[39mdevice)\n\u001B[0;32m    271\u001B[0m     out \u001B[38;5;241m=\u001B[39m elem\u001B[38;5;241m.\u001B[39mnew(storage)\u001B[38;5;241m.\u001B[39mresize_(\u001B[38;5;28mlen\u001B[39m(batch), \u001B[38;5;241m*\u001B[39m\u001B[38;5;28mlist\u001B[39m(elem\u001B[38;5;241m.\u001B[39msize()))\n\u001B[1;32m--> 272\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstack\u001B[49m\u001B[43m(\u001B[49m\u001B[43mbatch\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mout\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mout\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 99
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-15T14:44:52.785548Z",
     "start_time": "2025-07-15T14:44:52.739551Z"
    }
   },
   "cell_type": "code",
   "source": [
    "plt.plot(loss_lst)\n",
    "plt.plot(vloss_lst)\n",
    "plt.show()"
   ],
   "id": "826613b63fcbd7f1",
   "outputs": [],
   "execution_count": 152
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-10T07:40:06.630580Z",
     "start_time": "2025-07-10T07:40:06.615581Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model_name = \"autoencoder_regressor_combined\"\n",
    "torch.save(model_PSD, f\"data/models/torch/{model_name}\") "
   ],
   "id": "8b63396d69bd925e",
   "outputs": [],
   "execution_count": 127
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-10T08:47:49.601637Z",
     "start_time": "2025-07-10T08:47:49.561905Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model_name = \"autoencoder_regressor\"\n",
    "model_PSD = torch.load(f\"data/models/torch/{model_name}\", weights_only=False)"
   ],
   "id": "5df22fca0b595bb7",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-15T10:41:11.168395Z",
     "start_time": "2025-07-15T10:41:11.096028Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model_PSD.eval()\n",
    "y_train_PSD = model_PSD.model(torch.tensor(latent_vectors_train, dtype=torch.float32).to(device)).detach().cpu().numpy()\n",
    "y_test_PSD = model_PSD.model(torch.tensor(latent_vectors_test, dtype=torch.float32).to(device)).detach().cpu().numpy()\n",
    "y_test_exp_PSD = model_PSD.model(torch.tensor(latent_vectors_test_exp, dtype=torch.float32).to(device)).detach().cpu().numpy()"
   ],
   "id": "bd320d01dab9040b",
   "outputs": [],
   "execution_count": 101
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-15T14:48:46.248553Z",
     "start_time": "2025-07-15T14:47:57.350536Z"
    }
   },
   "cell_type": "code",
   "source": "plot_preds(x_test_exp, y_test_exp, y_test_exp_PSD)",
   "id": "424bd83f3fe2a9f9",
   "outputs": [],
   "execution_count": 156
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-10T07:40:41.166755Z",
     "start_time": "2025-07-10T07:40:41.138746Z"
    }
   },
   "cell_type": "code",
   "source": [
    "np.savez(f\"data/models/metrics/{model_name}\", x=x_test_exp, y=y_test_exp_PSD)\n",
    "model_name\n"
   ],
   "id": "b11eabe3085b3dc5",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'autoencoder_regressor_combined'"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 131
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "89e0e8e7540ebf2e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "3b5400f1a2ea29b1"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-10T09:56:27.468085Z",
     "start_time": "2025-07-10T09:56:27.447714Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model2 = \"autoencoder_regressor_combined\"\n",
    "model2_data = np.load(f\"data/models/metrics/{model2}.npz\")\n",
    "model2_x = model2_data[\"x\"]\n",
    "model2_y = model2_data[\"y\"]"
   ],
   "id": "ff4850d3c12aad63",
   "outputs": [],
   "execution_count": 72
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-10T09:57:49.497143Z",
     "start_time": "2025-07-10T09:56:28.082908Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "model2_y_math = [fit_linear(model2_x[i], data_sorb[:, :-10], 0).x for i in range(len(model2_x))]"
   ],
   "id": "d9638b31eec76efc",
   "outputs": [],
   "execution_count": 73
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-15T14:47:45.836216Z",
     "start_time": "2025-07-15T14:47:45.805216Z"
    }
   },
   "cell_type": "code",
   "source": "plot_preds(model2_x, model2_y_math, model2_y)",
   "id": "190a38ad01d9974d",
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model2_x' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[155], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m plot_preds(\u001B[43mmodel2_x\u001B[49m, model2_y_math, model2_y)\n",
      "\u001B[1;31mNameError\u001B[0m: name 'model2_x' is not defined"
     ]
    }
   ],
   "execution_count": 155
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "b57282e80f242f36"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-15T07:15:08.894169Z",
     "start_time": "2025-07-15T07:15:08.862561Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv(\"data/initial kernels/excel/kernel_N2_77K.csv\")"
   ],
   "id": "7d218eed32a5f654",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-15T07:15:09.860213Z",
     "start_time": "2025-07-15T07:15:09.844147Z"
    }
   },
   "cell_type": "code",
   "source": "pressures = data[\"# P/P0\\\\H[nm]\"]",
   "id": "6329324e18f5cd71",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-15T07:15:10.928378Z",
     "start_time": "2025-07-15T07:15:10.912497Z"
    }
   },
   "cell_type": "code",
   "source": [
    "a_array = []\n",
    "for i in (data.columns[1:]).to_numpy():\n",
    "    a_array.append(float(i))\n",
    "a_array = np.array(a_array)"
   ],
   "id": "adba2d4b571ef4d7",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-15T07:15:11.792976Z",
     "start_time": "2025-07-15T07:15:11.779194Z"
    }
   },
   "cell_type": "code",
   "source": "new_kernel = data.to_numpy()",
   "id": "16cd3fe05cb482f9",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-15T07:47:36.650309Z",
     "start_time": "2025-07-15T07:47:36.645799Z"
    }
   },
   "cell_type": "code",
   "source": [
    "np.save(\"data/initial kernels/new_kernel/pressure.npy\", pressures)\n",
    "np.save(\"data/initial kernels/new_kernel/pore_sizes.npy\", a_array)\n",
    "np.save(\"data/initial kernels/new_kernel/kernel.npy\", new_kernel[:, 1:].T)"
   ],
   "id": "40dc6c7db073b0e0",
   "outputs": [],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-15T07:47:12.647087Z",
     "start_time": "2025-07-15T07:47:12.640577Z"
    }
   },
   "cell_type": "code",
   "source": "new_kernel[:, 1:]",
   "id": "b55bbb0915d15841",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3.95232682e+00, 3.73193260e+00, 3.32136295e+00, ...,\n",
       "        1.99026908e-04, 1.76855432e-04, 1.59128603e-04],\n",
       "       [4.19957099e+00, 3.94315055e+00, 3.55339145e+00, ...,\n",
       "        2.29265468e-04, 2.03725435e-04, 1.83305333e-04],\n",
       "       [4.42334610e+00, 4.19271427e+00, 3.78090471e+00, ...,\n",
       "        2.88698360e-04, 2.56537540e-04, 2.30823899e-04],\n",
       "       ...,\n",
       "       [1.53910509e+01, 1.49431184e+01, 1.45971719e+01, ...,\n",
       "        1.72948254e+01, 1.72948254e+01, 1.72948254e+01],\n",
       "       [1.53992906e+01, 1.49613487e+01, 1.45868859e+01, ...,\n",
       "        1.73099488e+01, 1.73099488e+01, 1.73099488e+01],\n",
       "       [1.53999701e+01, 1.49585595e+01, 1.45743595e+01, ...,\n",
       "        1.73115993e+01, 1.73115993e+01, 1.73115993e+01]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "46e86efa1df2af42"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
