{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-06-19T17:49:38.594438Z",
     "start_time": "2025-06-19T17:49:36.100901Z"
    }
   },
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from datasetLoader import load_dataset\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "from inverse import fit_linear\n",
    "from tools import model_tester\n",
    "%matplotlib qt"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-19T17:49:38.610515Z",
     "start_time": "2025-06-19T17:49:38.594438Z"
    }
   },
   "cell_type": "code",
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ],
   "id": "d322819b15356399",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-19T17:49:43.651661Z",
     "start_time": "2025-06-19T17:49:38.799443Z"
    }
   },
   "cell_type": "code",
   "source": [
    "pore_widths = np.load(\"data/initial kernels/Size_Kernel_Silica_Adsorption.npy\")\n",
    "pressures = np.load(\"data/initial kernels/Pressure_Silica.npy\")\n",
    "with open(\"data/initial kernels/Kernel_Silica_Adsorption.npy\", 'rb') as f:\n",
    "    data_sorb = np.load(f)\n",
    "\n",
    "x, y = load_dataset('data/datasets/silica_random.npz')\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.15, random_state=1)\n",
    "\n",
    "x_exp, y_exp = load_dataset('data/datasets/SMP_CUT_NOT_ZERO.npz')\n",
    "\n",
    "x_train_exp, x_test_exp, y_train_exp, y_test_exp = train_test_split(x_exp, y_exp, test_size=0.15, random_state=1)"
   ],
   "id": "d29956229295484c",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-19T17:24:01.722700Z",
     "start_time": "2025-06-19T17:24:01.507951Z"
    }
   },
   "cell_type": "code",
   "source": [
    "figure, axis = plt.subplots(3, 4)\n",
    "for i in range(3):\n",
    "    for j in range(4):\n",
    "        k = np.random.randint(0, len(x_train))\n",
    "        axis[i, j].plot(pore_widths, y_train[k], marker=\".\")\n",
    "        axis[i, j].grid()\n",
    "plt.show()"
   ],
   "id": "15f9ff24438da3f5",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-18T11:43:49.397871Z",
     "start_time": "2025-06-18T11:43:49.317259Z"
    }
   },
   "cell_type": "code",
   "source": "plt.plot(pore_widths, sum(y_train), marker=\".\")",
   "id": "d204e5d61104e3ed",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1e7b3b47a00>]"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 189
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-13T09:02:46.726188Z",
     "start_time": "2025-06-13T09:02:46.680189Z"
    }
   },
   "cell_type": "code",
   "source": [
    "i = np.random.randint(0, len(x_train))\n",
    "plt.plot(pressures[:-10], x_train[i], marker=\".\")\n",
    "plt.grid()\n",
    "plt.show()"
   ],
   "id": "3201b1f78762f4ff",
   "outputs": [],
   "execution_count": 272
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-19T18:32:01.271523Z",
     "start_time": "2025-06-19T18:32:00.956114Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class IsothermDataset(Dataset):\n",
    "    def __init__(self, isotherms, transform=None):\n",
    "        self.data = torch.tensor(isotherms, dtype=torch.float32).to(device)\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = self.data[idx]\n",
    "        if self.transform:\n",
    "            x = self.transform(x)\n",
    "        return x, x\n",
    "\n",
    "x_mixed_train = np.concatenate((x_train_exp, x_train))\n",
    "x_mixed_test = np.concatenate((x_test_exp, x_test))\n",
    "\n",
    "# dataset = IsothermDataset(np.concatenate((x_train_exp, x_train_exp)))\n",
    "# dataset_test = IsothermDataset(np.concatenate((x_test_exp, x_test_exp)))\n",
    "dataset = IsothermDataset(np.concatenate((x_mixed_train, x_mixed_train)))\n",
    "dataset_test = IsothermDataset(np.concatenate((x_mixed_test, x_mixed_test)))\n",
    "\n",
    "\n",
    "batch_size = 512\n",
    "loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "loader_test = DataLoader(dataset_test, batch_size=batch_size, shuffle=False)"
   ],
   "id": "5cefc1b65fc7fa63",
   "outputs": [],
   "execution_count": 61
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-19T18:32:01.853214Z",
     "start_time": "2025-06-19T18:32:01.839177Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self, input_dim, latent_dim):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, latent_dim)\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(latent_dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, input_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        z = self.encoder(x)\n",
    "        x_recon = self.decoder(z)\n",
    "        return x_recon, z\n",
    "\n",
    "input_dim = 448\n",
    "latent_dim = 16\n",
    "epochs = 200\n",
    "learning_rate = 1e-3\n",
    "\n",
    "model = Autoencoder(input_dim=input_dim, latent_dim=latent_dim)\n",
    "model.to(device)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "def train_autoencoder(model, loader, loader_test):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    total_vloss = 0\n",
    "    for x, _ in loader:\n",
    "        optimizer.zero_grad()\n",
    "        x_recon, _ = model(x)\n",
    "        loss = criterion(x_recon, x)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for x, _ in loader_test:\n",
    "            x_recon, _  = model(x)\n",
    "            vloss = criterion(x_recon, x)\n",
    "            total_vloss += vloss.item()\n",
    "\n",
    "    return total_loss / len(loader.dataset), total_vloss / len(loader_test.dataset)\n",
    "\n",
    "\n",
    "# sample_z = model.encoder(torch.tensor(isotherms_np[0], dtype=torch.float32))"
   ],
   "id": "c960463d00766e7c",
   "outputs": [],
   "execution_count": 62
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-19T18:33:15.754383Z",
     "start_time": "2025-06-19T18:32:03.389327Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for epoch in range(1, epochs+1):\n",
    "    loss, vloss = train_autoencoder(model, loader,loader_test)\n",
    "    if epoch % 1 == 0:\n",
    "        print(f\"Epoch {epoch}/{epochs}, Loss: {loss*100:.8f} Test loss: {vloss*100:.8f}\")"
   ],
   "id": "6f625ed77c7b7164",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200, Loss: 0.00356772 Test loss: 0.00030211\n",
      "Epoch 2/200, Loss: 0.00014139 Test loss: 0.00011367\n",
      "Epoch 3/200, Loss: 0.00009425 Test loss: 0.00008442\n",
      "Epoch 4/200, Loss: 0.00006589 Test loss: 0.00005766\n",
      "Epoch 5/200, Loss: 0.00005241 Test loss: 0.00004579\n",
      "Epoch 6/200, Loss: 0.00004097 Test loss: 0.00003817\n",
      "Epoch 7/200, Loss: 0.00003918 Test loss: 0.00003647\n",
      "Epoch 8/200, Loss: 0.00003675 Test loss: 0.00003650\n",
      "Epoch 9/200, Loss: 0.00003175 Test loss: 0.00002873\n",
      "Epoch 10/200, Loss: 0.00002786 Test loss: 0.00002396\n",
      "Epoch 11/200, Loss: 0.00002318 Test loss: 0.00002057\n",
      "Epoch 12/200, Loss: 0.00002173 Test loss: 0.00001898\n",
      "Epoch 13/200, Loss: 0.00002060 Test loss: 0.00001772\n",
      "Epoch 14/200, Loss: 0.00001858 Test loss: 0.00001779\n",
      "Epoch 15/200, Loss: 0.00001778 Test loss: 0.00001547\n",
      "Epoch 16/200, Loss: 0.00001733 Test loss: 0.00001747\n",
      "Epoch 17/200, Loss: 0.00001665 Test loss: 0.00001501\n",
      "Epoch 18/200, Loss: 0.00001750 Test loss: 0.00001852\n",
      "Epoch 19/200, Loss: 0.00001365 Test loss: 0.00001399\n",
      "Epoch 20/200, Loss: 0.00001452 Test loss: 0.00001216\n",
      "Epoch 21/200, Loss: 0.00001465 Test loss: 0.00001994\n",
      "Epoch 22/200, Loss: 0.00001307 Test loss: 0.00001265\n",
      "Epoch 23/200, Loss: 0.00001308 Test loss: 0.00001256\n",
      "Epoch 24/200, Loss: 0.00001264 Test loss: 0.00000996\n",
      "Epoch 25/200, Loss: 0.00001232 Test loss: 0.00001148\n",
      "Epoch 26/200, Loss: 0.00001197 Test loss: 0.00000994\n",
      "Epoch 27/200, Loss: 0.00001108 Test loss: 0.00001131\n",
      "Epoch 28/200, Loss: 0.00001109 Test loss: 0.00001614\n",
      "Epoch 29/200, Loss: 0.00001070 Test loss: 0.00001364\n",
      "Epoch 30/200, Loss: 0.00001034 Test loss: 0.00000764\n",
      "Epoch 31/200, Loss: 0.00001121 Test loss: 0.00000725\n",
      "Epoch 32/200, Loss: 0.00000888 Test loss: 0.00000970\n",
      "Epoch 33/200, Loss: 0.00000964 Test loss: 0.00000731\n",
      "Epoch 34/200, Loss: 0.00000874 Test loss: 0.00000815\n",
      "Epoch 35/200, Loss: 0.00000906 Test loss: 0.00000715\n",
      "Epoch 36/200, Loss: 0.00000844 Test loss: 0.00001547\n",
      "Epoch 37/200, Loss: 0.00000850 Test loss: 0.00000745\n",
      "Epoch 38/200, Loss: 0.00000827 Test loss: 0.00000738\n",
      "Epoch 39/200, Loss: 0.00000781 Test loss: 0.00000660\n",
      "Epoch 40/200, Loss: 0.00000844 Test loss: 0.00000782\n",
      "Epoch 41/200, Loss: 0.00000721 Test loss: 0.00000886\n",
      "Epoch 42/200, Loss: 0.00000705 Test loss: 0.00000694\n",
      "Epoch 43/200, Loss: 0.00000819 Test loss: 0.00000778\n",
      "Epoch 44/200, Loss: 0.00000611 Test loss: 0.00000791\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[63], line 2\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m epoch \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;241m1\u001B[39m, epochs\u001B[38;5;241m+\u001B[39m\u001B[38;5;241m1\u001B[39m):\n\u001B[1;32m----> 2\u001B[0m     loss, vloss \u001B[38;5;241m=\u001B[39m \u001B[43mtrain_autoencoder\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mloader\u001B[49m\u001B[43m,\u001B[49m\u001B[43mloader_test\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m      3\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m epoch \u001B[38;5;241m%\u001B[39m \u001B[38;5;241m1\u001B[39m \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[0;32m      4\u001B[0m         \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mEpoch \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mepoch\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m/\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mepochs\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m, Loss: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mloss\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m100\u001B[39m\u001B[38;5;132;01m:\u001B[39;00m\u001B[38;5;124m.8f\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m Test loss: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mvloss\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m100\u001B[39m\u001B[38;5;132;01m:\u001B[39;00m\u001B[38;5;124m.8f\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n",
      "Cell \u001B[1;32mIn[62], line 41\u001B[0m, in \u001B[0;36mtrain_autoencoder\u001B[1;34m(model, loader, loader_test)\u001B[0m\n\u001B[0;32m     39\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m x, _ \u001B[38;5;129;01min\u001B[39;00m loader:\n\u001B[0;32m     40\u001B[0m     optimizer\u001B[38;5;241m.\u001B[39mzero_grad()\n\u001B[1;32m---> 41\u001B[0m     x_recon, _ \u001B[38;5;241m=\u001B[39m \u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     42\u001B[0m     loss \u001B[38;5;241m=\u001B[39m criterion(x_recon, x)\n\u001B[0;32m     43\u001B[0m     loss\u001B[38;5;241m.\u001B[39mbackward()\n",
      "File \u001B[1;32m~\\PycharmProjects\\isotherm\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1749\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1750\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1751\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32m~\\PycharmProjects\\isotherm\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1757\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1758\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1759\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1760\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1761\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1762\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1764\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m   1765\u001B[0m called_always_called_hooks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m()\n",
      "Cell \u001B[1;32mIn[62], line 20\u001B[0m, in \u001B[0;36mAutoencoder.forward\u001B[1;34m(self, x)\u001B[0m\n\u001B[0;32m     19\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, x):\n\u001B[1;32m---> 20\u001B[0m     z \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mencoder\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     21\u001B[0m     x_recon \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdecoder(z)\n\u001B[0;32m     22\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m x_recon, z\n",
      "File \u001B[1;32m~\\PycharmProjects\\isotherm\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1749\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1750\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1751\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32m~\\PycharmProjects\\isotherm\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1757\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1758\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1759\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1760\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1761\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1762\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1764\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m   1765\u001B[0m called_always_called_hooks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m()\n",
      "File \u001B[1;32m~\\PycharmProjects\\isotherm\\.venv\\lib\\site-packages\\torch\\nn\\modules\\container.py:240\u001B[0m, in \u001B[0;36mSequential.forward\u001B[1;34m(self, input)\u001B[0m\n\u001B[0;32m    238\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m):\n\u001B[0;32m    239\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m module \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m:\n\u001B[1;32m--> 240\u001B[0m         \u001B[38;5;28minput\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[43mmodule\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m    241\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28minput\u001B[39m\n",
      "File \u001B[1;32m~\\PycharmProjects\\isotherm\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1749\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1750\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1751\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32m~\\PycharmProjects\\isotherm\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1757\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1758\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1759\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1760\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1761\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1762\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1764\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m   1765\u001B[0m called_always_called_hooks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m()\n",
      "File \u001B[1;32m~\\PycharmProjects\\isotherm\\.venv\\lib\\site-packages\\torch\\nn\\modules\\activation.py:133\u001B[0m, in \u001B[0;36mReLU.forward\u001B[1;34m(self, input)\u001B[0m\n\u001B[0;32m    132\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m: Tensor) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tensor:\n\u001B[1;32m--> 133\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mF\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrelu\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minplace\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43minplace\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\PycharmProjects\\isotherm\\.venv\\lib\\site-packages\\torch\\nn\\functional.py:1704\u001B[0m, in \u001B[0;36mrelu\u001B[1;34m(input, inplace)\u001B[0m\n\u001B[0;32m   1702\u001B[0m     result \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mrelu_(\u001B[38;5;28minput\u001B[39m)\n\u001B[0;32m   1703\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1704\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrelu\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1705\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m result\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 63
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-18T09:30:34.136669Z",
     "start_time": "2025-06-18T09:30:34.110776Z"
    }
   },
   "cell_type": "code",
   "source": "torch.save(model, \"data/models/torch/autoencoder_exp.pt\")",
   "id": "6e5a9e294045c74c",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-19T17:49:44.870603Z",
     "start_time": "2025-06-19T17:49:44.851980Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model = torch.load(\"data/models/torch/autoencoder_exp.pt\", weights_only=False)\n",
    "model.eval()"
   ],
   "id": "f8af0cb897395d7c",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Autoencoder(\n",
       "  (encoder): Sequential(\n",
       "    (0): Linear(in_features=448, out_features=128, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=128, out_features=64, bias=True)\n",
       "    (3): ReLU()\n",
       "    (4): Linear(in_features=64, out_features=16, bias=True)\n",
       "  )\n",
       "  (decoder): Sequential(\n",
       "    (0): Linear(in_features=16, out_features=64, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=64, out_features=128, bias=True)\n",
       "    (3): ReLU()\n",
       "    (4): Linear(in_features=128, out_features=448, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-19T18:33:25.037571Z",
     "start_time": "2025-06-19T18:33:24.936697Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model.eval()\n",
    "latent_vectors_train = model.encoder(torch.tensor(x_train, dtype=torch.float32).to(device)).detach().cpu().numpy()\n",
    "latent_vectors_test = model.encoder(torch.tensor(x_test, dtype=torch.float32).to(device)).detach().cpu().numpy()\n",
    "latent_vectors_test_exp = model.encoder(torch.tensor(x_test_exp, dtype=torch.float32).to(device)).detach().cpu().numpy()"
   ],
   "id": "59307777a8d5284",
   "outputs": [],
   "execution_count": 64
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-19T18:33:25.620169Z",
     "start_time": "2025-06-19T18:33:25.603120Z"
    }
   },
   "cell_type": "code",
   "source": "decoded = model.decoder(model.encoder(torch.tensor(x_test_exp, dtype=torch.float32).to(device))).detach().cpu().numpy()",
   "id": "a95fe3351e57df09",
   "outputs": [],
   "execution_count": 65
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-19T18:33:46.270752Z",
     "start_time": "2025-06-19T18:33:46.170967Z"
    }
   },
   "cell_type": "code",
   "source": [
    "figure, axis = plt.subplots(3, 3)\n",
    "for i in range(3):\n",
    "    for j in range(3):\n",
    "        k=np.random.randint(0, len(decoded))\n",
    "        axis[i, j].plot(pressures[:-10], x_test_exp[k], marker=\".\", label = \"origin\")\n",
    "        axis[i, j].plot(pressures[:-10], decoded[k], marker=\".\", label = \"decoded\")\n",
    "        axis[i, j].grid(True)\n",
    "axis[i, j].legend()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# k=np.random.randint(0, len(decoded))\n",
    "# plt.plot(pressures[:-10], x_test_exp[k], marker=\".\", label = \"origin\")\n",
    "# plt.plot(pressures[:-10], decoded[k], marker=\".\", label = \"decoded\")\n",
    "# plt.legend()\n",
    "# plt.grid(True)\n",
    "# plt.show()"
   ],
   "id": "dbe5444885056917",
   "outputs": [],
   "execution_count": 67
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-19T18:34:05.619364Z",
     "start_time": "2025-06-19T18:34:05.037516Z"
    }
   },
   "cell_type": "code",
   "source": [
    "np.random.seed(0)\n",
    "labels = None \n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "latent_pca = pca.fit_transform(latent_vectors_train[:100])\n",
    "latent_pca_exp = pca.fit_transform(latent_vectors_test[:100])\n",
    "\n",
    "tsne = TSNE(n_components=2, init='pca', random_state=0)\n",
    "latent_tsne = tsne.fit_transform(latent_vectors_train[:100])\n",
    "latent_tsne_exp = tsne.fit_transform(latent_vectors_test[:100])\n",
    "\n",
    "plt.figure()\n",
    "plt.scatter(latent_pca[:, 0], latent_pca[:, 1], label=\"train\")\n",
    "plt.scatter(latent_pca_exp[:, 0], latent_pca_exp[:, 1], label=\"exp\")\n",
    "plt.title(\"PCA of Latent Space\")\n",
    "plt.xlabel(\"PC1\")\n",
    "plt.ylabel(\"PC2\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "plt.scatter(latent_tsne[:, 0], latent_tsne[:, 1], label=\"train\")\n",
    "plt.scatter(latent_tsne_exp[:, 0], latent_tsne_exp[:, 1], label=\"exp\")\n",
    "for i in range(latent_tsne_exp.shape[0]):\n",
    "        plt.text(latent_tsne_exp[i, 0], latent_tsne_exp[i, 1], str(i), fontsize=8, ha='center', va='center')\n",
    "plt.title(\"t-SNE of Latent Space\")\n",
    "plt.xlabel(\"Dim 1\")\n",
    "plt.ylabel(\"Dim 2\")\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "id": "f578073be61beebe",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\anton\\PycharmProjects\\isotherm\\.venv\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:810: FutureWarning: The default learning rate in TSNE will change from 200.0 to 'auto' in 1.2.\n",
      "  warnings.warn(\n",
      "C:\\Users\\anton\\PycharmProjects\\isotherm\\.venv\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:996: FutureWarning: The PCA initialization in TSNE will change to have the standard deviation of PC1 equal to 1e-4 in 1.2. This will ensure better convergence.\n",
      "  warnings.warn(\n",
      "C:\\Users\\anton\\PycharmProjects\\isotherm\\.venv\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:810: FutureWarning: The default learning rate in TSNE will change from 200.0 to 'auto' in 1.2.\n",
      "  warnings.warn(\n",
      "C:\\Users\\anton\\PycharmProjects\\isotherm\\.venv\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:996: FutureWarning: The PCA initialization in TSNE will change to have the standard deviation of PC1 equal to 1e-4 in 1.2. This will ensure better convergence.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "execution_count": 68
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-19T17:49:52.723745Z",
     "start_time": "2025-06-19T17:49:52.707932Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def plot_preds(x, y, preds): \n",
    "    NX, NY = 3, 4\n",
    "    figure, axis = plt.subplots(NX, NY)\n",
    "    for i in range(NX):\n",
    "        for j in range(NY):\n",
    "            k = np.random.randint(0, len(preds)) \n",
    "            iso_axis = axis[i, j].twiny()\n",
    "            iso_axis.set_xlabel(\"P/P$^0$\",fontsize=8)\n",
    "            iso_axis.plot(pressures[:-10], x[k], label=\"Isotherm\", color = 'green')\n",
    "            kernel = (data_sorb.T[:-10])\n",
    "            iso_axis.plot(pressures[:-10], np.dot(kernel, preds[k][:128]), label=\"Isotherm by model\", color=\"red\")\n",
    "            axis[i, j].set_title(f\"№ {k}\")\n",
    "            axis[i, j].title.set_size(10)\n",
    "            axis[i, j].grid()\n",
    "            axis[i, j].set_xlabel(\"nm\",fontsize=8)\n",
    "            axis[i, j].plot(pore_widths, (preds[k]), marker=\".\", label=f\"Model PSD\")\n",
    "            axis[i, j].plot(pore_widths, y[k], marker=\".\", label=\"PSD\")\n",
    "    plt.subplots_adjust(hspace=0.6, right=0.95, left=0.05, bottom=0.05, top=0.9)\n",
    "    plt.legend()\n",
    "    axis[0, 0].legend()\n",
    "    plt.show()"
   ],
   "id": "a07cbe66d234c80f",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-11T07:54:34.696430Z",
     "start_time": "2025-06-11T07:54:34.349935Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from tools import model_tester\n",
    "from inverse import fit_linear\n",
    "\n",
    "error_lst, roughness_lst = model_tester.test_model_predictions(preds, x_test_exp, kernel=data_sorb[:, :-10])\n",
    "kde_x, kde_error, kde_fun = model_tester.calculate_kde_data(error_lst, stop=150)\n",
    "print(\"average error:\", np.mean(error_lst))\n",
    "plt.plot(kde_x, kde_error, label=model_name)\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.plot()"
   ],
   "id": "1316a794fbe60afa",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average error: 19.576227837865584\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 56
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-19T18:21:02.593500Z",
     "start_time": "2025-06-19T18:21:02.577678Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class DynamicWeightAveraging:\n",
    "    def __init__(self, num_tasks, T=2.0):\n",
    "        self.num_tasks = num_tasks\n",
    "        self.T = T\n",
    "        self.loss_history = []  # список списков: [ [L1_1, L1_2, ...], [L2_1, L2_2, ...], ... ]\n",
    "\n",
    "    def update_weights(self):\n",
    "        # Требуются как минимум 2 эпохи\n",
    "        if len(self.loss_history[0]) < 2:\n",
    "            return np.ones(self.num_tasks) / self.num_tasks  # равномерные веса\n",
    "\n",
    "        r = []\n",
    "        for i in range(self.num_tasks):\n",
    "            li = self.loss_history[i]\n",
    "            r_i = li[-1] / (li[-2] + 1e-8)\n",
    "            r.append(r_i)\n",
    "\n",
    "        r = np.array(r)\n",
    "        weights = self.T * np.exp(r / self.T)\n",
    "        weights /= weights.sum()\n",
    "        return weights\n",
    "\n",
    "    def append_losses(self, losses):  # losses — список текущих значений потерь [L1, L2, ...]\n",
    "        if not self.loss_history:\n",
    "            self.loss_history = [[] for _ in range(len(losses))]\n",
    "        for i, l in enumerate(losses):\n",
    "            self.loss_history[i].append(l)\n",
    "dwa = DynamicWeightAveraging(num_tasks=2)"
   ],
   "id": "8a3f50c19ae75247",
   "outputs": [],
   "execution_count": 45
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-19T18:34:27.602986Z",
     "start_time": "2025-06-19T18:34:27.554540Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class PSD_model(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(PSD_model, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_dim, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.15),\n",
    "            nn.Linear(32, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.15),\n",
    "            nn.Linear(64, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.15),\n",
    "            nn.Linear(128, output_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        psd = self.model(x)\n",
    "        return psd\n",
    "\n",
    "class Isotherm_PSD_Dataset(Dataset):\n",
    "    def __init__(self, x, y, original_x, transform=None):\n",
    "        self.x = torch.tensor(x, dtype=torch.float32).to(device)\n",
    "        self.y = torch.tensor(y, dtype=torch.float32).to(device)\n",
    "        self.original_x = torch.tensor(original_x, dtype=torch.float32).to(device)\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = self.x[idx]\n",
    "        y = self.y[idx]\n",
    "        original_x = self.original_x[idx]\n",
    "        if self.transform:\n",
    "            x = self.transform(x)\n",
    "        return x, y, original_x\n",
    "\n",
    "train_PSD = Isotherm_PSD_Dataset(latent_vectors_train, (y_train), x_train)\n",
    "test_PSD = Isotherm_PSD_Dataset(latent_vectors_test, (y_test), x_test)\n",
    "\n",
    "batch_size = 512\n",
    "PSD_loader = DataLoader(train_PSD, batch_size=batch_size, shuffle=True)\n",
    "PSD_loader_test = DataLoader(test_PSD, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "\n",
    "model_PSD = PSD_model(input_dim=latent_dim, output_dim=128)\n",
    "model_PSD.to(device)\n",
    "\n",
    "optimizer = optim.Adam(model_PSD.parameters(), lr=learning_rate)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "\n",
    "with open(\"data/initial kernels/Kernel_Silica_Adsorption.npy\", 'rb') as f:\n",
    "    data_sorb_torch = torch.tensor(np.load(f)[:, :-10])\n",
    "    data_sorb_torch = data_sorb_torch.to(torch.float32)\n",
    "\n",
    "def isoterm_loss(predicted_y, x):\n",
    "    restored_isotherm = torch.matmul(predicted_y, data_sorb_torch)\n",
    "    loss = torch.mean((x - restored_isotherm) ** 2)\n",
    "    return loss\n",
    "\n",
    "def train_PSD_model(model, loader, loader_test):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    total_vloss = 0\n",
    "    for x, y, original_x in loader:\n",
    "        optimizer.zero_grad()\n",
    "        y_recon = model(x)\n",
    "        loss = criterion(y_recon, y)\n",
    "        iso_loss = isoterm_loss(y_recon, original_x)\n",
    "        dwa.append_losses([loss.item(), iso_loss.item()])\n",
    "        weights = dwa.update_weights()\n",
    "        loss = weights[0] * loss + weights[1] * iso_loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for x, y, original_x in loader_test:\n",
    "            y_recon  = model(x)\n",
    "            loss = criterion(y_recon, y)\n",
    "            iso_loss = isoterm_loss(y_recon, original_x)\n",
    "            vloss = weights[0] * loss + weights[1] * iso_loss\n",
    "            total_vloss += vloss.item()\n",
    "\n",
    "    return total_loss / len(loader.dataset), total_vloss / len(loader_test.dataset)\n"
   ],
   "id": "e5e1fac9c7ab87a1",
   "outputs": [],
   "execution_count": 69
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-19T18:34:27.886671Z",
     "start_time": "2025-06-19T18:34:27.871621Z"
    }
   },
   "cell_type": "code",
   "source": "dwa.update_weights()",
   "id": "1d4575cb48177097",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.50297639, 0.49702361])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 70
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-19T18:35:37.534852Z",
     "start_time": "2025-06-19T18:34:30.738921Z"
    }
   },
   "cell_type": "code",
   "source": [
    "epochs = 50\n",
    "loss_lst = []\n",
    "vloss_lst = []\n",
    "for epoch in range(1, epochs+1):\n",
    "    loss, vloss = train_PSD_model(model_PSD, PSD_loader, PSD_loader_test)\n",
    "    loss_lst.append(loss)\n",
    "    vloss_lst.append(vloss)\n",
    "    if epoch % 1 == 0:\n",
    "        print(f\"Epoch {epoch}/{epochs}, Loss: {loss*100:.8f} Test loss: {vloss*100:.8f}\")"
   ],
   "id": "2fd72aae7361b860",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50, Loss: 0.00402995 Test loss: 0.00154724\n",
      "Epoch 2/50, Loss: 0.00160973 Test loss: 0.00080544\n",
      "Epoch 3/50, Loss: 0.00098062 Test loss: 0.00068448\n",
      "Epoch 4/50, Loss: 0.00080046 Test loss: 0.00051201\n",
      "Epoch 5/50, Loss: 0.00071016 Test loss: 0.00050408\n",
      "Epoch 6/50, Loss: 0.00061367 Test loss: 0.00040244\n",
      "Epoch 7/50, Loss: 0.00056074 Test loss: 0.00042816\n",
      "Epoch 8/50, Loss: 0.00052721 Test loss: 0.00049521\n",
      "Epoch 9/50, Loss: 0.00052020 Test loss: 0.00042756\n",
      "Epoch 10/50, Loss: 0.00054076 Test loss: 0.00039911\n",
      "Epoch 11/50, Loss: 0.00049221 Test loss: 0.00049548\n",
      "Epoch 12/50, Loss: 0.00049799 Test loss: 0.00035964\n",
      "Epoch 13/50, Loss: 0.00042775 Test loss: 0.00047554\n",
      "Epoch 14/50, Loss: 0.00046024 Test loss: 0.00036754\n",
      "Epoch 15/50, Loss: 0.00042541 Test loss: 0.00041672\n",
      "Epoch 16/50, Loss: 0.00044517 Test loss: 0.00038077\n",
      "Epoch 17/50, Loss: 0.00044720 Test loss: 0.00043657\n",
      "Epoch 18/50, Loss: 0.00049191 Test loss: 0.00061936\n",
      "Epoch 19/50, Loss: 0.00073271 Test loss: 0.00049764\n",
      "Epoch 20/50, Loss: 0.00046143 Test loss: 0.00037975\n",
      "Epoch 21/50, Loss: 0.00044232 Test loss: 0.00038127\n",
      "Epoch 22/50, Loss: 0.00041906 Test loss: 0.00042368\n",
      "Epoch 23/50, Loss: 0.00039012 Test loss: 0.00033397\n",
      "Epoch 24/50, Loss: 0.00038132 Test loss: 0.00041768\n",
      "Epoch 25/50, Loss: 0.00038556 Test loss: 0.00040958\n",
      "Epoch 26/50, Loss: 0.00036412 Test loss: 0.00049557\n",
      "Epoch 27/50, Loss: 0.00036713 Test loss: 0.00039681\n",
      "Epoch 28/50, Loss: 0.00036398 Test loss: 0.00042709\n",
      "Epoch 29/50, Loss: 0.00035592 Test loss: 0.00047517\n",
      "Epoch 30/50, Loss: 0.00035053 Test loss: 0.00046865\n",
      "Epoch 31/50, Loss: 0.00037511 Test loss: 0.00046426\n",
      "Epoch 32/50, Loss: 0.00037370 Test loss: 0.00046117\n",
      "Epoch 33/50, Loss: 0.00039092 Test loss: 0.00040675\n",
      "Epoch 34/50, Loss: 0.00036466 Test loss: 0.00033223\n",
      "Epoch 35/50, Loss: 0.00037923 Test loss: 0.00041889\n",
      "Epoch 36/50, Loss: 0.00035571 Test loss: 0.00040296\n",
      "Epoch 37/50, Loss: 0.00035466 Test loss: 0.00050150\n",
      "Epoch 38/50, Loss: 0.00034808 Test loss: 0.00043836\n",
      "Epoch 39/50, Loss: 0.00035145 Test loss: 0.00044371\n",
      "Epoch 40/50, Loss: 0.00035344 Test loss: 0.00036254\n",
      "Epoch 41/50, Loss: 0.00036435 Test loss: 0.00049208\n",
      "Epoch 42/50, Loss: 0.00034404 Test loss: 0.00037729\n",
      "Epoch 43/50, Loss: 0.00036043 Test loss: 0.00067479\n",
      "Epoch 44/50, Loss: 0.00036434 Test loss: 0.00044122\n",
      "Epoch 45/50, Loss: 0.00032721 Test loss: 0.00042996\n",
      "Epoch 46/50, Loss: 0.00034807 Test loss: 0.00046364\n",
      "Epoch 47/50, Loss: 0.00034260 Test loss: 0.00043429\n",
      "Epoch 48/50, Loss: 0.00035841 Test loss: 0.00048305\n",
      "Epoch 49/50, Loss: 0.00058334 Test loss: 0.00047358\n",
      "Epoch 50/50, Loss: 0.00037757 Test loss: 0.00062625\n"
     ]
    }
   ],
   "execution_count": 71
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-19T18:35:42.286305Z",
     "start_time": "2025-06-19T18:35:42.236549Z"
    }
   },
   "cell_type": "code",
   "source": [
    "plt.plot(loss_lst)\n",
    "plt.plot(vloss_lst)\n",
    "plt.show()"
   ],
   "id": "826613b63fcbd7f1",
   "outputs": [],
   "execution_count": 72
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-19T18:04:56.450770Z",
     "start_time": "2025-06-19T18:04:56.436381Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model_name = \"autoencoder_regressor_pinn\"\n",
    "torch.save(model_PSD, f\"data/models/torch/{model_name}\") "
   ],
   "id": "8b63396d69bd925e",
   "outputs": [],
   "execution_count": 28
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-19T18:04:57.317092Z",
     "start_time": "2025-06-19T18:04:57.302121Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model_name = \"autoencoder_regressor_pinn\"\n",
    "model_PSD = torch.load(f\"data/models/torch/{model_name}\", weights_only=False)"
   ],
   "id": "5df22fca0b595bb7",
   "outputs": [],
   "execution_count": 29
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-19T18:35:46.984940Z",
     "start_time": "2025-06-19T18:35:46.936664Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model_PSD.eval()\n",
    "y_train_PSD = model_PSD.model(torch.tensor(latent_vectors_train, dtype=torch.float32).to(device)).detach().cpu().numpy()\n",
    "y_test_PSD = model_PSD.model(torch.tensor(latent_vectors_test, dtype=torch.float32).to(device)).detach().cpu().numpy()\n",
    "y_test_exp_PSD = model_PSD.model(torch.tensor(latent_vectors_test_exp, dtype=torch.float32).to(device)).detach().cpu().numpy()"
   ],
   "id": "bd320d01dab9040b",
   "outputs": [],
   "execution_count": 73
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-20T12:32:56.525750Z",
     "start_time": "2025-06-20T12:32:56.220454Z"
    }
   },
   "cell_type": "code",
   "source": "plot_preds(x_test_exp, y_test_exp, y_test_exp_PSD)",
   "id": "424bd83f3fe2a9f9",
   "outputs": [],
   "execution_count": 76
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-19T18:24:36.791935Z",
     "start_time": "2025-06-19T18:24:36.778937Z"
    }
   },
   "cell_type": "code",
   "source": [
    "np.savez(f\"data/models/metrics/{model_name}\", x=x_test_exp, y=y_test_exp_PSD)\n",
    "model_name\n"
   ],
   "id": "b11eabe3085b3dc5",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'autoencoder_regressor_pinn'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 51
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "89e0e8e7540ebf2e"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
