{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-06-13T07:26:33.466371Z",
     "start_time": "2025-06-13T07:26:28.608142Z"
    }
   },
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from datasetLoader import load_dataset\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib qt"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-13T07:26:33.482886Z",
     "start_time": "2025-06-13T07:26:33.476884Z"
    }
   },
   "cell_type": "code",
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ],
   "id": "d322819b15356399",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-13T07:26:37.941961Z",
     "start_time": "2025-06-13T07:26:34.091444Z"
    }
   },
   "cell_type": "code",
   "source": [
    "pore_widths = np.load(\"data/initial kernels/Size_Kernel_Silica_Adsorption.npy\")\n",
    "pressures = np.load(\"data/initial kernels/Pressure_Silica.npy\")\n",
    "with open(\"data/initial kernels/Kernel_Silica_Adsorption.npy\", 'rb') as f:\n",
    "    data_sorb = np.load(f)\n",
    "\n",
    "x, y = load_dataset('data/datasets/silica_random.npz')\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.15, random_state=1)\n",
    "\n",
    "x_exp, y_exp = load_dataset('data/datasets/SMP_CUT_NOT_ZERO.npz')\n",
    "\n",
    "x_train_exp, x_test_exp, y_train_exp, y_test_exp = train_test_split(x_exp, y_exp, test_size=0.15, random_state=1)"
   ],
   "id": "d29956229295484c",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-11T07:42:52.858404Z",
     "start_time": "2025-06-11T07:42:52.688415Z"
    }
   },
   "cell_type": "code",
   "source": [
    "i = np.random.randint(0, len(x_train))\n",
    "plt.plot(pore_widths, y_train[i], marker=\".\")\n",
    "plt.grid()\n",
    "plt.show()"
   ],
   "id": "15f9ff24438da3f5",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-11T07:42:53.353616Z",
     "start_time": "2025-06-11T07:42:53.341874Z"
    }
   },
   "cell_type": "code",
   "source": [
    "i = np.random.randint(0, len(x_train))\n",
    "plt.plot(pressures[:-10], x_train[i], marker=\".\")\n",
    "plt.grid()\n",
    "plt.show()"
   ],
   "id": "3201b1f78762f4ff",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-13T07:26:41.980852Z",
     "start_time": "2025-06-13T07:26:41.945294Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class IsothermDataset(Dataset):\n",
    "    def __init__(self, isotherms, transform=None):\n",
    "        self.data = torch.tensor(isotherms, dtype=torch.float32).to(device)\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = self.data[idx]\n",
    "        if self.transform:\n",
    "            x = self.transform(x)\n",
    "        return x, x\n",
    "\n",
    "dataset = IsothermDataset(np.concatenate((x_train_exp, x_train_exp)))\n",
    "dataset_test = IsothermDataset(np.concatenate((x_test_exp, x_test_exp)))\n",
    "\n",
    "\n",
    "batch_size = 512\n",
    "loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "loader_test = DataLoader(dataset_test, batch_size=batch_size, shuffle=False)"
   ],
   "id": "5cefc1b65fc7fa63",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-13T07:26:43.987438Z",
     "start_time": "2025-06-13T07:26:42.230788Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self, input_dim, latent_dim):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, latent_dim)\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(latent_dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, input_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        z = self.encoder(x)\n",
    "        x_recon = self.decoder(z)\n",
    "        return x_recon, z\n",
    "\n",
    "input_dim = 448\n",
    "latent_dim = 16\n",
    "epochs = 200\n",
    "learning_rate = 1e-3\n",
    "\n",
    "model = Autoencoder(input_dim=input_dim, latent_dim=latent_dim)\n",
    "model.to(device)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "def train_autoencoder(model, loader, loader_test):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    total_vloss = 0\n",
    "    for x, _ in loader:\n",
    "        optimizer.zero_grad()\n",
    "        x_recon, _ = model(x)\n",
    "        loss = criterion(x_recon, x)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for x, _ in loader_test:\n",
    "            x_recon, _  = model(x)\n",
    "            vloss = criterion(x_recon, x)\n",
    "            total_vloss += vloss.item()\n",
    "\n",
    "    return total_loss / len(loader.dataset), total_vloss / len(loader_test.dataset)\n",
    "\n",
    "\n",
    "# sample_z = model.encoder(torch.tensor(isotherms_np[0], dtype=torch.float32))"
   ],
   "id": "c960463d00766e7c",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-13T07:27:24.049384Z",
     "start_time": "2025-06-13T07:27:16.808767Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for epoch in range(1, epochs+1):\n",
    "    loss, vloss = train_autoencoder(model, loader,loader_test)\n",
    "    if epoch % 1 == 0:\n",
    "        print(f\"Epoch {epoch}/{epochs}, Loss: {loss*100:.8f} Test loss: {vloss*100:.8f}\")"
   ],
   "id": "6f625ed77c7b7164",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200, Loss: 0.00002977 Test loss: 0.00003961\n",
      "Epoch 2/200, Loss: 0.00002448 Test loss: 0.00002959\n",
      "Epoch 3/200, Loss: 0.00002201 Test loss: 0.00002942\n",
      "Epoch 4/200, Loss: 0.00002089 Test loss: 0.00002840\n",
      "Epoch 5/200, Loss: 0.00002038 Test loss: 0.00002761\n",
      "Epoch 6/200, Loss: 0.00002025 Test loss: 0.00002858\n",
      "Epoch 7/200, Loss: 0.00002035 Test loss: 0.00002915\n",
      "Epoch 8/200, Loss: 0.00002208 Test loss: 0.00003152\n",
      "Epoch 9/200, Loss: 0.00002714 Test loss: 0.00004133\n",
      "Epoch 10/200, Loss: 0.00002569 Test loss: 0.00003298\n",
      "Epoch 11/200, Loss: 0.00002190 Test loss: 0.00002802\n",
      "Epoch 12/200, Loss: 0.00002091 Test loss: 0.00002742\n",
      "Epoch 13/200, Loss: 0.00002008 Test loss: 0.00002709\n",
      "Epoch 14/200, Loss: 0.00002082 Test loss: 0.00003315\n",
      "Epoch 15/200, Loss: 0.00002422 Test loss: 0.00004077\n",
      "Epoch 16/200, Loss: 0.00004248 Test loss: 0.00008431\n",
      "Epoch 17/200, Loss: 0.00004545 Test loss: 0.00004041\n",
      "Epoch 18/200, Loss: 0.00003042 Test loss: 0.00003758\n",
      "Epoch 19/200, Loss: 0.00002451 Test loss: 0.00003073\n",
      "Epoch 20/200, Loss: 0.00002143 Test loss: 0.00002723\n",
      "Epoch 21/200, Loss: 0.00002110 Test loss: 0.00002776\n",
      "Epoch 22/200, Loss: 0.00002015 Test loss: 0.00002749\n",
      "Epoch 23/200, Loss: 0.00002042 Test loss: 0.00002963\n",
      "Epoch 24/200, Loss: 0.00002058 Test loss: 0.00002783\n",
      "Epoch 25/200, Loss: 0.00002048 Test loss: 0.00002917\n",
      "Epoch 26/200, Loss: 0.00002009 Test loss: 0.00002724\n",
      "Epoch 27/200, Loss: 0.00002039 Test loss: 0.00002966\n",
      "Epoch 28/200, Loss: 0.00002343 Test loss: 0.00003105\n",
      "Epoch 29/200, Loss: 0.00002215 Test loss: 0.00003159\n",
      "Epoch 30/200, Loss: 0.00002059 Test loss: 0.00002829\n",
      "Epoch 31/200, Loss: 0.00002057 Test loss: 0.00002861\n",
      "Epoch 32/200, Loss: 0.00002242 Test loss: 0.00003205\n",
      "Epoch 33/200, Loss: 0.00002325 Test loss: 0.00003345\n",
      "Epoch 34/200, Loss: 0.00002687 Test loss: 0.00004323\n",
      "Epoch 35/200, Loss: 0.00002955 Test loss: 0.00003104\n",
      "Epoch 36/200, Loss: 0.00002471 Test loss: 0.00004132\n",
      "Epoch 37/200, Loss: 0.00003051 Test loss: 0.00003926\n",
      "Epoch 38/200, Loss: 0.00002466 Test loss: 0.00003303\n",
      "Epoch 39/200, Loss: 0.00002270 Test loss: 0.00002792\n",
      "Epoch 40/200, Loss: 0.00001994 Test loss: 0.00002673\n",
      "Epoch 41/200, Loss: 0.00001965 Test loss: 0.00002744\n",
      "Epoch 42/200, Loss: 0.00002136 Test loss: 0.00003135\n",
      "Epoch 43/200, Loss: 0.00002101 Test loss: 0.00003151\n",
      "Epoch 44/200, Loss: 0.00002242 Test loss: 0.00003210\n",
      "Epoch 45/200, Loss: 0.00002561 Test loss: 0.00003998\n",
      "Epoch 46/200, Loss: 0.00003595 Test loss: 0.00004377\n",
      "Epoch 47/200, Loss: 0.00002693 Test loss: 0.00002780\n",
      "Epoch 48/200, Loss: 0.00002376 Test loss: 0.00002871\n",
      "Epoch 49/200, Loss: 0.00002098 Test loss: 0.00002888\n",
      "Epoch 50/200, Loss: 0.00001972 Test loss: 0.00002661\n",
      "Epoch 51/200, Loss: 0.00001908 Test loss: 0.00002645\n",
      "Epoch 52/200, Loss: 0.00001953 Test loss: 0.00003035\n",
      "Epoch 53/200, Loss: 0.00002645 Test loss: 0.00003533\n",
      "Epoch 54/200, Loss: 0.00002986 Test loss: 0.00004160\n",
      "Epoch 55/200, Loss: 0.00002775 Test loss: 0.00002952\n",
      "Epoch 56/200, Loss: 0.00002131 Test loss: 0.00002870\n",
      "Epoch 57/200, Loss: 0.00001958 Test loss: 0.00002727\n",
      "Epoch 58/200, Loss: 0.00001960 Test loss: 0.00002800\n",
      "Epoch 59/200, Loss: 0.00002260 Test loss: 0.00003426\n",
      "Epoch 60/200, Loss: 0.00002947 Test loss: 0.00004972\n",
      "Epoch 61/200, Loss: 0.00002630 Test loss: 0.00002921\n",
      "Epoch 62/200, Loss: 0.00002178 Test loss: 0.00002959\n",
      "Epoch 63/200, Loss: 0.00002065 Test loss: 0.00003067\n",
      "Epoch 64/200, Loss: 0.00002113 Test loss: 0.00003122\n",
      "Epoch 65/200, Loss: 0.00002352 Test loss: 0.00003076\n",
      "Epoch 66/200, Loss: 0.00002501 Test loss: 0.00004632\n",
      "Epoch 67/200, Loss: 0.00003790 Test loss: 0.00006093\n",
      "Epoch 68/200, Loss: 0.00003051 Test loss: 0.00003119\n",
      "Epoch 69/200, Loss: 0.00002557 Test loss: 0.00003360\n",
      "Epoch 70/200, Loss: 0.00002058 Test loss: 0.00002735\n",
      "Epoch 71/200, Loss: 0.00001976 Test loss: 0.00002680\n",
      "Epoch 72/200, Loss: 0.00001937 Test loss: 0.00002676\n",
      "Epoch 73/200, Loss: 0.00001896 Test loss: 0.00002643\n",
      "Epoch 74/200, Loss: 0.00001874 Test loss: 0.00002621\n",
      "Epoch 75/200, Loss: 0.00001873 Test loss: 0.00002663\n",
      "Epoch 76/200, Loss: 0.00002328 Test loss: 0.00004029\n",
      "Epoch 77/200, Loss: 0.00002622 Test loss: 0.00003475\n",
      "Epoch 78/200, Loss: 0.00003181 Test loss: 0.00003327\n",
      "Epoch 79/200, Loss: 0.00002678 Test loss: 0.00003509\n",
      "Epoch 80/200, Loss: 0.00002392 Test loss: 0.00002849\n",
      "Epoch 81/200, Loss: 0.00002006 Test loss: 0.00002840\n",
      "Epoch 82/200, Loss: 0.00002308 Test loss: 0.00003242\n",
      "Epoch 83/200, Loss: 0.00002682 Test loss: 0.00003069\n",
      "Epoch 84/200, Loss: 0.00002054 Test loss: 0.00003383\n",
      "Epoch 85/200, Loss: 0.00002290 Test loss: 0.00002765\n",
      "Epoch 86/200, Loss: 0.00001879 Test loss: 0.00002610\n",
      "Epoch 87/200, Loss: 0.00002006 Test loss: 0.00002885\n",
      "Epoch 88/200, Loss: 0.00002233 Test loss: 0.00003355\n",
      "Epoch 89/200, Loss: 0.00002586 Test loss: 0.00004050\n",
      "Epoch 90/200, Loss: 0.00002518 Test loss: 0.00003019\n",
      "Epoch 91/200, Loss: 0.00002064 Test loss: 0.00002786\n",
      "Epoch 92/200, Loss: 0.00002120 Test loss: 0.00003274\n",
      "Epoch 93/200, Loss: 0.00002332 Test loss: 0.00003453\n",
      "Epoch 94/200, Loss: 0.00002467 Test loss: 0.00003456\n",
      "Epoch 95/200, Loss: 0.00002315 Test loss: 0.00003105\n",
      "Epoch 96/200, Loss: 0.00002201 Test loss: 0.00002843\n",
      "Epoch 97/200, Loss: 0.00002048 Test loss: 0.00003048\n",
      "Epoch 98/200, Loss: 0.00001997 Test loss: 0.00002993\n",
      "Epoch 99/200, Loss: 0.00002030 Test loss: 0.00002544\n",
      "Epoch 100/200, Loss: 0.00001853 Test loss: 0.00002558\n",
      "Epoch 101/200, Loss: 0.00001834 Test loss: 0.00002623\n",
      "Epoch 102/200, Loss: 0.00002292 Test loss: 0.00005032\n",
      "Epoch 103/200, Loss: 0.00007244 Test loss: 0.00006554\n",
      "Epoch 104/200, Loss: 0.00003403 Test loss: 0.00005073\n",
      "Epoch 105/200, Loss: 0.00002533 Test loss: 0.00003278\n",
      "Epoch 106/200, Loss: 0.00002172 Test loss: 0.00002854\n",
      "Epoch 107/200, Loss: 0.00001995 Test loss: 0.00002758\n",
      "Epoch 108/200, Loss: 0.00001988 Test loss: 0.00002558\n",
      "Epoch 109/200, Loss: 0.00002016 Test loss: 0.00002589\n",
      "Epoch 110/200, Loss: 0.00001904 Test loss: 0.00002666\n",
      "Epoch 111/200, Loss: 0.00001981 Test loss: 0.00002573\n",
      "Epoch 112/200, Loss: 0.00001923 Test loss: 0.00002507\n",
      "Epoch 113/200, Loss: 0.00001826 Test loss: 0.00002537\n",
      "Epoch 114/200, Loss: 0.00001811 Test loss: 0.00002539\n",
      "Epoch 115/200, Loss: 0.00001811 Test loss: 0.00002694\n",
      "Epoch 116/200, Loss: 0.00002033 Test loss: 0.00002717\n",
      "Epoch 117/200, Loss: 0.00001886 Test loss: 0.00002750\n",
      "Epoch 118/200, Loss: 0.00001856 Test loss: 0.00002780\n",
      "Epoch 119/200, Loss: 0.00002027 Test loss: 0.00003087\n",
      "Epoch 120/200, Loss: 0.00002666 Test loss: 0.00003852\n",
      "Epoch 121/200, Loss: 0.00002378 Test loss: 0.00002606\n",
      "Epoch 122/200, Loss: 0.00001887 Test loss: 0.00002644\n",
      "Epoch 123/200, Loss: 0.00002022 Test loss: 0.00002876\n",
      "Epoch 124/200, Loss: 0.00002281 Test loss: 0.00003277\n",
      "Epoch 125/200, Loss: 0.00002062 Test loss: 0.00003117\n",
      "Epoch 126/200, Loss: 0.00002309 Test loss: 0.00003683\n",
      "Epoch 127/200, Loss: 0.00003552 Test loss: 0.00004481\n",
      "Epoch 128/200, Loss: 0.00002422 Test loss: 0.00002704\n",
      "Epoch 129/200, Loss: 0.00001955 Test loss: 0.00002463\n",
      "Epoch 130/200, Loss: 0.00001849 Test loss: 0.00002484\n",
      "Epoch 131/200, Loss: 0.00001807 Test loss: 0.00002595\n",
      "Epoch 132/200, Loss: 0.00002068 Test loss: 0.00003549\n",
      "Epoch 133/200, Loss: 0.00002854 Test loss: 0.00004041\n",
      "Epoch 134/200, Loss: 0.00002337 Test loss: 0.00003360\n",
      "Epoch 135/200, Loss: 0.00002013 Test loss: 0.00002863\n",
      "Epoch 136/200, Loss: 0.00001957 Test loss: 0.00002877\n",
      "Epoch 137/200, Loss: 0.00002128 Test loss: 0.00003396\n",
      "Epoch 138/200, Loss: 0.00002606 Test loss: 0.00004280\n",
      "Epoch 139/200, Loss: 0.00002930 Test loss: 0.00003168\n",
      "Epoch 140/200, Loss: 0.00002002 Test loss: 0.00002617\n",
      "Epoch 141/200, Loss: 0.00001912 Test loss: 0.00002482\n",
      "Epoch 142/200, Loss: 0.00001807 Test loss: 0.00002556\n",
      "Epoch 143/200, Loss: 0.00001902 Test loss: 0.00002535\n",
      "Epoch 144/200, Loss: 0.00001825 Test loss: 0.00002612\n",
      "Epoch 145/200, Loss: 0.00002063 Test loss: 0.00003017\n",
      "Epoch 146/200, Loss: 0.00002205 Test loss: 0.00003692\n",
      "Epoch 147/200, Loss: 0.00003513 Test loss: 0.00005603\n",
      "Epoch 148/200, Loss: 0.00002982 Test loss: 0.00002864\n",
      "Epoch 149/200, Loss: 0.00001895 Test loss: 0.00002608\n",
      "Epoch 150/200, Loss: 0.00001812 Test loss: 0.00002468\n",
      "Epoch 151/200, Loss: 0.00001801 Test loss: 0.00002612\n",
      "Epoch 152/200, Loss: 0.00001856 Test loss: 0.00002740\n",
      "Epoch 153/200, Loss: 0.00002037 Test loss: 0.00002868\n",
      "Epoch 154/200, Loss: 0.00002638 Test loss: 0.00003060\n",
      "Epoch 155/200, Loss: 0.00002037 Test loss: 0.00002662\n",
      "Epoch 156/200, Loss: 0.00001910 Test loss: 0.00002773\n",
      "Epoch 157/200, Loss: 0.00002036 Test loss: 0.00003145\n",
      "Epoch 158/200, Loss: 0.00002331 Test loss: 0.00003720\n",
      "Epoch 159/200, Loss: 0.00002905 Test loss: 0.00003623\n",
      "Epoch 160/200, Loss: 0.00002254 Test loss: 0.00002476\n",
      "Epoch 161/200, Loss: 0.00002131 Test loss: 0.00002844\n",
      "Epoch 162/200, Loss: 0.00001822 Test loss: 0.00002529\n",
      "Epoch 163/200, Loss: 0.00001820 Test loss: 0.00002491\n",
      "Epoch 164/200, Loss: 0.00001819 Test loss: 0.00002543\n",
      "Epoch 165/200, Loss: 0.00001883 Test loss: 0.00002693\n",
      "Epoch 166/200, Loss: 0.00001929 Test loss: 0.00002905\n",
      "Epoch 167/200, Loss: 0.00002636 Test loss: 0.00004366\n",
      "Epoch 168/200, Loss: 0.00002958 Test loss: 0.00004144\n",
      "Epoch 169/200, Loss: 0.00002841 Test loss: 0.00003211\n",
      "Epoch 170/200, Loss: 0.00001996 Test loss: 0.00002839\n",
      "Epoch 171/200, Loss: 0.00001970 Test loss: 0.00002482\n",
      "Epoch 172/200, Loss: 0.00001768 Test loss: 0.00002420\n",
      "Epoch 173/200, Loss: 0.00001700 Test loss: 0.00002400\n",
      "Epoch 174/200, Loss: 0.00001691 Test loss: 0.00002436\n",
      "Epoch 175/200, Loss: 0.00001734 Test loss: 0.00002519\n",
      "Epoch 176/200, Loss: 0.00001915 Test loss: 0.00003228\n",
      "Epoch 177/200, Loss: 0.00003110 Test loss: 0.00007597\n",
      "Epoch 178/200, Loss: 0.00004219 Test loss: 0.00004388\n",
      "Epoch 179/200, Loss: 0.00002934 Test loss: 0.00003885\n",
      "Epoch 180/200, Loss: 0.00002274 Test loss: 0.00002669\n",
      "Epoch 181/200, Loss: 0.00001966 Test loss: 0.00003137\n",
      "Epoch 182/200, Loss: 0.00002055 Test loss: 0.00002932\n",
      "Epoch 183/200, Loss: 0.00001950 Test loss: 0.00002513\n",
      "Epoch 184/200, Loss: 0.00001820 Test loss: 0.00002452\n",
      "Epoch 185/200, Loss: 0.00001719 Test loss: 0.00002413\n",
      "Epoch 186/200, Loss: 0.00001824 Test loss: 0.00003019\n",
      "Epoch 187/200, Loss: 0.00003100 Test loss: 0.00005505\n",
      "Epoch 188/200, Loss: 0.00002810 Test loss: 0.00002672\n",
      "Epoch 189/200, Loss: 0.00002034 Test loss: 0.00002429\n",
      "Epoch 190/200, Loss: 0.00001814 Test loss: 0.00002399\n",
      "Epoch 191/200, Loss: 0.00001712 Test loss: 0.00002501\n",
      "Epoch 192/200, Loss: 0.00001739 Test loss: 0.00002350\n",
      "Epoch 193/200, Loss: 0.00001716 Test loss: 0.00002545\n",
      "Epoch 194/200, Loss: 0.00001971 Test loss: 0.00002726\n",
      "Epoch 195/200, Loss: 0.00002151 Test loss: 0.00002826\n",
      "Epoch 196/200, Loss: 0.00002010 Test loss: 0.00002328\n",
      "Epoch 197/200, Loss: 0.00001831 Test loss: 0.00002372\n",
      "Epoch 198/200, Loss: 0.00001849 Test loss: 0.00002492\n",
      "Epoch 199/200, Loss: 0.00001968 Test loss: 0.00002732\n",
      "Epoch 200/200, Loss: 0.00001966 Test loss: 0.00002897\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-13T07:27:25.698422Z",
     "start_time": "2025-06-13T07:27:25.675296Z"
    }
   },
   "cell_type": "code",
   "source": "torch.save(model, \"data/models/torch/autoencoder_exp.pt\")",
   "id": "6e5a9e294045c74c",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-13T07:27:26.866427Z",
     "start_time": "2025-06-13T07:27:26.845071Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model = torch.load(\"data/models/torch/autoencoder_exp.pt\", weights_only=False)\n",
    "model.eval()"
   ],
   "id": "f8af0cb897395d7c",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Autoencoder(\n",
       "  (encoder): Sequential(\n",
       "    (0): Linear(in_features=448, out_features=128, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=128, out_features=64, bias=True)\n",
       "    (3): ReLU()\n",
       "    (4): Linear(in_features=64, out_features=16, bias=True)\n",
       "  )\n",
       "  (decoder): Sequential(\n",
       "    (0): Linear(in_features=16, out_features=64, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=64, out_features=128, bias=True)\n",
       "    (3): ReLU()\n",
       "    (4): Linear(in_features=128, out_features=448, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-13T07:27:29.913285Z",
     "start_time": "2025-06-13T07:27:29.811855Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model.eval()\n",
    "latent_vectors_train = model.encoder(torch.tensor(x_train, dtype=torch.float32).to(device)).detach().cpu().numpy()\n",
    "latent_vectors_test = model.encoder(torch.tensor(x_test, dtype=torch.float32).to(device)).detach().cpu().numpy()\n",
    "latent_vectors_test_exp = model.encoder(torch.tensor(x_test_exp, dtype=torch.float32).to(device)).detach().cpu().numpy()"
   ],
   "id": "59307777a8d5284",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-13T07:27:36.477152Z",
     "start_time": "2025-06-13T07:27:35.829286Z"
    }
   },
   "cell_type": "code",
   "source": [
    "np.random.seed(0)\n",
    "labels = None \n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "latent_pca = pca.fit_transform(latent_vectors_train[:100])\n",
    "latent_pca_exp = pca.fit_transform(latent_vectors_test[:100])\n",
    "\n",
    "tsne = TSNE(n_components=2, init='pca', random_state=0)\n",
    "latent_tsne = tsne.fit_transform(latent_vectors_train[:100])\n",
    "latent_tsne_exp = tsne.fit_transform(latent_vectors_test[:100])\n",
    "\n",
    "plt.figure()\n",
    "plt.scatter(latent_pca[:, 0], latent_pca[:, 1], label=\"train\")\n",
    "plt.scatter(latent_pca_exp[:, 0], latent_pca_exp[:, 1], label=\"exp\")\n",
    "plt.title(\"PCA of Latent Space\")\n",
    "plt.xlabel(\"PC1\")\n",
    "plt.ylabel(\"PC2\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "plt.scatter(latent_tsne[:, 0], latent_tsne[:, 1], label=\"train\")\n",
    "plt.scatter(latent_tsne_exp[:, 0], latent_tsne_exp[:, 1], label=\"exp\")\n",
    "for i in range(latent_tsne_exp.shape[0]):\n",
    "        plt.text(latent_tsne_exp[i, 0], latent_tsne_exp[i, 1], str(i), fontsize=8, ha='center', va='center')\n",
    "plt.title(\"t-SNE of Latent Space\")\n",
    "plt.xlabel(\"Dim 1\")\n",
    "plt.ylabel(\"Dim 2\")\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "id": "f578073be61beebe",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\anton\\PycharmProjects\\isotherm\\.venv\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:810: FutureWarning: The default learning rate in TSNE will change from 200.0 to 'auto' in 1.2.\n",
      "  warnings.warn(\n",
      "C:\\Users\\anton\\PycharmProjects\\isotherm\\.venv\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:996: FutureWarning: The PCA initialization in TSNE will change to have the standard deviation of PC1 equal to 1e-4 in 1.2. This will ensure better convergence.\n",
      "  warnings.warn(\n",
      "C:\\Users\\anton\\PycharmProjects\\isotherm\\.venv\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:810: FutureWarning: The default learning rate in TSNE will change from 200.0 to 'auto' in 1.2.\n",
      "  warnings.warn(\n",
      "C:\\Users\\anton\\PycharmProjects\\isotherm\\.venv\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:996: FutureWarning: The PCA initialization in TSNE will change to have the standard deviation of PC1 equal to 1e-4 in 1.2. This will ensure better convergence.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-11T07:48:07.989508Z",
     "start_time": "2025-06-11T07:46:44.526053Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from xgboost import XGBRegressor\n",
    "bst = XGBRegressor(n_estimators=500, max_depth=5)\n",
    "bst.fit(latent_vectors_train, y_train, verbose=True)"
   ],
   "id": "514e7653c7462863",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBRegressor(base_score=None, booster=None, callbacks=None,\n",
       "             colsample_bylevel=None, colsample_bynode=None,\n",
       "             colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
       "             enable_categorical=False, eval_metric=None, feature_types=None,\n",
       "             feature_weights=None, gamma=None, grow_policy=None,\n",
       "             importance_type=None, interaction_constraints=None,\n",
       "             learning_rate=None, max_bin=None, max_cat_threshold=None,\n",
       "             max_cat_to_onehot=None, max_delta_step=None, max_depth=5,\n",
       "             max_leaves=None, min_child_weight=None, missing=nan,\n",
       "             monotone_constraints=None, multi_strategy=None, n_estimators=500,\n",
       "             n_jobs=None, num_parallel_tree=None, ...)"
      ],
      "text/html": [
       "<style>#sk-container-id-1 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: #000;\n",
       "  --sklearn-color-text-muted: #666;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
       "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-icon: #696969;\n",
       "\n",
       "  @media (prefers-color-scheme: dark) {\n",
       "    /* Redefinition of color scheme for dark theme */\n",
       "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
       "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-icon: #878787;\n",
       "  }\n",
       "}\n",
       "\n",
       "#sk-container-id-1 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-1 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: flex;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "  align-items: start;\n",
       "  justify-content: space-between;\n",
       "  gap: 0.5em;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label .caption {\n",
       "  font-size: 0.6rem;\n",
       "  font-weight: lighter;\n",
       "  color: var(--sklearn-color-text-muted);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"▸\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content {\n",
       "  max-height: 0;\n",
       "  max-width: 0;\n",
       "  overflow: hidden;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  max-height: 200px;\n",
       "  max-width: 100%;\n",
       "  overflow: auto;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"▾\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-1 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  display: inline-block;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-1 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-1 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 0.5em;\n",
       "  text-align: center;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-1 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>XGBRegressor(base_score=None, booster=None, callbacks=None,\n",
       "             colsample_bylevel=None, colsample_bynode=None,\n",
       "             colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
       "             enable_categorical=False, eval_metric=None, feature_types=None,\n",
       "             feature_weights=None, gamma=None, grow_policy=None,\n",
       "             importance_type=None, interaction_constraints=None,\n",
       "             learning_rate=None, max_bin=None, max_cat_threshold=None,\n",
       "             max_cat_to_onehot=None, max_delta_step=None, max_depth=5,\n",
       "             max_leaves=None, min_child_weight=None, missing=nan,\n",
       "             monotone_constraints=None, multi_strategy=None, n_estimators=500,\n",
       "             n_jobs=None, num_parallel_tree=None, ...)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow\"><div><div>XGBRegressor</div></div><div><a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://xgboost.readthedocs.io/en/release_3.0.0/python/python_api.html#xgboost.XGBRegressor\">?<span>Documentation for XGBRegressor</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></div></label><div class=\"sk-toggleable__content fitted\"><pre>XGBRegressor(base_score=None, booster=None, callbacks=None,\n",
       "             colsample_bylevel=None, colsample_bynode=None,\n",
       "             colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
       "             enable_categorical=False, eval_metric=None, feature_types=None,\n",
       "             feature_weights=None, gamma=None, grow_policy=None,\n",
       "             importance_type=None, interaction_constraints=None,\n",
       "             learning_rate=None, max_bin=None, max_cat_threshold=None,\n",
       "             max_cat_to_onehot=None, max_delta_step=None, max_depth=5,\n",
       "             max_leaves=None, min_child_weight=None, missing=nan,\n",
       "             monotone_constraints=None, multi_strategy=None, n_estimators=500,\n",
       "             n_jobs=None, num_parallel_tree=None, ...)</pre></div> </div></div></div></div>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-11T07:51:00.983523Z",
     "start_time": "2025-06-11T07:50:59.896512Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model_name = 'forest_exp.ubj'\n",
    "bst.save_model(f\"data/models/torch/{model_name}\")"
   ],
   "id": "90ccd965ffcf79a0",
   "outputs": [],
   "execution_count": 43
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-09T13:44:05.478510Z",
     "start_time": "2025-06-09T13:43:58.027981Z"
    }
   },
   "cell_type": "code",
   "source": [
    "accuracies = []\n",
    "for i in range(1, bst.n_estimators + 1):\n",
    "    y_pred = bst.predict(latent_vectors_test[:500, :], iteration_range=(0, i))\n",
    "    acc = np.sum(np.abs(y_test[:500, :]-y_pred))/len(y_test[:500, :])\n",
    "    accuracies.append(acc)\n",
    "    print(f\"Error using {i} trees: {acc:.4f}\")"
   ],
   "id": "900fdf0ec49da772",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error using 1 trees: 7.6471\n",
      "Error using 2 trees: 5.7654\n",
      "Error using 3 trees: 4.4765\n",
      "Error using 4 trees: 3.5926\n",
      "Error using 5 trees: 2.9841\n",
      "Error using 6 trees: 2.5630\n",
      "Error using 7 trees: 2.2792\n",
      "Error using 8 trees: 2.0820\n",
      "Error using 9 trees: 1.9480\n",
      "Error using 10 trees: 1.8525\n",
      "Error using 11 trees: 1.7820\n",
      "Error using 12 trees: 1.7316\n",
      "Error using 13 trees: 1.6910\n",
      "Error using 14 trees: 1.6570\n",
      "Error using 15 trees: 1.6278\n",
      "Error using 16 trees: 1.6031\n",
      "Error using 17 trees: 1.5842\n",
      "Error using 18 trees: 1.5637\n",
      "Error using 19 trees: 1.5468\n",
      "Error using 20 trees: 1.5317\n",
      "Error using 21 trees: 1.5176\n",
      "Error using 22 trees: 1.5055\n",
      "Error using 23 trees: 1.4927\n",
      "Error using 24 trees: 1.4821\n",
      "Error using 25 trees: 1.4727\n",
      "Error using 26 trees: 1.4621\n",
      "Error using 27 trees: 1.4523\n",
      "Error using 28 trees: 1.4433\n",
      "Error using 29 trees: 1.4357\n",
      "Error using 30 trees: 1.4279\n",
      "Error using 31 trees: 1.4212\n",
      "Error using 32 trees: 1.4145\n",
      "Error using 33 trees: 1.4081\n",
      "Error using 34 trees: 1.4014\n",
      "Error using 35 trees: 1.3950\n",
      "Error using 36 trees: 1.3886\n",
      "Error using 37 trees: 1.3821\n",
      "Error using 38 trees: 1.3762\n",
      "Error using 39 trees: 1.3709\n",
      "Error using 40 trees: 1.3658\n",
      "Error using 41 trees: 1.3614\n",
      "Error using 42 trees: 1.3571\n",
      "Error using 43 trees: 1.3520\n",
      "Error using 44 trees: 1.3480\n",
      "Error using 45 trees: 1.3431\n",
      "Error using 46 trees: 1.3385\n",
      "Error using 47 trees: 1.3345\n",
      "Error using 48 trees: 1.3305\n",
      "Error using 49 trees: 1.3269\n",
      "Error using 50 trees: 1.3231\n",
      "Error using 51 trees: 1.3193\n",
      "Error using 52 trees: 1.3155\n",
      "Error using 53 trees: 1.3115\n",
      "Error using 54 trees: 1.3080\n",
      "Error using 55 trees: 1.3048\n",
      "Error using 56 trees: 1.3013\n",
      "Error using 57 trees: 1.2983\n",
      "Error using 58 trees: 1.2949\n",
      "Error using 59 trees: 1.2915\n",
      "Error using 60 trees: 1.2885\n",
      "Error using 61 trees: 1.2855\n",
      "Error using 62 trees: 1.2830\n",
      "Error using 63 trees: 1.2800\n",
      "Error using 64 trees: 1.2770\n",
      "Error using 65 trees: 1.2746\n",
      "Error using 66 trees: 1.2717\n",
      "Error using 67 trees: 1.2690\n",
      "Error using 68 trees: 1.2666\n",
      "Error using 69 trees: 1.2644\n",
      "Error using 70 trees: 1.2618\n",
      "Error using 71 trees: 1.2599\n",
      "Error using 72 trees: 1.2576\n",
      "Error using 73 trees: 1.2549\n",
      "Error using 74 trees: 1.2525\n",
      "Error using 75 trees: 1.2499\n",
      "Error using 76 trees: 1.2476\n",
      "Error using 77 trees: 1.2451\n",
      "Error using 78 trees: 1.2427\n",
      "Error using 79 trees: 1.2405\n",
      "Error using 80 trees: 1.2387\n",
      "Error using 81 trees: 1.2368\n",
      "Error using 82 trees: 1.2346\n",
      "Error using 83 trees: 1.2329\n",
      "Error using 84 trees: 1.2312\n",
      "Error using 85 trees: 1.2288\n",
      "Error using 86 trees: 1.2270\n",
      "Error using 87 trees: 1.2247\n",
      "Error using 88 trees: 1.2232\n",
      "Error using 89 trees: 1.2214\n",
      "Error using 90 trees: 1.2197\n",
      "Error using 91 trees: 1.2177\n",
      "Error using 92 trees: 1.2162\n",
      "Error using 93 trees: 1.2142\n",
      "Error using 94 trees: 1.2124\n",
      "Error using 95 trees: 1.2107\n",
      "Error using 96 trees: 1.2089\n",
      "Error using 97 trees: 1.2070\n",
      "Error using 98 trees: 1.2053\n",
      "Error using 99 trees: 1.2034\n",
      "Error using 100 trees: 1.2019\n",
      "Error using 101 trees: 1.2006\n",
      "Error using 102 trees: 1.1990\n",
      "Error using 103 trees: 1.1973\n",
      "Error using 104 trees: 1.1958\n",
      "Error using 105 trees: 1.1943\n",
      "Error using 106 trees: 1.1930\n",
      "Error using 107 trees: 1.1914\n",
      "Error using 108 trees: 1.1902\n",
      "Error using 109 trees: 1.1890\n",
      "Error using 110 trees: 1.1876\n",
      "Error using 111 trees: 1.1860\n",
      "Error using 112 trees: 1.1848\n",
      "Error using 113 trees: 1.1834\n",
      "Error using 114 trees: 1.1817\n",
      "Error using 115 trees: 1.1805\n",
      "Error using 116 trees: 1.1795\n",
      "Error using 117 trees: 1.1783\n",
      "Error using 118 trees: 1.1772\n",
      "Error using 119 trees: 1.1760\n",
      "Error using 120 trees: 1.1748\n",
      "Error using 121 trees: 1.1734\n",
      "Error using 122 trees: 1.1724\n",
      "Error using 123 trees: 1.1712\n",
      "Error using 124 trees: 1.1702\n",
      "Error using 125 trees: 1.1691\n",
      "Error using 126 trees: 1.1678\n",
      "Error using 127 trees: 1.1666\n",
      "Error using 128 trees: 1.1651\n",
      "Error using 129 trees: 1.1642\n",
      "Error using 130 trees: 1.1630\n",
      "Error using 131 trees: 1.1621\n",
      "Error using 132 trees: 1.1610\n",
      "Error using 133 trees: 1.1601\n",
      "Error using 134 trees: 1.1590\n",
      "Error using 135 trees: 1.1581\n",
      "Error using 136 trees: 1.1572\n",
      "Error using 137 trees: 1.1560\n",
      "Error using 138 trees: 1.1552\n",
      "Error using 139 trees: 1.1542\n",
      "Error using 140 trees: 1.1532\n",
      "Error using 141 trees: 1.1522\n",
      "Error using 142 trees: 1.1512\n",
      "Error using 143 trees: 1.1503\n",
      "Error using 144 trees: 1.1494\n",
      "Error using 145 trees: 1.1485\n",
      "Error using 146 trees: 1.1475\n",
      "Error using 147 trees: 1.1464\n",
      "Error using 148 trees: 1.1454\n",
      "Error using 149 trees: 1.1445\n",
      "Error using 150 trees: 1.1435\n",
      "Error using 151 trees: 1.1425\n",
      "Error using 152 trees: 1.1417\n",
      "Error using 153 trees: 1.1409\n",
      "Error using 154 trees: 1.1403\n",
      "Error using 155 trees: 1.1393\n",
      "Error using 156 trees: 1.1385\n",
      "Error using 157 trees: 1.1377\n",
      "Error using 158 trees: 1.1366\n",
      "Error using 159 trees: 1.1359\n",
      "Error using 160 trees: 1.1352\n",
      "Error using 161 trees: 1.1346\n",
      "Error using 162 trees: 1.1336\n",
      "Error using 163 trees: 1.1332\n",
      "Error using 164 trees: 1.1324\n",
      "Error using 165 trees: 1.1316\n",
      "Error using 166 trees: 1.1309\n",
      "Error using 167 trees: 1.1303\n",
      "Error using 168 trees: 1.1296\n",
      "Error using 169 trees: 1.1290\n",
      "Error using 170 trees: 1.1283\n",
      "Error using 171 trees: 1.1276\n",
      "Error using 172 trees: 1.1268\n",
      "Error using 173 trees: 1.1262\n",
      "Error using 174 trees: 1.1253\n",
      "Error using 175 trees: 1.1246\n",
      "Error using 176 trees: 1.1238\n",
      "Error using 177 trees: 1.1231\n",
      "Error using 178 trees: 1.1224\n",
      "Error using 179 trees: 1.1215\n",
      "Error using 180 trees: 1.1207\n",
      "Error using 181 trees: 1.1197\n",
      "Error using 182 trees: 1.1189\n",
      "Error using 183 trees: 1.1181\n",
      "Error using 184 trees: 1.1173\n",
      "Error using 185 trees: 1.1166\n",
      "Error using 186 trees: 1.1161\n",
      "Error using 187 trees: 1.1154\n",
      "Error using 188 trees: 1.1149\n",
      "Error using 189 trees: 1.1144\n",
      "Error using 190 trees: 1.1138\n",
      "Error using 191 trees: 1.1132\n",
      "Error using 192 trees: 1.1125\n",
      "Error using 193 trees: 1.1120\n",
      "Error using 194 trees: 1.1116\n",
      "Error using 195 trees: 1.1109\n",
      "Error using 196 trees: 1.1102\n",
      "Error using 197 trees: 1.1096\n",
      "Error using 198 trees: 1.1093\n",
      "Error using 199 trees: 1.1086\n",
      "Error using 200 trees: 1.1080\n",
      "Error using 201 trees: 1.1073\n",
      "Error using 202 trees: 1.1069\n",
      "Error using 203 trees: 1.1063\n",
      "Error using 204 trees: 1.1059\n",
      "Error using 205 trees: 1.1054\n",
      "Error using 206 trees: 1.1049\n",
      "Error using 207 trees: 1.1044\n",
      "Error using 208 trees: 1.1039\n",
      "Error using 209 trees: 1.1034\n",
      "Error using 210 trees: 1.1028\n",
      "Error using 211 trees: 1.1022\n",
      "Error using 212 trees: 1.1017\n",
      "Error using 213 trees: 1.1014\n",
      "Error using 214 trees: 1.1010\n",
      "Error using 215 trees: 1.1007\n",
      "Error using 216 trees: 1.1005\n",
      "Error using 217 trees: 1.1002\n",
      "Error using 218 trees: 1.0996\n",
      "Error using 219 trees: 1.0990\n",
      "Error using 220 trees: 1.0986\n",
      "Error using 221 trees: 1.0979\n",
      "Error using 222 trees: 1.0973\n",
      "Error using 223 trees: 1.0969\n",
      "Error using 224 trees: 1.0963\n",
      "Error using 225 trees: 1.0958\n",
      "Error using 226 trees: 1.0955\n",
      "Error using 227 trees: 1.0953\n",
      "Error using 228 trees: 1.0946\n",
      "Error using 229 trees: 1.0941\n",
      "Error using 230 trees: 1.0936\n",
      "Error using 231 trees: 1.0931\n",
      "Error using 232 trees: 1.0928\n",
      "Error using 233 trees: 1.0925\n",
      "Error using 234 trees: 1.0921\n",
      "Error using 235 trees: 1.0918\n",
      "Error using 236 trees: 1.0914\n",
      "Error using 237 trees: 1.0909\n",
      "Error using 238 trees: 1.0904\n",
      "Error using 239 trees: 1.0902\n",
      "Error using 240 trees: 1.0897\n",
      "Error using 241 trees: 1.0891\n",
      "Error using 242 trees: 1.0886\n",
      "Error using 243 trees: 1.0881\n",
      "Error using 244 trees: 1.0878\n",
      "Error using 245 trees: 1.0871\n",
      "Error using 246 trees: 1.0867\n",
      "Error using 247 trees: 1.0863\n",
      "Error using 248 trees: 1.0860\n",
      "Error using 249 trees: 1.0856\n",
      "Error using 250 trees: 1.0853\n",
      "Error using 251 trees: 1.0848\n",
      "Error using 252 trees: 1.0844\n",
      "Error using 253 trees: 1.0841\n",
      "Error using 254 trees: 1.0837\n",
      "Error using 255 trees: 1.0833\n",
      "Error using 256 trees: 1.0830\n",
      "Error using 257 trees: 1.0828\n",
      "Error using 258 trees: 1.0825\n",
      "Error using 259 trees: 1.0823\n",
      "Error using 260 trees: 1.0820\n",
      "Error using 261 trees: 1.0816\n",
      "Error using 262 trees: 1.0813\n",
      "Error using 263 trees: 1.0808\n",
      "Error using 264 trees: 1.0802\n",
      "Error using 265 trees: 1.0799\n",
      "Error using 266 trees: 1.0796\n",
      "Error using 267 trees: 1.0792\n",
      "Error using 268 trees: 1.0790\n",
      "Error using 269 trees: 1.0787\n",
      "Error using 270 trees: 1.0782\n",
      "Error using 271 trees: 1.0776\n",
      "Error using 272 trees: 1.0774\n",
      "Error using 273 trees: 1.0771\n",
      "Error using 274 trees: 1.0768\n",
      "Error using 275 trees: 1.0763\n",
      "Error using 276 trees: 1.0760\n",
      "Error using 277 trees: 1.0757\n",
      "Error using 278 trees: 1.0755\n",
      "Error using 279 trees: 1.0755\n",
      "Error using 280 trees: 1.0749\n",
      "Error using 281 trees: 1.0746\n",
      "Error using 282 trees: 1.0740\n",
      "Error using 283 trees: 1.0740\n",
      "Error using 284 trees: 1.0737\n",
      "Error using 285 trees: 1.0733\n",
      "Error using 286 trees: 1.0730\n",
      "Error using 287 trees: 1.0727\n",
      "Error using 288 trees: 1.0724\n",
      "Error using 289 trees: 1.0721\n",
      "Error using 290 trees: 1.0717\n",
      "Error using 291 trees: 1.0713\n",
      "Error using 292 trees: 1.0711\n",
      "Error using 293 trees: 1.0709\n",
      "Error using 294 trees: 1.0707\n",
      "Error using 295 trees: 1.0705\n",
      "Error using 296 trees: 1.0702\n",
      "Error using 297 trees: 1.0698\n",
      "Error using 298 trees: 1.0696\n",
      "Error using 299 trees: 1.0693\n",
      "Error using 300 trees: 1.0691\n"
     ]
    }
   ],
   "execution_count": 158
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-09T12:16:27.311899Z",
     "start_time": "2025-06-09T12:16:27.266277Z"
    }
   },
   "cell_type": "code",
   "source": [
    "plt.plot(accuracies, marker=\".\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ],
   "id": "ef1c6a399de52286",
   "outputs": [],
   "execution_count": 34
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-11T08:04:17.671691Z",
     "start_time": "2025-06-11T08:04:17.448616Z"
    }
   },
   "cell_type": "code",
   "source": "preds = bst.predict(latent_vectors_test_exp)",
   "id": "f0c2f20f26efef7d",
   "outputs": [],
   "execution_count": 84
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-11T08:18:58.629236Z",
     "start_time": "2025-06-11T08:18:58.483829Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def plot_preds(x, y, preds): \n",
    "    NX, NY = 3, 4\n",
    "    figure, axis = plt.subplots(NX, NY)\n",
    "    for i in range(NX):\n",
    "        for j in range(NY):\n",
    "            k = np.random.randint(0, len(preds)) \n",
    "            iso_axis = axis[i, j].twiny()\n",
    "            iso_axis.set_xlabel(\"P/P$^0$\",fontsize=8)\n",
    "            iso_axis.plot(pressures[:-10], x[k], label=\"Isotherm\", color = 'green')\n",
    "            kernel = (data_sorb.T[:-10])\n",
    "            iso_axis.plot(pressures[:-10], np.dot(kernel, preds[k][:128]), label=\"Isotherm by model\", color=\"red\")\n",
    "            axis[i, j].set_title(f\"№ {k}\")\n",
    "            axis[i, j].title.set_size(10)\n",
    "            axis[i, j].grid()\n",
    "            axis[i, j].set_xlabel(\"nm\",fontsize=8)\n",
    "            axis[i, j].plot(pore_widths, (preds[k]), marker=\".\", label=f\"Model PSD\")\n",
    "            axis[i, j].plot(pore_widths, y[k], marker=\".\", label=\"PSD\")\n",
    "    plt.subplots_adjust(hspace=0.6, right=0.95, left=0.05, bottom=0.05, top=0.9)\n",
    "    plt.legend()\n",
    "    axis[0, 0].legend()\n",
    "    plt.show()\n",
    "    \n",
    "plot_preds(x_test_exp, y_test_exp, preds)"
   ],
   "id": "a07cbe66d234c80f",
   "outputs": [],
   "execution_count": 127
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-11T07:54:34.696430Z",
     "start_time": "2025-06-11T07:54:34.349935Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from tools import model_tester\n",
    "from inverse import fit_linear\n",
    "\n",
    "error_lst, roughness_lst = model_tester.test_model_predictions(preds, x_test_exp, kernel=data_sorb[:, :-10])\n",
    "kde_x, kde_error, kde_fun = model_tester.calculate_kde_data(error_lst, stop=150)\n",
    "print(\"average error:\", np.mean(error_lst))\n",
    "plt.plot(kde_x, kde_error, label=model_name)\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.plot()"
   ],
   "id": "1316a794fbe60afa",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average error: 19.576227837865584\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 56
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-13T07:27:48.048637Z",
     "start_time": "2025-06-13T07:27:48.031585Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class PSD_model(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(PSD_model, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(64, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(128, output_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(128, output_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(128, output_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        psd = self.model(x)\n",
    "        return psd\n",
    "\n",
    "class Isotherm_PSD_Dataset(Dataset):\n",
    "    def __init__(self, x, y, transform=None):\n",
    "        self.x = torch.tensor(x, dtype=torch.float32).to(device)\n",
    "        self.y = torch.tensor(y, dtype=torch.float32).to(device)\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = self.x[idx]\n",
    "        y = self.y[idx]\n",
    "        if self.transform:\n",
    "            x = self.transform(x)\n",
    "        return x, y\n",
    "\n",
    "train_PSD = Isotherm_PSD_Dataset(latent_vectors_train, (y_train))\n",
    "test_PSD = Isotherm_PSD_Dataset(latent_vectors_test, (y_test))\n",
    "\n",
    "batch_size = 512\n",
    "PSD_loader = DataLoader(train_PSD, batch_size=batch_size, shuffle=True)\n",
    "PSD_loader_test = DataLoader(test_PSD, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "\n",
    "model_PSD = PSD_model(input_dim=latent_dim, output_dim=128)\n",
    "model_PSD.to(device)\n",
    "\n",
    "optimizer = optim.Adam(model_PSD.parameters(), lr=learning_rate)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "def train_PSD_model(model, loader, loader_test):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    total_vloss = 0\n",
    "    for x, y in loader:\n",
    "        optimizer.zero_grad()\n",
    "        y_recon = model(x)\n",
    "        loss = criterion(y_recon, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for x, y in loader_test:\n",
    "            y_recon  = model(x)\n",
    "            vloss = criterion(y_recon, y)\n",
    "            total_vloss += vloss.item()\n",
    "\n",
    "    return total_loss / len(loader.dataset), total_vloss / len(loader_test.dataset)\n"
   ],
   "id": "e5e1fac9c7ab87a1",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-13T07:33:35.474779Z",
     "start_time": "2025-06-13T07:27:49.581773Z"
    }
   },
   "cell_type": "code",
   "source": [
    "epochs = 300\n",
    "for epoch in range(1, epochs+1):\n",
    "    loss, vloss = train_PSD_model(model_PSD, PSD_loader, PSD_loader_test)\n",
    "    if epoch % 1 == 0:\n",
    "        print(f\"Epoch {epoch}/{epochs}, Loss: {loss*100:.8f} Test loss: {vloss*100:.8f}\")"
   ],
   "id": "2fd72aae7361b860",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300, Loss: 0.00278849 Test loss: 0.00155914\n",
      "Epoch 2/300, Loss: 0.00161460 Test loss: 0.00106561\n",
      "Epoch 3/300, Loss: 0.00105126 Test loss: 0.00087033\n",
      "Epoch 4/300, Loss: 0.00088638 Test loss: 0.00070937\n",
      "Epoch 5/300, Loss: 0.00081221 Test loss: 0.00059989\n",
      "Epoch 6/300, Loss: 0.00073967 Test loss: 0.00071905\n",
      "Epoch 7/300, Loss: 0.00067042 Test loss: 0.00063340\n",
      "Epoch 8/300, Loss: 0.00068083 Test loss: 0.00059451\n",
      "Epoch 9/300, Loss: 0.00061890 Test loss: 0.00054444\n",
      "Epoch 10/300, Loss: 0.00055777 Test loss: 0.00052487\n",
      "Epoch 11/300, Loss: 0.00057298 Test loss: 0.00043978\n",
      "Epoch 12/300, Loss: 0.00062566 Test loss: 0.00044594\n",
      "Epoch 13/300, Loss: 0.00057907 Test loss: 0.00040018\n",
      "Epoch 14/300, Loss: 0.00057865 Test loss: 0.00050906\n",
      "Epoch 15/300, Loss: 0.00062882 Test loss: 0.00047552\n",
      "Epoch 16/300, Loss: 0.00051313 Test loss: 0.00037603\n",
      "Epoch 17/300, Loss: 0.00047756 Test loss: 0.00044809\n",
      "Epoch 18/300, Loss: 0.00045021 Test loss: 0.00040530\n",
      "Epoch 19/300, Loss: 0.00048411 Test loss: 0.00035291\n",
      "Epoch 20/300, Loss: 0.00051223 Test loss: 0.00043766\n",
      "Epoch 21/300, Loss: 0.00051934 Test loss: 0.00138806\n",
      "Epoch 22/300, Loss: 0.00055100 Test loss: 0.00038860\n",
      "Epoch 23/300, Loss: 0.00044945 Test loss: 0.00043843\n",
      "Epoch 24/300, Loss: 0.00053403 Test loss: 0.00054462\n",
      "Epoch 25/300, Loss: 0.00047005 Test loss: 0.00040442\n",
      "Epoch 26/300, Loss: 0.00043249 Test loss: 0.00032898\n",
      "Epoch 27/300, Loss: 0.00045904 Test loss: 0.00033876\n",
      "Epoch 28/300, Loss: 0.00044880 Test loss: 0.00044073\n",
      "Epoch 29/300, Loss: 0.00049941 Test loss: 0.00063384\n",
      "Epoch 30/300, Loss: 0.00046695 Test loss: 0.00036036\n",
      "Epoch 31/300, Loss: 0.00045363 Test loss: 0.00033028\n",
      "Epoch 32/300, Loss: 0.00042619 Test loss: 0.00059385\n",
      "Epoch 33/300, Loss: 0.00046363 Test loss: 0.00040056\n",
      "Epoch 34/300, Loss: 0.00041396 Test loss: 0.00045298\n",
      "Epoch 35/300, Loss: 0.00043549 Test loss: 0.00040686\n",
      "Epoch 36/300, Loss: 0.00042578 Test loss: 0.00036829\n",
      "Epoch 37/300, Loss: 0.00047986 Test loss: 0.00039551\n",
      "Epoch 38/300, Loss: 0.00041887 Test loss: 0.00049243\n",
      "Epoch 39/300, Loss: 0.00046792 Test loss: 0.00078090\n",
      "Epoch 40/300, Loss: 0.00045510 Test loss: 0.00060263\n",
      "Epoch 41/300, Loss: 0.00043567 Test loss: 0.00051245\n",
      "Epoch 42/300, Loss: 0.00046522 Test loss: 0.00041396\n",
      "Epoch 43/300, Loss: 0.00042545 Test loss: 0.00033423\n",
      "Epoch 44/300, Loss: 0.00046670 Test loss: 0.00042910\n",
      "Epoch 45/300, Loss: 0.00042658 Test loss: 0.00046300\n",
      "Epoch 46/300, Loss: 0.00042203 Test loss: 0.00029605\n",
      "Epoch 47/300, Loss: 0.00039924 Test loss: 0.00032312\n",
      "Epoch 48/300, Loss: 0.00043431 Test loss: 0.00036457\n",
      "Epoch 49/300, Loss: 0.00039194 Test loss: 0.00030384\n",
      "Epoch 50/300, Loss: 0.00047756 Test loss: 0.00054165\n",
      "Epoch 51/300, Loss: 0.00044599 Test loss: 0.00040705\n",
      "Epoch 52/300, Loss: 0.00037964 Test loss: 0.00032713\n",
      "Epoch 53/300, Loss: 0.00044275 Test loss: 0.00043041\n",
      "Epoch 54/300, Loss: 0.00038513 Test loss: 0.00043515\n",
      "Epoch 55/300, Loss: 0.00039752 Test loss: 0.00035292\n",
      "Epoch 56/300, Loss: 0.00037744 Test loss: 0.00029632\n",
      "Epoch 57/300, Loss: 0.00047685 Test loss: 0.00033115\n",
      "Epoch 58/300, Loss: 0.00040243 Test loss: 0.00034719\n",
      "Epoch 59/300, Loss: 0.00040213 Test loss: 0.00039680\n",
      "Epoch 60/300, Loss: 0.00039219 Test loss: 0.00031106\n",
      "Epoch 61/300, Loss: 0.00047761 Test loss: 0.00050622\n",
      "Epoch 62/300, Loss: 0.00041922 Test loss: 0.00058217\n",
      "Epoch 63/300, Loss: 0.00038805 Test loss: 0.00041919\n",
      "Epoch 64/300, Loss: 0.00038799 Test loss: 0.00049230\n",
      "Epoch 65/300, Loss: 0.00043026 Test loss: 0.00059810\n",
      "Epoch 66/300, Loss: 0.00059989 Test loss: 0.00052359\n",
      "Epoch 67/300, Loss: 0.00043677 Test loss: 0.00087759\n",
      "Epoch 68/300, Loss: 0.00043555 Test loss: 0.00032671\n",
      "Epoch 69/300, Loss: 0.00040601 Test loss: 0.00086064\n",
      "Epoch 70/300, Loss: 0.00070910 Test loss: 0.00038986\n",
      "Epoch 71/300, Loss: 0.00046064 Test loss: 0.00034866\n",
      "Epoch 72/300, Loss: 0.00040798 Test loss: 0.00044758\n",
      "Epoch 73/300, Loss: 0.00041486 Test loss: 0.00035662\n",
      "Epoch 74/300, Loss: 0.00041445 Test loss: 0.00041958\n",
      "Epoch 75/300, Loss: 0.00045647 Test loss: 0.00036870\n",
      "Epoch 76/300, Loss: 0.00049143 Test loss: 0.00047317\n",
      "Epoch 77/300, Loss: 0.00040505 Test loss: 0.00037357\n",
      "Epoch 78/300, Loss: 0.00039596 Test loss: 0.00037518\n",
      "Epoch 79/300, Loss: 0.00041284 Test loss: 0.00085150\n",
      "Epoch 80/300, Loss: 0.00039667 Test loss: 0.00037422\n",
      "Epoch 81/300, Loss: 0.00039052 Test loss: 0.00036135\n",
      "Epoch 82/300, Loss: 0.00040697 Test loss: 0.00035987\n",
      "Epoch 83/300, Loss: 0.00041097 Test loss: 0.00032876\n",
      "Epoch 84/300, Loss: 0.00041452 Test loss: 0.00034094\n",
      "Epoch 85/300, Loss: 0.00036691 Test loss: 0.00039366\n",
      "Epoch 86/300, Loss: 0.00038310 Test loss: 0.00047671\n",
      "Epoch 87/300, Loss: 0.00040077 Test loss: 0.00050029\n",
      "Epoch 88/300, Loss: 0.00039764 Test loss: 0.00057021\n",
      "Epoch 89/300, Loss: 0.00037891 Test loss: 0.00036106\n",
      "Epoch 90/300, Loss: 0.00037761 Test loss: 0.00035700\n",
      "Epoch 91/300, Loss: 0.00034279 Test loss: 0.00032682\n",
      "Epoch 92/300, Loss: 0.00038872 Test loss: 0.00048244\n",
      "Epoch 93/300, Loss: 0.00039218 Test loss: 0.00052572\n",
      "Epoch 94/300, Loss: 0.00037335 Test loss: 0.00038013\n",
      "Epoch 95/300, Loss: 0.00039459 Test loss: 0.00050465\n",
      "Epoch 96/300, Loss: 0.00036450 Test loss: 0.00034786\n",
      "Epoch 97/300, Loss: 0.00037185 Test loss: 0.00036412\n",
      "Epoch 98/300, Loss: 0.00037887 Test loss: 0.00040049\n",
      "Epoch 99/300, Loss: 0.00037494 Test loss: 0.00048978\n",
      "Epoch 100/300, Loss: 0.00036088 Test loss: 0.00045286\n",
      "Epoch 101/300, Loss: 0.00041053 Test loss: 0.00037710\n",
      "Epoch 102/300, Loss: 0.00036614 Test loss: 0.00031348\n",
      "Epoch 103/300, Loss: 0.00034127 Test loss: 0.00033073\n",
      "Epoch 104/300, Loss: 0.00037107 Test loss: 0.00032259\n",
      "Epoch 105/300, Loss: 0.00040433 Test loss: 0.00034902\n",
      "Epoch 106/300, Loss: 0.00041879 Test loss: 0.00035473\n",
      "Epoch 107/300, Loss: 0.00040969 Test loss: 0.00033420\n",
      "Epoch 108/300, Loss: 0.00040169 Test loss: 0.00046572\n",
      "Epoch 109/300, Loss: 0.00055661 Test loss: 0.00041183\n",
      "Epoch 110/300, Loss: 0.00038083 Test loss: 0.00031733\n",
      "Epoch 111/300, Loss: 0.00035442 Test loss: 0.00035186\n",
      "Epoch 112/300, Loss: 0.00035900 Test loss: 0.00037621\n",
      "Epoch 113/300, Loss: 0.00036536 Test loss: 0.00035046\n",
      "Epoch 114/300, Loss: 0.00036099 Test loss: 0.00066664\n",
      "Epoch 115/300, Loss: 0.00038637 Test loss: 0.00040019\n",
      "Epoch 116/300, Loss: 0.00034586 Test loss: 0.00043863\n",
      "Epoch 117/300, Loss: 0.00033913 Test loss: 0.00047997\n",
      "Epoch 118/300, Loss: 0.00035369 Test loss: 0.00052592\n",
      "Epoch 119/300, Loss: 0.00041758 Test loss: 0.00035120\n",
      "Epoch 120/300, Loss: 0.00034758 Test loss: 0.00035980\n",
      "Epoch 121/300, Loss: 0.00036320 Test loss: 0.00046234\n",
      "Epoch 122/300, Loss: 0.00039830 Test loss: 0.00035342\n",
      "Epoch 123/300, Loss: 0.00034047 Test loss: 0.00043108\n",
      "Epoch 124/300, Loss: 0.00033891 Test loss: 0.00039082\n",
      "Epoch 125/300, Loss: 0.00034953 Test loss: 0.00036295\n",
      "Epoch 126/300, Loss: 0.00035700 Test loss: 0.00042694\n",
      "Epoch 127/300, Loss: 0.00034428 Test loss: 0.00031234\n",
      "Epoch 128/300, Loss: 0.00043477 Test loss: 0.00033405\n",
      "Epoch 129/300, Loss: 0.00034440 Test loss: 0.00037268\n",
      "Epoch 130/300, Loss: 0.00035531 Test loss: 0.00035585\n",
      "Epoch 131/300, Loss: 0.00035230 Test loss: 0.00036656\n",
      "Epoch 132/300, Loss: 0.00033941 Test loss: 0.00032015\n",
      "Epoch 133/300, Loss: 0.00036398 Test loss: 0.00045043\n",
      "Epoch 134/300, Loss: 0.00035878 Test loss: 0.00041889\n",
      "Epoch 135/300, Loss: 0.00036972 Test loss: 0.00043809\n",
      "Epoch 136/300, Loss: 0.00035652 Test loss: 0.00034814\n",
      "Epoch 137/300, Loss: 0.00035531 Test loss: 0.00052384\n",
      "Epoch 138/300, Loss: 0.00033640 Test loss: 0.00030064\n",
      "Epoch 139/300, Loss: 0.00034132 Test loss: 0.00042049\n",
      "Epoch 140/300, Loss: 0.00035655 Test loss: 0.00041547\n",
      "Epoch 141/300, Loss: 0.00038357 Test loss: 0.00038918\n",
      "Epoch 142/300, Loss: 0.00032884 Test loss: 0.00032047\n",
      "Epoch 143/300, Loss: 0.00033642 Test loss: 0.00032456\n",
      "Epoch 144/300, Loss: 0.00035023 Test loss: 0.00036617\n",
      "Epoch 145/300, Loss: 0.00033276 Test loss: 0.00043207\n",
      "Epoch 146/300, Loss: 0.00032713 Test loss: 0.00032894\n",
      "Epoch 147/300, Loss: 0.00038937 Test loss: 0.00035827\n",
      "Epoch 148/300, Loss: 0.00033906 Test loss: 0.00030604\n",
      "Epoch 149/300, Loss: 0.00035408 Test loss: 0.00051328\n",
      "Epoch 150/300, Loss: 0.00033782 Test loss: 0.00052645\n",
      "Epoch 151/300, Loss: 0.00034345 Test loss: 0.00051198\n",
      "Epoch 152/300, Loss: 0.00035307 Test loss: 0.00034521\n",
      "Epoch 153/300, Loss: 0.00033410 Test loss: 0.00037518\n",
      "Epoch 154/300, Loss: 0.00031598 Test loss: 0.00034537\n",
      "Epoch 155/300, Loss: 0.00035962 Test loss: 0.00039419\n",
      "Epoch 156/300, Loss: 0.00036341 Test loss: 0.00032984\n",
      "Epoch 157/300, Loss: 0.00032407 Test loss: 0.00038244\n",
      "Epoch 158/300, Loss: 0.00036682 Test loss: 0.00048257\n",
      "Epoch 159/300, Loss: 0.00034063 Test loss: 0.00034363\n",
      "Epoch 160/300, Loss: 0.00032579 Test loss: 0.00032846\n",
      "Epoch 161/300, Loss: 0.00035175 Test loss: 0.00036873\n",
      "Epoch 162/300, Loss: 0.00033741 Test loss: 0.00030904\n",
      "Epoch 163/300, Loss: 0.00033457 Test loss: 0.00039147\n",
      "Epoch 164/300, Loss: 0.00031608 Test loss: 0.00032310\n",
      "Epoch 165/300, Loss: 0.00033394 Test loss: 0.00042155\n",
      "Epoch 166/300, Loss: 0.00036012 Test loss: 0.00034806\n",
      "Epoch 167/300, Loss: 0.00031312 Test loss: 0.00031212\n",
      "Epoch 168/300, Loss: 0.00035522 Test loss: 0.00036796\n",
      "Epoch 169/300, Loss: 0.00035890 Test loss: 0.00035160\n",
      "Epoch 170/300, Loss: 0.00032881 Test loss: 0.00035490\n",
      "Epoch 171/300, Loss: 0.00034327 Test loss: 0.00040574\n",
      "Epoch 172/300, Loss: 0.00036651 Test loss: 0.00035275\n",
      "Epoch 173/300, Loss: 0.00032835 Test loss: 0.00032869\n",
      "Epoch 174/300, Loss: 0.00034345 Test loss: 0.00034246\n",
      "Epoch 175/300, Loss: 0.00032500 Test loss: 0.00047975\n",
      "Epoch 176/300, Loss: 0.00033768 Test loss: 0.00035776\n",
      "Epoch 177/300, Loss: 0.00032840 Test loss: 0.00043559\n",
      "Epoch 178/300, Loss: 0.00033966 Test loss: 0.00040530\n",
      "Epoch 179/300, Loss: 0.00033742 Test loss: 0.00047009\n",
      "Epoch 180/300, Loss: 0.00032902 Test loss: 0.00032267\n",
      "Epoch 181/300, Loss: 0.00032884 Test loss: 0.00041010\n",
      "Epoch 182/300, Loss: 0.00032847 Test loss: 0.00037348\n",
      "Epoch 183/300, Loss: 0.00031882 Test loss: 0.00046130\n",
      "Epoch 184/300, Loss: 0.00033773 Test loss: 0.00054000\n",
      "Epoch 185/300, Loss: 0.00039544 Test loss: 0.00036375\n",
      "Epoch 186/300, Loss: 0.00031270 Test loss: 0.00043260\n",
      "Epoch 187/300, Loss: 0.00033691 Test loss: 0.00048037\n",
      "Epoch 188/300, Loss: 0.00033699 Test loss: 0.00037190\n",
      "Epoch 189/300, Loss: 0.00035957 Test loss: 0.00045815\n",
      "Epoch 190/300, Loss: 0.00032766 Test loss: 0.00036471\n",
      "Epoch 191/300, Loss: 0.00031406 Test loss: 0.00036224\n",
      "Epoch 192/300, Loss: 0.00030802 Test loss: 0.00034704\n",
      "Epoch 193/300, Loss: 0.00032527 Test loss: 0.00042417\n",
      "Epoch 194/300, Loss: 0.00031454 Test loss: 0.00044226\n",
      "Epoch 195/300, Loss: 0.00034852 Test loss: 0.00034573\n",
      "Epoch 196/300, Loss: 0.00034187 Test loss: 0.00043213\n",
      "Epoch 197/300, Loss: 0.00031855 Test loss: 0.00038185\n",
      "Epoch 198/300, Loss: 0.00031993 Test loss: 0.00041620\n",
      "Epoch 199/300, Loss: 0.00031513 Test loss: 0.00031081\n",
      "Epoch 200/300, Loss: 0.00031845 Test loss: 0.00035176\n",
      "Epoch 201/300, Loss: 0.00032623 Test loss: 0.00035953\n",
      "Epoch 202/300, Loss: 0.00040021 Test loss: 0.00070852\n",
      "Epoch 203/300, Loss: 0.00035201 Test loss: 0.00030909\n",
      "Epoch 204/300, Loss: 0.00031944 Test loss: 0.00038545\n",
      "Epoch 205/300, Loss: 0.00032336 Test loss: 0.00048965\n",
      "Epoch 206/300, Loss: 0.00032503 Test loss: 0.00044718\n",
      "Epoch 207/300, Loss: 0.00032798 Test loss: 0.00046321\n",
      "Epoch 208/300, Loss: 0.00032333 Test loss: 0.00038349\n",
      "Epoch 209/300, Loss: 0.00031325 Test loss: 0.00048992\n",
      "Epoch 210/300, Loss: 0.00032738 Test loss: 0.00037805\n",
      "Epoch 211/300, Loss: 0.00031988 Test loss: 0.00047117\n",
      "Epoch 212/300, Loss: 0.00032549 Test loss: 0.00033246\n",
      "Epoch 213/300, Loss: 0.00030747 Test loss: 0.00037229\n",
      "Epoch 214/300, Loss: 0.00033087 Test loss: 0.00044862\n",
      "Epoch 215/300, Loss: 0.00032610 Test loss: 0.00038579\n",
      "Epoch 216/300, Loss: 0.00033474 Test loss: 0.00034879\n",
      "Epoch 217/300, Loss: 0.00034083 Test loss: 0.00038321\n",
      "Epoch 218/300, Loss: 0.00033035 Test loss: 0.00042330\n",
      "Epoch 219/300, Loss: 0.00035462 Test loss: 0.00049615\n",
      "Epoch 220/300, Loss: 0.00032650 Test loss: 0.00040665\n",
      "Epoch 221/300, Loss: 0.00030014 Test loss: 0.00037567\n",
      "Epoch 222/300, Loss: 0.00030249 Test loss: 0.00045007\n",
      "Epoch 223/300, Loss: 0.00032941 Test loss: 0.00040299\n",
      "Epoch 224/300, Loss: 0.00030336 Test loss: 0.00043094\n",
      "Epoch 225/300, Loss: 0.00033319 Test loss: 0.00040349\n",
      "Epoch 226/300, Loss: 0.00030490 Test loss: 0.00047965\n",
      "Epoch 227/300, Loss: 0.00027734 Test loss: 0.00053177\n",
      "Epoch 228/300, Loss: 0.00030476 Test loss: 0.00033078\n",
      "Epoch 229/300, Loss: 0.00027103 Test loss: 0.00030899\n",
      "Epoch 230/300, Loss: 0.00027237 Test loss: 0.00039442\n",
      "Epoch 231/300, Loss: 0.00030151 Test loss: 0.00044901\n",
      "Epoch 232/300, Loss: 0.00033404 Test loss: 0.00035432\n",
      "Epoch 233/300, Loss: 0.00030336 Test loss: 0.00032347\n",
      "Epoch 234/300, Loss: 0.00027504 Test loss: 0.00035510\n",
      "Epoch 235/300, Loss: 0.00027097 Test loss: 0.00038307\n",
      "Epoch 236/300, Loss: 0.00027997 Test loss: 0.00034873\n",
      "Epoch 237/300, Loss: 0.00031076 Test loss: 0.00038032\n",
      "Epoch 238/300, Loss: 0.00027114 Test loss: 0.00032537\n",
      "Epoch 239/300, Loss: 0.00025957 Test loss: 0.00038288\n",
      "Epoch 240/300, Loss: 0.00028686 Test loss: 0.00036296\n",
      "Epoch 241/300, Loss: 0.00028388 Test loss: 0.00042318\n",
      "Epoch 242/300, Loss: 0.00030600 Test loss: 0.00035994\n",
      "Epoch 243/300, Loss: 0.00029494 Test loss: 0.00043692\n",
      "Epoch 244/300, Loss: 0.00026594 Test loss: 0.00036431\n",
      "Epoch 245/300, Loss: 0.00034419 Test loss: 0.00044109\n",
      "Epoch 246/300, Loss: 0.00026918 Test loss: 0.00034434\n",
      "Epoch 247/300, Loss: 0.00026399 Test loss: 0.00034557\n",
      "Epoch 248/300, Loss: 0.00030634 Test loss: 0.00038483\n",
      "Epoch 249/300, Loss: 0.00029431 Test loss: 0.00037089\n",
      "Epoch 250/300, Loss: 0.00026747 Test loss: 0.00037584\n",
      "Epoch 251/300, Loss: 0.00027503 Test loss: 0.00047382\n",
      "Epoch 252/300, Loss: 0.00028557 Test loss: 0.00045837\n",
      "Epoch 253/300, Loss: 0.00028028 Test loss: 0.00034491\n",
      "Epoch 254/300, Loss: 0.00028248 Test loss: 0.00037556\n",
      "Epoch 255/300, Loss: 0.00026790 Test loss: 0.00032908\n",
      "Epoch 256/300, Loss: 0.00029677 Test loss: 0.00048130\n",
      "Epoch 257/300, Loss: 0.00032484 Test loss: 0.00048845\n",
      "Epoch 258/300, Loss: 0.00028604 Test loss: 0.00044887\n",
      "Epoch 259/300, Loss: 0.00030909 Test loss: 0.00037362\n",
      "Epoch 260/300, Loss: 0.00027499 Test loss: 0.00034954\n",
      "Epoch 261/300, Loss: 0.00030229 Test loss: 0.00043026\n",
      "Epoch 262/300, Loss: 0.00027063 Test loss: 0.00062106\n",
      "Epoch 263/300, Loss: 0.00028071 Test loss: 0.00037972\n",
      "Epoch 264/300, Loss: 0.00026116 Test loss: 0.00031815\n",
      "Epoch 265/300, Loss: 0.00026626 Test loss: 0.00046448\n",
      "Epoch 266/300, Loss: 0.00027694 Test loss: 0.00042647\n",
      "Epoch 267/300, Loss: 0.00026391 Test loss: 0.00032621\n",
      "Epoch 268/300, Loss: 0.00026326 Test loss: 0.00027503\n",
      "Epoch 269/300, Loss: 0.00026386 Test loss: 0.00040927\n",
      "Epoch 270/300, Loss: 0.00026210 Test loss: 0.00047583\n",
      "Epoch 271/300, Loss: 0.00026159 Test loss: 0.00040747\n",
      "Epoch 272/300, Loss: 0.00025743 Test loss: 0.00040456\n",
      "Epoch 273/300, Loss: 0.00026719 Test loss: 0.00037494\n",
      "Epoch 274/300, Loss: 0.00028370 Test loss: 0.00048539\n",
      "Epoch 275/300, Loss: 0.00029297 Test loss: 0.00042918\n",
      "Epoch 276/300, Loss: 0.00025926 Test loss: 0.00042159\n",
      "Epoch 277/300, Loss: 0.00026999 Test loss: 0.00047483\n",
      "Epoch 278/300, Loss: 0.00027169 Test loss: 0.00036590\n",
      "Epoch 279/300, Loss: 0.00025721 Test loss: 0.00048385\n",
      "Epoch 280/300, Loss: 0.00026320 Test loss: 0.00033512\n",
      "Epoch 281/300, Loss: 0.00026445 Test loss: 0.00036473\n",
      "Epoch 282/300, Loss: 0.00027403 Test loss: 0.00032723\n",
      "Epoch 283/300, Loss: 0.00026464 Test loss: 0.00056234\n",
      "Epoch 284/300, Loss: 0.00030992 Test loss: 0.00033221\n",
      "Epoch 285/300, Loss: 0.00026134 Test loss: 0.00037478\n",
      "Epoch 286/300, Loss: 0.00025399 Test loss: 0.00040675\n",
      "Epoch 287/300, Loss: 0.00028754 Test loss: 0.00049780\n",
      "Epoch 288/300, Loss: 0.00030001 Test loss: 0.00032755\n",
      "Epoch 289/300, Loss: 0.00029889 Test loss: 0.00047062\n",
      "Epoch 290/300, Loss: 0.00026818 Test loss: 0.00041911\n",
      "Epoch 291/300, Loss: 0.00026205 Test loss: 0.00034708\n",
      "Epoch 292/300, Loss: 0.00025280 Test loss: 0.00040249\n",
      "Epoch 293/300, Loss: 0.00027169 Test loss: 0.00037211\n",
      "Epoch 294/300, Loss: 0.00027283 Test loss: 0.00035962\n",
      "Epoch 295/300, Loss: 0.00026807 Test loss: 0.00044657\n",
      "Epoch 296/300, Loss: 0.00026222 Test loss: 0.00042889\n",
      "Epoch 297/300, Loss: 0.00025790 Test loss: 0.00039932\n",
      "Epoch 298/300, Loss: 0.00027688 Test loss: 0.00042833\n",
      "Epoch 299/300, Loss: 0.00026412 Test loss: 0.00038858\n",
      "Epoch 300/300, Loss: 0.00026703 Test loss: 0.00040601\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-11T08:16:41.652606Z",
     "start_time": "2025-06-11T08:16:41.404047Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model_PSD.eval()\n",
    "y_train_PSD = model_PSD.model(torch.tensor(latent_vectors_train, dtype=torch.float32).to(device)).detach().cpu().numpy()\n",
    "y_test_PSD = model_PSD.model(torch.tensor(latent_vectors_test, dtype=torch.float32).to(device)).detach().cpu().numpy()\n",
    "y_test_exp_PSD = model_PSD.model(torch.tensor(latent_vectors_test_exp, dtype=torch.float32).to(device)).detach().cpu().numpy()"
   ],
   "id": "bd320d01dab9040b",
   "outputs": [],
   "execution_count": 119
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-11T10:36:37.725602Z",
     "start_time": "2025-06-11T10:36:37.498993Z"
    }
   },
   "cell_type": "code",
   "source": [
    "math_psds = [fit_linear(x_test_exp[i], data_sorb[:, :-10], 0).x for i in range(len(x_test_exp))]\n",
    "restored_isotherms = [np.dot(data_sorb[:, :-10].T, psd) for psd in math_psds]"
   ],
   "id": "6c93f1ac56f30964",
   "outputs": [],
   "execution_count": 161
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-11T10:41:18.377742Z",
     "start_time": "2025-06-11T10:41:17.787797Z"
    }
   },
   "cell_type": "code",
   "source": "model_tester.plot_testing_graphs(y_test_exp_PSD, x_test_exp, restored_isotherms, data_sorb[:, :-10].T, model_name)",
   "id": "dbc5b29070bcaec0",
   "outputs": [],
   "execution_count": 167
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-11T10:49:37.489882Z",
     "start_time": "2025-06-11T10:49:37.348345Z"
    }
   },
   "cell_type": "code",
   "source": "plot_preds(x_test_exp, y_test_exp, y_test_exp_PSD)",
   "id": "424bd83f3fe2a9f9",
   "outputs": [],
   "execution_count": 168
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-11T08:17:54.375895Z",
     "start_time": "2025-06-11T08:17:54.328079Z"
    }
   },
   "cell_type": "code",
   "source": [
    "i = np.random.randint(0, len(y_test_exp_PSD))\n",
    "plt.plot(pore_widths, y_test_exp_PSD[i], marker=\".\", label=\"model\")\n",
    "plt.plot(pore_widths, y_test_exp[i], marker=\".\", label=\"exp\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "i"
   ],
   "id": "9e68645ca1b513dd",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1198"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 123
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-11T08:18:01.125634Z",
     "start_time": "2025-06-11T08:18:01.079639Z"
    }
   },
   "cell_type": "code",
   "source": [
    "restored_isotherm = np.dot(data_sorb.T, y_test_exp_PSD[i])\n",
    "plt.plot(pressures, restored_isotherm, marker=\".\", label=\"model\")\n",
    "plt.plot(pressures[:-10], x_test_exp[i], marker=\".\", label=\"exp\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ],
   "id": "f52375e5b6c7afe6",
   "outputs": [],
   "execution_count": 124
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-09T14:25:57.318452Z",
     "start_time": "2025-06-09T14:25:57.305452Z"
    }
   },
   "cell_type": "code",
   "source": "len(y_test_exp_PSD)",
   "id": "6f2f574e2772cef7",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3411"
      ]
     },
     "execution_count": 298,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 298
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "b11eabe3085b3dc5"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
