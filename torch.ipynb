{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-06-18T04:24:37.796303Z",
     "start_time": "2025-06-18T04:24:33.205426Z"
    }
   },
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from datasetLoader import load_dataset\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "from inverse import fit_linear\n",
    "from tools import model_tester\n",
    "%matplotlib qt"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-18T04:24:37.812319Z",
     "start_time": "2025-06-18T04:24:37.799395Z"
    }
   },
   "cell_type": "code",
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ],
   "id": "d322819b15356399",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-18T05:50:38.657495Z",
     "start_time": "2025-06-18T05:50:34.153470Z"
    }
   },
   "cell_type": "code",
   "source": [
    "pore_widths = np.load(\"data/initial kernels/Size_Kernel_Silica_Adsorption.npy\")\n",
    "pressures = np.load(\"data/initial kernels/Pressure_Silica.npy\")\n",
    "with open(\"data/initial kernels/Kernel_Silica_Adsorption.npy\", 'rb') as f:\n",
    "    data_sorb = np.load(f)\n",
    "\n",
    "x, y = load_dataset('data/datasets/silica_random_lognormal.npz')\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.15, random_state=1)\n",
    "\n",
    "x_exp, y_exp = load_dataset('data/datasets/SMP_CUT_ALL_KERNEL.npz')\n",
    "\n",
    "x_train_exp, x_test_exp, y_train_exp, y_test_exp = train_test_split(x_exp, y_exp, test_size=0.15, random_state=1)"
   ],
   "id": "d29956229295484c",
   "outputs": [],
   "execution_count": 191
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-18T05:50:53.567937Z",
     "start_time": "2025-06-18T05:50:53.522088Z"
    }
   },
   "cell_type": "code",
   "source": [
    "i = np.random.randint(0, len(x_train))\n",
    "plt.plot(pore_widths, y_train[i], marker=\".\")\n",
    "plt.grid()\n",
    "plt.show()"
   ],
   "id": "15f9ff24438da3f5",
   "outputs": [],
   "execution_count": 198
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-13T09:02:46.726188Z",
     "start_time": "2025-06-13T09:02:46.680189Z"
    }
   },
   "cell_type": "code",
   "source": [
    "i = np.random.randint(0, len(x_train))\n",
    "plt.plot(pressures[:-10], x_train[i], marker=\".\")\n",
    "plt.grid()\n",
    "plt.show()"
   ],
   "id": "3201b1f78762f4ff",
   "outputs": [],
   "execution_count": 272
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-18T04:24:42.973323Z",
     "start_time": "2025-06-18T04:24:42.902428Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class IsothermDataset(Dataset):\n",
    "    def __init__(self, isotherms, transform=None):\n",
    "        self.data = torch.tensor(isotherms, dtype=torch.float32).to(device)\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = self.data[idx]\n",
    "        if self.transform:\n",
    "            x = self.transform(x)\n",
    "        return x, x\n",
    "\n",
    "dataset = IsothermDataset(np.concatenate((x_train_exp, x_train_exp)))\n",
    "dataset_test = IsothermDataset(np.concatenate((x_test_exp, x_test_exp)))\n",
    "\n",
    "\n",
    "batch_size = 512\n",
    "loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "loader_test = DataLoader(dataset_test, batch_size=batch_size, shuffle=False)"
   ],
   "id": "5cefc1b65fc7fa63",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-18T04:54:45.655766Z",
     "start_time": "2025-06-18T04:54:45.640032Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self, input_dim, latent_dim):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, latent_dim)\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(latent_dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, input_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        z = self.encoder(x)\n",
    "        x_recon = self.decoder(z)\n",
    "        return x_recon, z\n",
    "\n",
    "input_dim = 448\n",
    "latent_dim = 16\n",
    "epochs = 200\n",
    "learning_rate = 1e-3\n",
    "\n",
    "model = Autoencoder(input_dim=input_dim, latent_dim=latent_dim)\n",
    "model.to(device)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "def train_autoencoder(model, loader, loader_test):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    total_vloss = 0\n",
    "    for x, _ in loader:\n",
    "        optimizer.zero_grad()\n",
    "        x_recon, _ = model(x)\n",
    "        loss = criterion(x_recon, x)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for x, _ in loader_test:\n",
    "            x_recon, _  = model(x)\n",
    "            vloss = criterion(x_recon, x)\n",
    "            total_vloss += vloss.item()\n",
    "\n",
    "    return total_loss / len(loader.dataset), total_vloss / len(loader_test.dataset)\n",
    "\n",
    "\n",
    "# sample_z = model.encoder(torch.tensor(isotherms_np[0], dtype=torch.float32))"
   ],
   "id": "c960463d00766e7c",
   "outputs": [],
   "execution_count": 50
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-18T04:55:51.833360Z",
     "start_time": "2025-06-18T04:54:48.833302Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for epoch in range(1, epochs+1):\n",
    "    loss, vloss = train_autoencoder(model, loader,loader_test)\n",
    "    if epoch % 1 == 0:\n",
    "        print(f\"Epoch {epoch}/{epochs}, Loss: {loss*100:.8f} Test loss: {vloss*100:.8f}\")"
   ],
   "id": "6f625ed77c7b7164",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200, Loss: 0.01585801 Test loss: 0.00337168\n",
      "Epoch 2/200, Loss: 0.00245036 Test loss: 0.00104837\n",
      "Epoch 3/200, Loss: 0.00084817 Test loss: 0.00079429\n",
      "Epoch 4/200, Loss: 0.00058515 Test loss: 0.00035496\n",
      "Epoch 5/200, Loss: 0.00021057 Test loss: 0.00016325\n",
      "Epoch 6/200, Loss: 0.00013958 Test loss: 0.00012834\n",
      "Epoch 7/200, Loss: 0.00012029 Test loss: 0.00011407\n",
      "Epoch 8/200, Loss: 0.00010710 Test loss: 0.00009815\n",
      "Epoch 9/200, Loss: 0.00009510 Test loss: 0.00008737\n",
      "Epoch 10/200, Loss: 0.00008981 Test loss: 0.00008279\n",
      "Epoch 11/200, Loss: 0.00008028 Test loss: 0.00007808\n",
      "Epoch 12/200, Loss: 0.00007649 Test loss: 0.00007453\n",
      "Epoch 13/200, Loss: 0.00007496 Test loss: 0.00007341\n",
      "Epoch 14/200, Loss: 0.00006775 Test loss: 0.00006699\n",
      "Epoch 15/200, Loss: 0.00006154 Test loss: 0.00005812\n",
      "Epoch 16/200, Loss: 0.00006018 Test loss: 0.00005725\n",
      "Epoch 17/200, Loss: 0.00005482 Test loss: 0.00005124\n",
      "Epoch 18/200, Loss: 0.00005318 Test loss: 0.00005174\n",
      "Epoch 19/200, Loss: 0.00005124 Test loss: 0.00004790\n",
      "Epoch 20/200, Loss: 0.00005044 Test loss: 0.00004987\n",
      "Epoch 21/200, Loss: 0.00005063 Test loss: 0.00005050\n",
      "Epoch 22/200, Loss: 0.00004707 Test loss: 0.00004400\n",
      "Epoch 23/200, Loss: 0.00004530 Test loss: 0.00004382\n",
      "Epoch 24/200, Loss: 0.00004452 Test loss: 0.00004201\n",
      "Epoch 25/200, Loss: 0.00004406 Test loss: 0.00004526\n",
      "Epoch 26/200, Loss: 0.00004374 Test loss: 0.00004066\n",
      "Epoch 27/200, Loss: 0.00004320 Test loss: 0.00003943\n",
      "Epoch 28/200, Loss: 0.00004085 Test loss: 0.00003797\n",
      "Epoch 29/200, Loss: 0.00004371 Test loss: 0.00003765\n",
      "Epoch 30/200, Loss: 0.00003993 Test loss: 0.00003626\n",
      "Epoch 31/200, Loss: 0.00004088 Test loss: 0.00003516\n",
      "Epoch 32/200, Loss: 0.00003891 Test loss: 0.00003594\n",
      "Epoch 33/200, Loss: 0.00003764 Test loss: 0.00003735\n",
      "Epoch 34/200, Loss: 0.00003856 Test loss: 0.00003447\n",
      "Epoch 35/200, Loss: 0.00003846 Test loss: 0.00003788\n",
      "Epoch 36/200, Loss: 0.00003630 Test loss: 0.00003364\n",
      "Epoch 37/200, Loss: 0.00003771 Test loss: 0.00003783\n",
      "Epoch 38/200, Loss: 0.00003416 Test loss: 0.00004105\n",
      "Epoch 39/200, Loss: 0.00003532 Test loss: 0.00003319\n",
      "Epoch 40/200, Loss: 0.00003292 Test loss: 0.00004908\n",
      "Epoch 41/200, Loss: 0.00003358 Test loss: 0.00002875\n",
      "Epoch 42/200, Loss: 0.00003240 Test loss: 0.00004076\n",
      "Epoch 43/200, Loss: 0.00003296 Test loss: 0.00003092\n",
      "Epoch 44/200, Loss: 0.00002839 Test loss: 0.00002803\n",
      "Epoch 45/200, Loss: 0.00002934 Test loss: 0.00004166\n",
      "Epoch 46/200, Loss: 0.00002961 Test loss: 0.00002820\n",
      "Epoch 47/200, Loss: 0.00003038 Test loss: 0.00002652\n",
      "Epoch 48/200, Loss: 0.00003188 Test loss: 0.00003773\n",
      "Epoch 49/200, Loss: 0.00002743 Test loss: 0.00003451\n",
      "Epoch 50/200, Loss: 0.00002701 Test loss: 0.00002436\n",
      "Epoch 51/200, Loss: 0.00003208 Test loss: 0.00002352\n",
      "Epoch 52/200, Loss: 0.00002427 Test loss: 0.00002655\n",
      "Epoch 53/200, Loss: 0.00002891 Test loss: 0.00002426\n",
      "Epoch 54/200, Loss: 0.00002653 Test loss: 0.00002159\n",
      "Epoch 55/200, Loss: 0.00002732 Test loss: 0.00002614\n",
      "Epoch 56/200, Loss: 0.00002321 Test loss: 0.00002714\n",
      "Epoch 57/200, Loss: 0.00002691 Test loss: 0.00002480\n",
      "Epoch 58/200, Loss: 0.00002495 Test loss: 0.00002116\n",
      "Epoch 59/200, Loss: 0.00002685 Test loss: 0.00002218\n",
      "Epoch 60/200, Loss: 0.00002226 Test loss: 0.00002885\n",
      "Epoch 61/200, Loss: 0.00002319 Test loss: 0.00002191\n",
      "Epoch 62/200, Loss: 0.00002422 Test loss: 0.00001987\n",
      "Epoch 63/200, Loss: 0.00002367 Test loss: 0.00002085\n",
      "Epoch 64/200, Loss: 0.00002304 Test loss: 0.00002149\n",
      "Epoch 65/200, Loss: 0.00002110 Test loss: 0.00003138\n",
      "Epoch 66/200, Loss: 0.00002238 Test loss: 0.00001890\n",
      "Epoch 67/200, Loss: 0.00002385 Test loss: 0.00001715\n",
      "Epoch 68/200, Loss: 0.00001903 Test loss: 0.00001990\n",
      "Epoch 69/200, Loss: 0.00002504 Test loss: 0.00001892\n",
      "Epoch 70/200, Loss: 0.00001811 Test loss: 0.00002068\n",
      "Epoch 71/200, Loss: 0.00002160 Test loss: 0.00001822\n",
      "Epoch 72/200, Loss: 0.00002157 Test loss: 0.00002195\n",
      "Epoch 73/200, Loss: 0.00001952 Test loss: 0.00001635\n",
      "Epoch 74/200, Loss: 0.00001702 Test loss: 0.00002298\n",
      "Epoch 75/200, Loss: 0.00002278 Test loss: 0.00001639\n",
      "Epoch 76/200, Loss: 0.00001969 Test loss: 0.00002124\n",
      "Epoch 77/200, Loss: 0.00001888 Test loss: 0.00001555\n",
      "Epoch 78/200, Loss: 0.00002109 Test loss: 0.00002009\n",
      "Epoch 79/200, Loss: 0.00001731 Test loss: 0.00001725\n",
      "Epoch 80/200, Loss: 0.00001955 Test loss: 0.00002049\n",
      "Epoch 81/200, Loss: 0.00001892 Test loss: 0.00001663\n",
      "Epoch 82/200, Loss: 0.00002087 Test loss: 0.00002290\n",
      "Epoch 83/200, Loss: 0.00001735 Test loss: 0.00001540\n",
      "Epoch 84/200, Loss: 0.00002202 Test loss: 0.00002576\n",
      "Epoch 85/200, Loss: 0.00001618 Test loss: 0.00001535\n",
      "Epoch 86/200, Loss: 0.00002138 Test loss: 0.00001835\n",
      "Epoch 87/200, Loss: 0.00001722 Test loss: 0.00002616\n",
      "Epoch 88/200, Loss: 0.00001636 Test loss: 0.00002103\n",
      "Epoch 89/200, Loss: 0.00001963 Test loss: 0.00002030\n",
      "Epoch 90/200, Loss: 0.00001717 Test loss: 0.00002656\n",
      "Epoch 91/200, Loss: 0.00001663 Test loss: 0.00001641\n",
      "Epoch 92/200, Loss: 0.00001938 Test loss: 0.00001540\n",
      "Epoch 93/200, Loss: 0.00001849 Test loss: 0.00001821\n",
      "Epoch 94/200, Loss: 0.00001570 Test loss: 0.00001516\n",
      "Epoch 95/200, Loss: 0.00001679 Test loss: 0.00001596\n",
      "Epoch 96/200, Loss: 0.00001669 Test loss: 0.00001559\n",
      "Epoch 97/200, Loss: 0.00001783 Test loss: 0.00002310\n",
      "Epoch 98/200, Loss: 0.00001588 Test loss: 0.00001410\n",
      "Epoch 99/200, Loss: 0.00001701 Test loss: 0.00001775\n",
      "Epoch 100/200, Loss: 0.00001717 Test loss: 0.00001498\n",
      "Epoch 101/200, Loss: 0.00001562 Test loss: 0.00001574\n",
      "Epoch 102/200, Loss: 0.00001820 Test loss: 0.00001690\n",
      "Epoch 103/200, Loss: 0.00001557 Test loss: 0.00001814\n",
      "Epoch 104/200, Loss: 0.00001876 Test loss: 0.00001272\n",
      "Epoch 105/200, Loss: 0.00001627 Test loss: 0.00001282\n",
      "Epoch 106/200, Loss: 0.00001418 Test loss: 0.00001347\n",
      "Epoch 107/200, Loss: 0.00001765 Test loss: 0.00001250\n",
      "Epoch 108/200, Loss: 0.00001530 Test loss: 0.00001630\n",
      "Epoch 109/200, Loss: 0.00001577 Test loss: 0.00002600\n",
      "Epoch 110/200, Loss: 0.00001643 Test loss: 0.00001383\n",
      "Epoch 111/200, Loss: 0.00001560 Test loss: 0.00002436\n",
      "Epoch 112/200, Loss: 0.00001571 Test loss: 0.00001236\n",
      "Epoch 113/200, Loss: 0.00001688 Test loss: 0.00001440\n",
      "Epoch 114/200, Loss: 0.00001566 Test loss: 0.00001287\n",
      "Epoch 115/200, Loss: 0.00001817 Test loss: 0.00001302\n",
      "Epoch 116/200, Loss: 0.00001397 Test loss: 0.00001222\n",
      "Epoch 117/200, Loss: 0.00001416 Test loss: 0.00001388\n",
      "Epoch 118/200, Loss: 0.00001448 Test loss: 0.00001516\n",
      "Epoch 119/200, Loss: 0.00001638 Test loss: 0.00003021\n",
      "Epoch 120/200, Loss: 0.00001477 Test loss: 0.00001177\n",
      "Epoch 121/200, Loss: 0.00001911 Test loss: 0.00001137\n",
      "Epoch 122/200, Loss: 0.00001355 Test loss: 0.00001418\n",
      "Epoch 123/200, Loss: 0.00001262 Test loss: 0.00001731\n",
      "Epoch 124/200, Loss: 0.00001538 Test loss: 0.00001334\n",
      "Epoch 125/200, Loss: 0.00001571 Test loss: 0.00001655\n",
      "Epoch 126/200, Loss: 0.00001311 Test loss: 0.00001944\n",
      "Epoch 127/200, Loss: 0.00001567 Test loss: 0.00002917\n",
      "Epoch 128/200, Loss: 0.00001438 Test loss: 0.00002643\n",
      "Epoch 129/200, Loss: 0.00001506 Test loss: 0.00002268\n",
      "Epoch 130/200, Loss: 0.00001396 Test loss: 0.00001212\n",
      "Epoch 131/200, Loss: 0.00001403 Test loss: 0.00001220\n",
      "Epoch 132/200, Loss: 0.00001454 Test loss: 0.00001463\n",
      "Epoch 133/200, Loss: 0.00001507 Test loss: 0.00001894\n",
      "Epoch 134/200, Loss: 0.00001422 Test loss: 0.00001221\n",
      "Epoch 135/200, Loss: 0.00001527 Test loss: 0.00001175\n",
      "Epoch 136/200, Loss: 0.00001394 Test loss: 0.00001705\n",
      "Epoch 137/200, Loss: 0.00001398 Test loss: 0.00001092\n",
      "Epoch 138/200, Loss: 0.00001329 Test loss: 0.00001202\n",
      "Epoch 139/200, Loss: 0.00001546 Test loss: 0.00001102\n",
      "Epoch 140/200, Loss: 0.00001519 Test loss: 0.00001879\n",
      "Epoch 141/200, Loss: 0.00001356 Test loss: 0.00001433\n",
      "Epoch 142/200, Loss: 0.00001437 Test loss: 0.00001351\n",
      "Epoch 143/200, Loss: 0.00001426 Test loss: 0.00001341\n",
      "Epoch 144/200, Loss: 0.00001376 Test loss: 0.00001130\n",
      "Epoch 145/200, Loss: 0.00001329 Test loss: 0.00001135\n",
      "Epoch 146/200, Loss: 0.00001579 Test loss: 0.00001163\n",
      "Epoch 147/200, Loss: 0.00001274 Test loss: 0.00001117\n",
      "Epoch 148/200, Loss: 0.00001355 Test loss: 0.00001175\n",
      "Epoch 149/200, Loss: 0.00001359 Test loss: 0.00001203\n",
      "Epoch 150/200, Loss: 0.00001494 Test loss: 0.00003031\n",
      "Epoch 151/200, Loss: 0.00001283 Test loss: 0.00001111\n",
      "Epoch 152/200, Loss: 0.00001638 Test loss: 0.00001216\n",
      "Epoch 153/200, Loss: 0.00001178 Test loss: 0.00001049\n",
      "Epoch 154/200, Loss: 0.00001429 Test loss: 0.00001207\n",
      "Epoch 155/200, Loss: 0.00001316 Test loss: 0.00001913\n",
      "Epoch 156/200, Loss: 0.00001366 Test loss: 0.00001341\n",
      "Epoch 157/200, Loss: 0.00001439 Test loss: 0.00001682\n",
      "Epoch 158/200, Loss: 0.00001319 Test loss: 0.00001479\n",
      "Epoch 159/200, Loss: 0.00001471 Test loss: 0.00001734\n",
      "Epoch 160/200, Loss: 0.00001291 Test loss: 0.00001066\n",
      "Epoch 161/200, Loss: 0.00001337 Test loss: 0.00001254\n",
      "Epoch 162/200, Loss: 0.00001285 Test loss: 0.00001398\n",
      "Epoch 163/200, Loss: 0.00001298 Test loss: 0.00001608\n",
      "Epoch 164/200, Loss: 0.00001409 Test loss: 0.00001265\n",
      "Epoch 165/200, Loss: 0.00001307 Test loss: 0.00001092\n",
      "Epoch 166/200, Loss: 0.00001403 Test loss: 0.00001094\n",
      "Epoch 167/200, Loss: 0.00001335 Test loss: 0.00001336\n",
      "Epoch 168/200, Loss: 0.00001342 Test loss: 0.00001652\n",
      "Epoch 169/200, Loss: 0.00001349 Test loss: 0.00001127\n",
      "Epoch 170/200, Loss: 0.00001247 Test loss: 0.00001507\n",
      "Epoch 171/200, Loss: 0.00001391 Test loss: 0.00001062\n",
      "Epoch 172/200, Loss: 0.00001428 Test loss: 0.00001134\n",
      "Epoch 173/200, Loss: 0.00001239 Test loss: 0.00001184\n",
      "Epoch 174/200, Loss: 0.00001501 Test loss: 0.00001056\n",
      "Epoch 175/200, Loss: 0.00001227 Test loss: 0.00001242\n",
      "Epoch 176/200, Loss: 0.00001331 Test loss: 0.00001190\n",
      "Epoch 177/200, Loss: 0.00001221 Test loss: 0.00001611\n",
      "Epoch 178/200, Loss: 0.00001293 Test loss: 0.00001113\n",
      "Epoch 179/200, Loss: 0.00001319 Test loss: 0.00001040\n",
      "Epoch 180/200, Loss: 0.00001341 Test loss: 0.00001023\n",
      "Epoch 181/200, Loss: 0.00001145 Test loss: 0.00001051\n",
      "Epoch 182/200, Loss: 0.00001388 Test loss: 0.00001219\n",
      "Epoch 183/200, Loss: 0.00001265 Test loss: 0.00001446\n",
      "Epoch 184/200, Loss: 0.00001240 Test loss: 0.00001157\n",
      "Epoch 185/200, Loss: 0.00001248 Test loss: 0.00001176\n",
      "Epoch 186/200, Loss: 0.00001285 Test loss: 0.00001482\n",
      "Epoch 187/200, Loss: 0.00001189 Test loss: 0.00001083\n",
      "Epoch 188/200, Loss: 0.00001292 Test loss: 0.00001044\n",
      "Epoch 189/200, Loss: 0.00001280 Test loss: 0.00001146\n",
      "Epoch 190/200, Loss: 0.00001475 Test loss: 0.00001859\n",
      "Epoch 191/200, Loss: 0.00001046 Test loss: 0.00000955\n",
      "Epoch 192/200, Loss: 0.00001239 Test loss: 0.00000955\n",
      "Epoch 193/200, Loss: 0.00001167 Test loss: 0.00000925\n",
      "Epoch 194/200, Loss: 0.00001299 Test loss: 0.00001739\n",
      "Epoch 195/200, Loss: 0.00001170 Test loss: 0.00000948\n",
      "Epoch 196/200, Loss: 0.00001164 Test loss: 0.00001657\n",
      "Epoch 197/200, Loss: 0.00001190 Test loss: 0.00001063\n",
      "Epoch 198/200, Loss: 0.00001119 Test loss: 0.00000953\n",
      "Epoch 199/200, Loss: 0.00001192 Test loss: 0.00001283\n",
      "Epoch 200/200, Loss: 0.00001340 Test loss: 0.00001721\n"
     ]
    }
   ],
   "execution_count": 51
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-18T04:55:55.111600Z",
     "start_time": "2025-06-18T04:55:55.095821Z"
    }
   },
   "cell_type": "code",
   "source": "torch.save(model, \"data/models/torch/autoencoder_exp.pt\")",
   "id": "6e5a9e294045c74c",
   "outputs": [],
   "execution_count": 52
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-18T04:55:55.550323Z",
     "start_time": "2025-06-18T04:55:55.518369Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model = torch.load(\"data/models/torch/autoencoder_exp.pt\", weights_only=False)\n",
    "model.eval()"
   ],
   "id": "f8af0cb897395d7c",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Autoencoder(\n",
       "  (encoder): Sequential(\n",
       "    (0): Linear(in_features=448, out_features=128, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=128, out_features=64, bias=True)\n",
       "    (3): ReLU()\n",
       "    (4): Linear(in_features=64, out_features=16, bias=True)\n",
       "  )\n",
       "  (decoder): Sequential(\n",
       "    (0): Linear(in_features=16, out_features=64, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=64, out_features=128, bias=True)\n",
       "    (3): ReLU()\n",
       "    (4): Linear(in_features=128, out_features=448, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 53
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-18T05:51:17.410282Z",
     "start_time": "2025-06-18T05:51:17.316283Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model.eval()\n",
    "latent_vectors_train = model.encoder(torch.tensor(x_train, dtype=torch.float32).to(device)).detach().cpu().numpy()\n",
    "latent_vectors_test = model.encoder(torch.tensor(x_test, dtype=torch.float32).to(device)).detach().cpu().numpy()\n",
    "latent_vectors_test_exp = model.encoder(torch.tensor(x_test_exp, dtype=torch.float32).to(device)).detach().cpu().numpy()"
   ],
   "id": "59307777a8d5284",
   "outputs": [],
   "execution_count": 201
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-18T05:51:20.496843Z",
     "start_time": "2025-06-18T05:51:19.958245Z"
    }
   },
   "cell_type": "code",
   "source": [
    "np.random.seed(0)\n",
    "labels = None \n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "latent_pca = pca.fit_transform(latent_vectors_train[:100])\n",
    "latent_pca_exp = pca.fit_transform(latent_vectors_test[:100])\n",
    "\n",
    "tsne = TSNE(n_components=2, init='pca', random_state=0)\n",
    "latent_tsne = tsne.fit_transform(latent_vectors_train[:100])\n",
    "latent_tsne_exp = tsne.fit_transform(latent_vectors_test[:100])\n",
    "\n",
    "plt.figure()\n",
    "plt.scatter(latent_pca[:, 0], latent_pca[:, 1], label=\"train\")\n",
    "plt.scatter(latent_pca_exp[:, 0], latent_pca_exp[:, 1], label=\"exp\")\n",
    "plt.title(\"PCA of Latent Space\")\n",
    "plt.xlabel(\"PC1\")\n",
    "plt.ylabel(\"PC2\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "plt.scatter(latent_tsne[:, 0], latent_tsne[:, 1], label=\"train\")\n",
    "plt.scatter(latent_tsne_exp[:, 0], latent_tsne_exp[:, 1], label=\"exp\")\n",
    "for i in range(latent_tsne_exp.shape[0]):\n",
    "        plt.text(latent_tsne_exp[i, 0], latent_tsne_exp[i, 1], str(i), fontsize=8, ha='center', va='center')\n",
    "plt.title(\"t-SNE of Latent Space\")\n",
    "plt.xlabel(\"Dim 1\")\n",
    "plt.ylabel(\"Dim 2\")\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "id": "f578073be61beebe",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\anton\\PycharmProjects\\isotherm\\.venv\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:810: FutureWarning: The default learning rate in TSNE will change from 200.0 to 'auto' in 1.2.\n",
      "  warnings.warn(\n",
      "C:\\Users\\anton\\PycharmProjects\\isotherm\\.venv\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:996: FutureWarning: The PCA initialization in TSNE will change to have the standard deviation of PC1 equal to 1e-4 in 1.2. This will ensure better convergence.\n",
      "  warnings.warn(\n",
      "C:\\Users\\anton\\PycharmProjects\\isotherm\\.venv\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:810: FutureWarning: The default learning rate in TSNE will change from 200.0 to 'auto' in 1.2.\n",
      "  warnings.warn(\n",
      "C:\\Users\\anton\\PycharmProjects\\isotherm\\.venv\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:996: FutureWarning: The PCA initialization in TSNE will change to have the standard deviation of PC1 equal to 1e-4 in 1.2. This will ensure better convergence.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "execution_count": 202
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-11T07:48:07.989508Z",
     "start_time": "2025-06-11T07:46:44.526053Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from xgboost import XGBRegressor\n",
    "bst = XGBRegressor(n_estimators=500, max_depth=5)\n",
    "bst.fit(latent_vectors_train, y_train, verbose=True)"
   ],
   "id": "514e7653c7462863",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBRegressor(base_score=None, booster=None, callbacks=None,\n",
       "             colsample_bylevel=None, colsample_bynode=None,\n",
       "             colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
       "             enable_categorical=False, eval_metric=None, feature_types=None,\n",
       "             feature_weights=None, gamma=None, grow_policy=None,\n",
       "             importance_type=None, interaction_constraints=None,\n",
       "             learning_rate=None, max_bin=None, max_cat_threshold=None,\n",
       "             max_cat_to_onehot=None, max_delta_step=None, max_depth=5,\n",
       "             max_leaves=None, min_child_weight=None, missing=nan,\n",
       "             monotone_constraints=None, multi_strategy=None, n_estimators=500,\n",
       "             n_jobs=None, num_parallel_tree=None, ...)"
      ],
      "text/html": [
       "<style>#sk-container-id-1 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: #000;\n",
       "  --sklearn-color-text-muted: #666;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
       "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-icon: #696969;\n",
       "\n",
       "  @media (prefers-color-scheme: dark) {\n",
       "    /* Redefinition of color scheme for dark theme */\n",
       "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
       "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-icon: #878787;\n",
       "  }\n",
       "}\n",
       "\n",
       "#sk-container-id-1 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-1 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: flex;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "  align-items: start;\n",
       "  justify-content: space-between;\n",
       "  gap: 0.5em;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label .caption {\n",
       "  font-size: 0.6rem;\n",
       "  font-weight: lighter;\n",
       "  color: var(--sklearn-color-text-muted);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"▸\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content {\n",
       "  max-height: 0;\n",
       "  max-width: 0;\n",
       "  overflow: hidden;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  max-height: 200px;\n",
       "  max-width: 100%;\n",
       "  overflow: auto;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"▾\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-1 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  display: inline-block;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-1 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-1 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 0.5em;\n",
       "  text-align: center;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-1 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>XGBRegressor(base_score=None, booster=None, callbacks=None,\n",
       "             colsample_bylevel=None, colsample_bynode=None,\n",
       "             colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
       "             enable_categorical=False, eval_metric=None, feature_types=None,\n",
       "             feature_weights=None, gamma=None, grow_policy=None,\n",
       "             importance_type=None, interaction_constraints=None,\n",
       "             learning_rate=None, max_bin=None, max_cat_threshold=None,\n",
       "             max_cat_to_onehot=None, max_delta_step=None, max_depth=5,\n",
       "             max_leaves=None, min_child_weight=None, missing=nan,\n",
       "             monotone_constraints=None, multi_strategy=None, n_estimators=500,\n",
       "             n_jobs=None, num_parallel_tree=None, ...)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow\"><div><div>XGBRegressor</div></div><div><a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://xgboost.readthedocs.io/en/release_3.0.0/python/python_api.html#xgboost.XGBRegressor\">?<span>Documentation for XGBRegressor</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></div></label><div class=\"sk-toggleable__content fitted\"><pre>XGBRegressor(base_score=None, booster=None, callbacks=None,\n",
       "             colsample_bylevel=None, colsample_bynode=None,\n",
       "             colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
       "             enable_categorical=False, eval_metric=None, feature_types=None,\n",
       "             feature_weights=None, gamma=None, grow_policy=None,\n",
       "             importance_type=None, interaction_constraints=None,\n",
       "             learning_rate=None, max_bin=None, max_cat_threshold=None,\n",
       "             max_cat_to_onehot=None, max_delta_step=None, max_depth=5,\n",
       "             max_leaves=None, min_child_weight=None, missing=nan,\n",
       "             monotone_constraints=None, multi_strategy=None, n_estimators=500,\n",
       "             n_jobs=None, num_parallel_tree=None, ...)</pre></div> </div></div></div></div>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-11T07:51:00.983523Z",
     "start_time": "2025-06-11T07:50:59.896512Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model_name = 'forest_exp.ubj'\n",
    "bst.save_model(f\"data/models/torch/{model_name}\")"
   ],
   "id": "90ccd965ffcf79a0",
   "outputs": [],
   "execution_count": 43
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-09T13:44:05.478510Z",
     "start_time": "2025-06-09T13:43:58.027981Z"
    }
   },
   "cell_type": "code",
   "source": [
    "accuracies = []\n",
    "for i in range(1, bst.n_estimators + 1):\n",
    "    y_pred = bst.predict(latent_vectors_test[:500, :], iteration_range=(0, i))\n",
    "    acc = np.sum(np.abs(y_test[:500, :]-y_pred))/len(y_test[:500, :])\n",
    "    accuracies.append(acc)\n",
    "    print(f\"Error using {i} trees: {acc:.4f}\")"
   ],
   "id": "900fdf0ec49da772",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error using 1 trees: 7.6471\n",
      "Error using 2 trees: 5.7654\n",
      "Error using 3 trees: 4.4765\n",
      "Error using 4 trees: 3.5926\n",
      "Error using 5 trees: 2.9841\n",
      "Error using 6 trees: 2.5630\n",
      "Error using 7 trees: 2.2792\n",
      "Error using 8 trees: 2.0820\n",
      "Error using 9 trees: 1.9480\n",
      "Error using 10 trees: 1.8525\n",
      "Error using 11 trees: 1.7820\n",
      "Error using 12 trees: 1.7316\n",
      "Error using 13 trees: 1.6910\n",
      "Error using 14 trees: 1.6570\n",
      "Error using 15 trees: 1.6278\n",
      "Error using 16 trees: 1.6031\n",
      "Error using 17 trees: 1.5842\n",
      "Error using 18 trees: 1.5637\n",
      "Error using 19 trees: 1.5468\n",
      "Error using 20 trees: 1.5317\n",
      "Error using 21 trees: 1.5176\n",
      "Error using 22 trees: 1.5055\n",
      "Error using 23 trees: 1.4927\n",
      "Error using 24 trees: 1.4821\n",
      "Error using 25 trees: 1.4727\n",
      "Error using 26 trees: 1.4621\n",
      "Error using 27 trees: 1.4523\n",
      "Error using 28 trees: 1.4433\n",
      "Error using 29 trees: 1.4357\n",
      "Error using 30 trees: 1.4279\n",
      "Error using 31 trees: 1.4212\n",
      "Error using 32 trees: 1.4145\n",
      "Error using 33 trees: 1.4081\n",
      "Error using 34 trees: 1.4014\n",
      "Error using 35 trees: 1.3950\n",
      "Error using 36 trees: 1.3886\n",
      "Error using 37 trees: 1.3821\n",
      "Error using 38 trees: 1.3762\n",
      "Error using 39 trees: 1.3709\n",
      "Error using 40 trees: 1.3658\n",
      "Error using 41 trees: 1.3614\n",
      "Error using 42 trees: 1.3571\n",
      "Error using 43 trees: 1.3520\n",
      "Error using 44 trees: 1.3480\n",
      "Error using 45 trees: 1.3431\n",
      "Error using 46 trees: 1.3385\n",
      "Error using 47 trees: 1.3345\n",
      "Error using 48 trees: 1.3305\n",
      "Error using 49 trees: 1.3269\n",
      "Error using 50 trees: 1.3231\n",
      "Error using 51 trees: 1.3193\n",
      "Error using 52 trees: 1.3155\n",
      "Error using 53 trees: 1.3115\n",
      "Error using 54 trees: 1.3080\n",
      "Error using 55 trees: 1.3048\n",
      "Error using 56 trees: 1.3013\n",
      "Error using 57 trees: 1.2983\n",
      "Error using 58 trees: 1.2949\n",
      "Error using 59 trees: 1.2915\n",
      "Error using 60 trees: 1.2885\n",
      "Error using 61 trees: 1.2855\n",
      "Error using 62 trees: 1.2830\n",
      "Error using 63 trees: 1.2800\n",
      "Error using 64 trees: 1.2770\n",
      "Error using 65 trees: 1.2746\n",
      "Error using 66 trees: 1.2717\n",
      "Error using 67 trees: 1.2690\n",
      "Error using 68 trees: 1.2666\n",
      "Error using 69 trees: 1.2644\n",
      "Error using 70 trees: 1.2618\n",
      "Error using 71 trees: 1.2599\n",
      "Error using 72 trees: 1.2576\n",
      "Error using 73 trees: 1.2549\n",
      "Error using 74 trees: 1.2525\n",
      "Error using 75 trees: 1.2499\n",
      "Error using 76 trees: 1.2476\n",
      "Error using 77 trees: 1.2451\n",
      "Error using 78 trees: 1.2427\n",
      "Error using 79 trees: 1.2405\n",
      "Error using 80 trees: 1.2387\n",
      "Error using 81 trees: 1.2368\n",
      "Error using 82 trees: 1.2346\n",
      "Error using 83 trees: 1.2329\n",
      "Error using 84 trees: 1.2312\n",
      "Error using 85 trees: 1.2288\n",
      "Error using 86 trees: 1.2270\n",
      "Error using 87 trees: 1.2247\n",
      "Error using 88 trees: 1.2232\n",
      "Error using 89 trees: 1.2214\n",
      "Error using 90 trees: 1.2197\n",
      "Error using 91 trees: 1.2177\n",
      "Error using 92 trees: 1.2162\n",
      "Error using 93 trees: 1.2142\n",
      "Error using 94 trees: 1.2124\n",
      "Error using 95 trees: 1.2107\n",
      "Error using 96 trees: 1.2089\n",
      "Error using 97 trees: 1.2070\n",
      "Error using 98 trees: 1.2053\n",
      "Error using 99 trees: 1.2034\n",
      "Error using 100 trees: 1.2019\n",
      "Error using 101 trees: 1.2006\n",
      "Error using 102 trees: 1.1990\n",
      "Error using 103 trees: 1.1973\n",
      "Error using 104 trees: 1.1958\n",
      "Error using 105 trees: 1.1943\n",
      "Error using 106 trees: 1.1930\n",
      "Error using 107 trees: 1.1914\n",
      "Error using 108 trees: 1.1902\n",
      "Error using 109 trees: 1.1890\n",
      "Error using 110 trees: 1.1876\n",
      "Error using 111 trees: 1.1860\n",
      "Error using 112 trees: 1.1848\n",
      "Error using 113 trees: 1.1834\n",
      "Error using 114 trees: 1.1817\n",
      "Error using 115 trees: 1.1805\n",
      "Error using 116 trees: 1.1795\n",
      "Error using 117 trees: 1.1783\n",
      "Error using 118 trees: 1.1772\n",
      "Error using 119 trees: 1.1760\n",
      "Error using 120 trees: 1.1748\n",
      "Error using 121 trees: 1.1734\n",
      "Error using 122 trees: 1.1724\n",
      "Error using 123 trees: 1.1712\n",
      "Error using 124 trees: 1.1702\n",
      "Error using 125 trees: 1.1691\n",
      "Error using 126 trees: 1.1678\n",
      "Error using 127 trees: 1.1666\n",
      "Error using 128 trees: 1.1651\n",
      "Error using 129 trees: 1.1642\n",
      "Error using 130 trees: 1.1630\n",
      "Error using 131 trees: 1.1621\n",
      "Error using 132 trees: 1.1610\n",
      "Error using 133 trees: 1.1601\n",
      "Error using 134 trees: 1.1590\n",
      "Error using 135 trees: 1.1581\n",
      "Error using 136 trees: 1.1572\n",
      "Error using 137 trees: 1.1560\n",
      "Error using 138 trees: 1.1552\n",
      "Error using 139 trees: 1.1542\n",
      "Error using 140 trees: 1.1532\n",
      "Error using 141 trees: 1.1522\n",
      "Error using 142 trees: 1.1512\n",
      "Error using 143 trees: 1.1503\n",
      "Error using 144 trees: 1.1494\n",
      "Error using 145 trees: 1.1485\n",
      "Error using 146 trees: 1.1475\n",
      "Error using 147 trees: 1.1464\n",
      "Error using 148 trees: 1.1454\n",
      "Error using 149 trees: 1.1445\n",
      "Error using 150 trees: 1.1435\n",
      "Error using 151 trees: 1.1425\n",
      "Error using 152 trees: 1.1417\n",
      "Error using 153 trees: 1.1409\n",
      "Error using 154 trees: 1.1403\n",
      "Error using 155 trees: 1.1393\n",
      "Error using 156 trees: 1.1385\n",
      "Error using 157 trees: 1.1377\n",
      "Error using 158 trees: 1.1366\n",
      "Error using 159 trees: 1.1359\n",
      "Error using 160 trees: 1.1352\n",
      "Error using 161 trees: 1.1346\n",
      "Error using 162 trees: 1.1336\n",
      "Error using 163 trees: 1.1332\n",
      "Error using 164 trees: 1.1324\n",
      "Error using 165 trees: 1.1316\n",
      "Error using 166 trees: 1.1309\n",
      "Error using 167 trees: 1.1303\n",
      "Error using 168 trees: 1.1296\n",
      "Error using 169 trees: 1.1290\n",
      "Error using 170 trees: 1.1283\n",
      "Error using 171 trees: 1.1276\n",
      "Error using 172 trees: 1.1268\n",
      "Error using 173 trees: 1.1262\n",
      "Error using 174 trees: 1.1253\n",
      "Error using 175 trees: 1.1246\n",
      "Error using 176 trees: 1.1238\n",
      "Error using 177 trees: 1.1231\n",
      "Error using 178 trees: 1.1224\n",
      "Error using 179 trees: 1.1215\n",
      "Error using 180 trees: 1.1207\n",
      "Error using 181 trees: 1.1197\n",
      "Error using 182 trees: 1.1189\n",
      "Error using 183 trees: 1.1181\n",
      "Error using 184 trees: 1.1173\n",
      "Error using 185 trees: 1.1166\n",
      "Error using 186 trees: 1.1161\n",
      "Error using 187 trees: 1.1154\n",
      "Error using 188 trees: 1.1149\n",
      "Error using 189 trees: 1.1144\n",
      "Error using 190 trees: 1.1138\n",
      "Error using 191 trees: 1.1132\n",
      "Error using 192 trees: 1.1125\n",
      "Error using 193 trees: 1.1120\n",
      "Error using 194 trees: 1.1116\n",
      "Error using 195 trees: 1.1109\n",
      "Error using 196 trees: 1.1102\n",
      "Error using 197 trees: 1.1096\n",
      "Error using 198 trees: 1.1093\n",
      "Error using 199 trees: 1.1086\n",
      "Error using 200 trees: 1.1080\n",
      "Error using 201 trees: 1.1073\n",
      "Error using 202 trees: 1.1069\n",
      "Error using 203 trees: 1.1063\n",
      "Error using 204 trees: 1.1059\n",
      "Error using 205 trees: 1.1054\n",
      "Error using 206 trees: 1.1049\n",
      "Error using 207 trees: 1.1044\n",
      "Error using 208 trees: 1.1039\n",
      "Error using 209 trees: 1.1034\n",
      "Error using 210 trees: 1.1028\n",
      "Error using 211 trees: 1.1022\n",
      "Error using 212 trees: 1.1017\n",
      "Error using 213 trees: 1.1014\n",
      "Error using 214 trees: 1.1010\n",
      "Error using 215 trees: 1.1007\n",
      "Error using 216 trees: 1.1005\n",
      "Error using 217 trees: 1.1002\n",
      "Error using 218 trees: 1.0996\n",
      "Error using 219 trees: 1.0990\n",
      "Error using 220 trees: 1.0986\n",
      "Error using 221 trees: 1.0979\n",
      "Error using 222 trees: 1.0973\n",
      "Error using 223 trees: 1.0969\n",
      "Error using 224 trees: 1.0963\n",
      "Error using 225 trees: 1.0958\n",
      "Error using 226 trees: 1.0955\n",
      "Error using 227 trees: 1.0953\n",
      "Error using 228 trees: 1.0946\n",
      "Error using 229 trees: 1.0941\n",
      "Error using 230 trees: 1.0936\n",
      "Error using 231 trees: 1.0931\n",
      "Error using 232 trees: 1.0928\n",
      "Error using 233 trees: 1.0925\n",
      "Error using 234 trees: 1.0921\n",
      "Error using 235 trees: 1.0918\n",
      "Error using 236 trees: 1.0914\n",
      "Error using 237 trees: 1.0909\n",
      "Error using 238 trees: 1.0904\n",
      "Error using 239 trees: 1.0902\n",
      "Error using 240 trees: 1.0897\n",
      "Error using 241 trees: 1.0891\n",
      "Error using 242 trees: 1.0886\n",
      "Error using 243 trees: 1.0881\n",
      "Error using 244 trees: 1.0878\n",
      "Error using 245 trees: 1.0871\n",
      "Error using 246 trees: 1.0867\n",
      "Error using 247 trees: 1.0863\n",
      "Error using 248 trees: 1.0860\n",
      "Error using 249 trees: 1.0856\n",
      "Error using 250 trees: 1.0853\n",
      "Error using 251 trees: 1.0848\n",
      "Error using 252 trees: 1.0844\n",
      "Error using 253 trees: 1.0841\n",
      "Error using 254 trees: 1.0837\n",
      "Error using 255 trees: 1.0833\n",
      "Error using 256 trees: 1.0830\n",
      "Error using 257 trees: 1.0828\n",
      "Error using 258 trees: 1.0825\n",
      "Error using 259 trees: 1.0823\n",
      "Error using 260 trees: 1.0820\n",
      "Error using 261 trees: 1.0816\n",
      "Error using 262 trees: 1.0813\n",
      "Error using 263 trees: 1.0808\n",
      "Error using 264 trees: 1.0802\n",
      "Error using 265 trees: 1.0799\n",
      "Error using 266 trees: 1.0796\n",
      "Error using 267 trees: 1.0792\n",
      "Error using 268 trees: 1.0790\n",
      "Error using 269 trees: 1.0787\n",
      "Error using 270 trees: 1.0782\n",
      "Error using 271 trees: 1.0776\n",
      "Error using 272 trees: 1.0774\n",
      "Error using 273 trees: 1.0771\n",
      "Error using 274 trees: 1.0768\n",
      "Error using 275 trees: 1.0763\n",
      "Error using 276 trees: 1.0760\n",
      "Error using 277 trees: 1.0757\n",
      "Error using 278 trees: 1.0755\n",
      "Error using 279 trees: 1.0755\n",
      "Error using 280 trees: 1.0749\n",
      "Error using 281 trees: 1.0746\n",
      "Error using 282 trees: 1.0740\n",
      "Error using 283 trees: 1.0740\n",
      "Error using 284 trees: 1.0737\n",
      "Error using 285 trees: 1.0733\n",
      "Error using 286 trees: 1.0730\n",
      "Error using 287 trees: 1.0727\n",
      "Error using 288 trees: 1.0724\n",
      "Error using 289 trees: 1.0721\n",
      "Error using 290 trees: 1.0717\n",
      "Error using 291 trees: 1.0713\n",
      "Error using 292 trees: 1.0711\n",
      "Error using 293 trees: 1.0709\n",
      "Error using 294 trees: 1.0707\n",
      "Error using 295 trees: 1.0705\n",
      "Error using 296 trees: 1.0702\n",
      "Error using 297 trees: 1.0698\n",
      "Error using 298 trees: 1.0696\n",
      "Error using 299 trees: 1.0693\n",
      "Error using 300 trees: 1.0691\n"
     ]
    }
   ],
   "execution_count": 158
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-09T12:16:27.311899Z",
     "start_time": "2025-06-09T12:16:27.266277Z"
    }
   },
   "cell_type": "code",
   "source": [
    "plt.plot(accuracies, marker=\".\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ],
   "id": "ef1c6a399de52286",
   "outputs": [],
   "execution_count": 34
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-18T04:26:01.151259Z",
     "start_time": "2025-06-18T04:26:01.119586Z"
    }
   },
   "cell_type": "code",
   "source": "preds = bst.predict(latent_vectors_test_exp)",
   "id": "f0c2f20f26efef7d",
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'bst' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[12], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m preds \u001B[38;5;241m=\u001B[39m \u001B[43mbst\u001B[49m\u001B[38;5;241m.\u001B[39mpredict(latent_vectors_test_exp)\n",
      "\u001B[1;31mNameError\u001B[0m: name 'bst' is not defined"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-18T04:30:23.866421Z",
     "start_time": "2025-06-18T04:30:23.850381Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def plot_preds(x, y, preds): \n",
    "    NX, NY = 3, 4\n",
    "    figure, axis = plt.subplots(NX, NY)\n",
    "    for i in range(NX):\n",
    "        for j in range(NY):\n",
    "            k = np.random.randint(0, len(preds)) \n",
    "            iso_axis = axis[i, j].twiny()\n",
    "            iso_axis.set_xlabel(\"P/P$^0$\",fontsize=8)\n",
    "            iso_axis.plot(pressures[:-10], x[k], label=\"Isotherm\", color = 'green')\n",
    "            kernel = (data_sorb.T[:-10])\n",
    "            iso_axis.plot(pressures[:-10], np.dot(kernel, preds[k][:128]), label=\"Isotherm by model\", color=\"red\")\n",
    "            axis[i, j].set_title(f\"№ {k}\")\n",
    "            axis[i, j].title.set_size(10)\n",
    "            axis[i, j].grid()\n",
    "            axis[i, j].set_xlabel(\"nm\",fontsize=8)\n",
    "            axis[i, j].plot(pore_widths, (preds[k]), marker=\".\", label=f\"Model PSD\")\n",
    "            axis[i, j].plot(pore_widths, y[k], marker=\".\", label=\"PSD\")\n",
    "    plt.subplots_adjust(hspace=0.6, right=0.95, left=0.05, bottom=0.05, top=0.9)\n",
    "    plt.legend()\n",
    "    axis[0, 0].legend()\n",
    "    plt.show()"
   ],
   "id": "a07cbe66d234c80f",
   "outputs": [],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-11T07:54:34.696430Z",
     "start_time": "2025-06-11T07:54:34.349935Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from tools import model_tester\n",
    "from inverse import fit_linear\n",
    "\n",
    "error_lst, roughness_lst = model_tester.test_model_predictions(preds, x_test_exp, kernel=data_sorb[:, :-10])\n",
    "kde_x, kde_error, kde_fun = model_tester.calculate_kde_data(error_lst, stop=150)\n",
    "print(\"average error:\", np.mean(error_lst))\n",
    "plt.plot(kde_x, kde_error, label=model_name)\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.plot()"
   ],
   "id": "1316a794fbe60afa",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average error: 19.576227837865584\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 56
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-18T05:51:31.740233Z",
     "start_time": "2025-06-18T05:51:31.709229Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class PSD_model(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(PSD_model, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_dim, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(32, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(64, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(128, output_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        psd = self.model(x)\n",
    "        return psd\n",
    "\n",
    "class Isotherm_PSD_Dataset(Dataset):\n",
    "    def __init__(self, x, y, transform=None):\n",
    "        self.x = torch.tensor(x, dtype=torch.float32).to(device)\n",
    "        self.y = torch.tensor(y, dtype=torch.float32).to(device)\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = self.x[idx]\n",
    "        y = self.y[idx]\n",
    "        if self.transform:\n",
    "            x = self.transform(x)\n",
    "        return x, y\n",
    "\n",
    "train_PSD = Isotherm_PSD_Dataset(latent_vectors_train, (y_train))\n",
    "test_PSD = Isotherm_PSD_Dataset(latent_vectors_test, (y_test))\n",
    "\n",
    "batch_size = 128\n",
    "PSD_loader = DataLoader(train_PSD, batch_size=batch_size, shuffle=True)\n",
    "PSD_loader_test = DataLoader(test_PSD, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "\n",
    "model_PSD = PSD_model(input_dim=latent_dim, output_dim=128)\n",
    "model_PSD.to(device)\n",
    "\n",
    "optimizer = optim.Adam(model_PSD.parameters(), lr=learning_rate)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "def train_PSD_model(model, loader, loader_test):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    total_vloss = 0\n",
    "    for x, y in loader:\n",
    "        optimizer.zero_grad()\n",
    "        y_recon = model(x)\n",
    "        loss = criterion(y_recon, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for x, y in loader_test:\n",
    "            y_recon  = model(x)\n",
    "            vloss = criterion(y_recon, y)\n",
    "            total_vloss += vloss.item()\n",
    "\n",
    "    return total_loss / len(loader.dataset), total_vloss / len(loader_test.dataset)\n"
   ],
   "id": "e5e1fac9c7ab87a1",
   "outputs": [],
   "execution_count": 203
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-18T05:55:55.043169Z",
     "start_time": "2025-06-18T05:51:32.357585Z"
    }
   },
   "cell_type": "code",
   "source": [
    "epochs = 200\n",
    "loss_lst = []\n",
    "vloss_lst = []\n",
    "for epoch in range(1, epochs+1):\n",
    "    loss, vloss = train_PSD_model(model_PSD, PSD_loader, PSD_loader_test)\n",
    "    loss_lst.append(loss)\n",
    "    vloss_lst.append(vloss)\n",
    "    if epoch % 1 == 0:\n",
    "        print(f\"Epoch {epoch}/{epochs}, Loss: {loss*100:.8f} Test loss: {vloss*100:.8f}\")"
   ],
   "id": "2fd72aae7361b860",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200, Loss: 0.00434732 Test loss: 0.00272493\n",
      "Epoch 2/200, Loss: 0.00229503 Test loss: 0.00199105\n",
      "Epoch 3/200, Loss: 0.00191460 Test loss: 0.00190605\n",
      "Epoch 4/200, Loss: 0.00166726 Test loss: 0.00167561\n",
      "Epoch 5/200, Loss: 0.00156679 Test loss: 0.00137561\n",
      "Epoch 6/200, Loss: 0.00147650 Test loss: 0.00149236\n",
      "Epoch 7/200, Loss: 0.00140147 Test loss: 0.00121641\n",
      "Epoch 8/200, Loss: 0.00133908 Test loss: 0.00139034\n",
      "Epoch 9/200, Loss: 0.00130058 Test loss: 0.00126971\n",
      "Epoch 10/200, Loss: 0.00127940 Test loss: 0.00120802\n",
      "Epoch 11/200, Loss: 0.00121901 Test loss: 0.00110940\n",
      "Epoch 12/200, Loss: 0.00119822 Test loss: 0.00112002\n",
      "Epoch 13/200, Loss: 0.00119029 Test loss: 0.00112626\n",
      "Epoch 14/200, Loss: 0.00115911 Test loss: 0.00101597\n",
      "Epoch 15/200, Loss: 0.00114349 Test loss: 0.00098868\n",
      "Epoch 16/200, Loss: 0.00115918 Test loss: 0.00108045\n",
      "Epoch 17/200, Loss: 0.00114737 Test loss: 0.00125388\n",
      "Epoch 18/200, Loss: 0.00114117 Test loss: 0.00116862\n",
      "Epoch 19/200, Loss: 0.00113307 Test loss: 0.00116605\n",
      "Epoch 20/200, Loss: 0.00113205 Test loss: 0.00096085\n",
      "Epoch 21/200, Loss: 0.00111917 Test loss: 0.00104056\n",
      "Epoch 22/200, Loss: 0.00111095 Test loss: 0.00103335\n",
      "Epoch 23/200, Loss: 0.00110983 Test loss: 0.00112011\n",
      "Epoch 24/200, Loss: 0.00108479 Test loss: 0.00098140\n",
      "Epoch 25/200, Loss: 0.00109331 Test loss: 0.00104048\n",
      "Epoch 26/200, Loss: 0.00110301 Test loss: 0.00095101\n",
      "Epoch 27/200, Loss: 0.00108025 Test loss: 0.00117988\n",
      "Epoch 28/200, Loss: 0.00105887 Test loss: 0.00112946\n",
      "Epoch 29/200, Loss: 0.00106903 Test loss: 0.00113162\n",
      "Epoch 30/200, Loss: 0.00110562 Test loss: 0.00107584\n",
      "Epoch 31/200, Loss: 0.00106597 Test loss: 0.00110571\n",
      "Epoch 32/200, Loss: 0.00107260 Test loss: 0.00110954\n",
      "Epoch 33/200, Loss: 0.00107842 Test loss: 0.00101984\n",
      "Epoch 34/200, Loss: 0.00105714 Test loss: 0.00098468\n",
      "Epoch 35/200, Loss: 0.00106151 Test loss: 0.00109426\n",
      "Epoch 36/200, Loss: 0.00106570 Test loss: 0.00090147\n",
      "Epoch 37/200, Loss: 0.00106111 Test loss: 0.00108615\n",
      "Epoch 38/200, Loss: 0.00104346 Test loss: 0.00090165\n",
      "Epoch 39/200, Loss: 0.00102856 Test loss: 0.00097484\n",
      "Epoch 40/200, Loss: 0.00103196 Test loss: 0.00098590\n",
      "Epoch 41/200, Loss: 0.00104068 Test loss: 0.00098759\n",
      "Epoch 42/200, Loss: 0.00102528 Test loss: 0.00095898\n",
      "Epoch 43/200, Loss: 0.00105745 Test loss: 0.00106697\n",
      "Epoch 44/200, Loss: 0.00102160 Test loss: 0.00114831\n",
      "Epoch 45/200, Loss: 0.00102185 Test loss: 0.00110640\n",
      "Epoch 46/200, Loss: 0.00105428 Test loss: 0.00098070\n",
      "Epoch 47/200, Loss: 0.00101596 Test loss: 0.00100310\n",
      "Epoch 48/200, Loss: 0.00104855 Test loss: 0.00109520\n",
      "Epoch 49/200, Loss: 0.00102477 Test loss: 0.00101896\n",
      "Epoch 50/200, Loss: 0.00101687 Test loss: 0.00109507\n",
      "Epoch 51/200, Loss: 0.00100119 Test loss: 0.00106236\n",
      "Epoch 52/200, Loss: 0.00102200 Test loss: 0.00100014\n",
      "Epoch 53/200, Loss: 0.00100951 Test loss: 0.00091870\n",
      "Epoch 54/200, Loss: 0.00101774 Test loss: 0.00101564\n",
      "Epoch 55/200, Loss: 0.00103341 Test loss: 0.00105537\n",
      "Epoch 56/200, Loss: 0.00108762 Test loss: 0.00118039\n",
      "Epoch 57/200, Loss: 0.00104200 Test loss: 0.00103704\n",
      "Epoch 58/200, Loss: 0.00104022 Test loss: 0.00111727\n",
      "Epoch 59/200, Loss: 0.00111727 Test loss: 0.00108828\n",
      "Epoch 60/200, Loss: 0.00098355 Test loss: 0.00110501\n",
      "Epoch 61/200, Loss: 0.00102611 Test loss: 0.00107287\n",
      "Epoch 62/200, Loss: 0.00101433 Test loss: 0.00099395\n",
      "Epoch 63/200, Loss: 0.00098478 Test loss: 0.00115282\n",
      "Epoch 64/200, Loss: 0.00100870 Test loss: 0.00104341\n",
      "Epoch 65/200, Loss: 0.00099744 Test loss: 0.00122059\n",
      "Epoch 66/200, Loss: 0.00101449 Test loss: 0.00108484\n",
      "Epoch 67/200, Loss: 0.00098391 Test loss: 0.00095634\n",
      "Epoch 68/200, Loss: 0.00097980 Test loss: 0.00111772\n",
      "Epoch 69/200, Loss: 0.00100402 Test loss: 0.00108579\n",
      "Epoch 70/200, Loss: 0.00101983 Test loss: 0.00101246\n",
      "Epoch 71/200, Loss: 0.00098912 Test loss: 0.00106526\n",
      "Epoch 72/200, Loss: 0.00098734 Test loss: 0.00121308\n",
      "Epoch 73/200, Loss: 0.00099021 Test loss: 0.00113648\n",
      "Epoch 74/200, Loss: 0.00096917 Test loss: 0.00112822\n",
      "Epoch 75/200, Loss: 0.00097683 Test loss: 0.00111019\n",
      "Epoch 76/200, Loss: 0.00100541 Test loss: 0.00112659\n",
      "Epoch 77/200, Loss: 0.00098966 Test loss: 0.00100857\n",
      "Epoch 78/200, Loss: 0.00096799 Test loss: 0.00114676\n",
      "Epoch 79/200, Loss: 0.00096839 Test loss: 0.00128717\n",
      "Epoch 80/200, Loss: 0.00098055 Test loss: 0.00123043\n",
      "Epoch 81/200, Loss: 0.00099074 Test loss: 0.00113658\n",
      "Epoch 82/200, Loss: 0.00097162 Test loss: 0.00103718\n",
      "Epoch 83/200, Loss: 0.00098982 Test loss: 0.00117541\n",
      "Epoch 84/200, Loss: 0.00093597 Test loss: 0.00113348\n",
      "Epoch 85/200, Loss: 0.00096437 Test loss: 0.00127701\n",
      "Epoch 86/200, Loss: 0.00097271 Test loss: 0.00108427\n",
      "Epoch 87/200, Loss: 0.00094248 Test loss: 0.00112159\n",
      "Epoch 88/200, Loss: 0.00098935 Test loss: 0.00122694\n",
      "Epoch 89/200, Loss: 0.00097188 Test loss: 0.00121066\n",
      "Epoch 90/200, Loss: 0.00096051 Test loss: 0.00100938\n",
      "Epoch 91/200, Loss: 0.00095148 Test loss: 0.00114996\n",
      "Epoch 92/200, Loss: 0.00092126 Test loss: 0.00152398\n",
      "Epoch 93/200, Loss: 0.00096324 Test loss: 0.00122222\n",
      "Epoch 94/200, Loss: 0.00096179 Test loss: 0.00104967\n",
      "Epoch 95/200, Loss: 0.00095931 Test loss: 0.00118181\n",
      "Epoch 96/200, Loss: 0.00095383 Test loss: 0.00101464\n",
      "Epoch 97/200, Loss: 0.00092624 Test loss: 0.00114018\n",
      "Epoch 98/200, Loss: 0.00093800 Test loss: 0.00096651\n",
      "Epoch 99/200, Loss: 0.00095678 Test loss: 0.00116333\n",
      "Epoch 100/200, Loss: 0.00095697 Test loss: 0.00124222\n",
      "Epoch 101/200, Loss: 0.00095801 Test loss: 0.00108395\n",
      "Epoch 102/200, Loss: 0.00092844 Test loss: 0.00123367\n",
      "Epoch 103/200, Loss: 0.00093353 Test loss: 0.00117388\n",
      "Epoch 104/200, Loss: 0.00092828 Test loss: 0.00112625\n",
      "Epoch 105/200, Loss: 0.00091651 Test loss: 0.00106645\n",
      "Epoch 106/200, Loss: 0.00091813 Test loss: 0.00112769\n",
      "Epoch 107/200, Loss: 0.00090679 Test loss: 0.00125412\n",
      "Epoch 108/200, Loss: 0.00095653 Test loss: 0.00116372\n",
      "Epoch 109/200, Loss: 0.00093915 Test loss: 0.00118960\n",
      "Epoch 110/200, Loss: 0.00094877 Test loss: 0.00103135\n",
      "Epoch 111/200, Loss: 0.00094130 Test loss: 0.00125487\n",
      "Epoch 112/200, Loss: 0.00091937 Test loss: 0.00107935\n",
      "Epoch 113/200, Loss: 0.00092370 Test loss: 0.00103293\n",
      "Epoch 114/200, Loss: 0.00094691 Test loss: 0.00110046\n",
      "Epoch 115/200, Loss: 0.00094130 Test loss: 0.00124266\n",
      "Epoch 116/200, Loss: 0.00092915 Test loss: 0.00139279\n",
      "Epoch 117/200, Loss: 0.00091775 Test loss: 0.00120549\n",
      "Epoch 118/200, Loss: 0.00093122 Test loss: 0.00151142\n",
      "Epoch 119/200, Loss: 0.00091388 Test loss: 0.00131684\n",
      "Epoch 120/200, Loss: 0.00092534 Test loss: 0.00119682\n",
      "Epoch 121/200, Loss: 0.00093365 Test loss: 0.00106581\n",
      "Epoch 122/200, Loss: 0.00093238 Test loss: 0.00123278\n",
      "Epoch 123/200, Loss: 0.00093929 Test loss: 0.00113255\n",
      "Epoch 124/200, Loss: 0.00094165 Test loss: 0.00120691\n",
      "Epoch 125/200, Loss: 0.00090318 Test loss: 0.00117228\n",
      "Epoch 126/200, Loss: 0.00091870 Test loss: 0.00106474\n",
      "Epoch 127/200, Loss: 0.00099855 Test loss: 0.00124067\n",
      "Epoch 128/200, Loss: 0.00095489 Test loss: 0.00099492\n",
      "Epoch 129/200, Loss: 0.00093091 Test loss: 0.00112071\n",
      "Epoch 130/200, Loss: 0.00089360 Test loss: 0.00107736\n",
      "Epoch 131/200, Loss: 0.00094369 Test loss: 0.00100161\n",
      "Epoch 132/200, Loss: 0.00092004 Test loss: 0.00111161\n",
      "Epoch 133/200, Loss: 0.00088514 Test loss: 0.00118622\n",
      "Epoch 134/200, Loss: 0.00095509 Test loss: 0.00105820\n",
      "Epoch 135/200, Loss: 0.00090635 Test loss: 0.00103803\n",
      "Epoch 136/200, Loss: 0.00091840 Test loss: 0.00118275\n",
      "Epoch 137/200, Loss: 0.00092595 Test loss: 0.00125751\n",
      "Epoch 138/200, Loss: 0.00092553 Test loss: 0.00119314\n",
      "Epoch 139/200, Loss: 0.00094390 Test loss: 0.00113847\n",
      "Epoch 140/200, Loss: 0.00095334 Test loss: 0.00124493\n",
      "Epoch 141/200, Loss: 0.00090466 Test loss: 0.00117113\n",
      "Epoch 142/200, Loss: 0.00087646 Test loss: 0.00106750\n",
      "Epoch 143/200, Loss: 0.00088702 Test loss: 0.00115050\n",
      "Epoch 144/200, Loss: 0.00093157 Test loss: 0.00107798\n",
      "Epoch 145/200, Loss: 0.00094614 Test loss: 0.00127315\n",
      "Epoch 146/200, Loss: 0.00090485 Test loss: 0.00110524\n",
      "Epoch 147/200, Loss: 0.00087412 Test loss: 0.00119774\n",
      "Epoch 148/200, Loss: 0.00092298 Test loss: 0.00119373\n",
      "Epoch 149/200, Loss: 0.00090845 Test loss: 0.00105299\n",
      "Epoch 150/200, Loss: 0.00089756 Test loss: 0.00117213\n",
      "Epoch 151/200, Loss: 0.00092109 Test loss: 0.00108739\n",
      "Epoch 152/200, Loss: 0.00092476 Test loss: 0.00107201\n",
      "Epoch 153/200, Loss: 0.00090927 Test loss: 0.00105183\n",
      "Epoch 154/200, Loss: 0.00093213 Test loss: 0.00110682\n",
      "Epoch 155/200, Loss: 0.00087236 Test loss: 0.00109424\n",
      "Epoch 156/200, Loss: 0.00088563 Test loss: 0.00106382\n",
      "Epoch 157/200, Loss: 0.00089729 Test loss: 0.00139629\n",
      "Epoch 158/200, Loss: 0.00089567 Test loss: 0.00118657\n",
      "Epoch 159/200, Loss: 0.00097421 Test loss: 0.00101570\n",
      "Epoch 160/200, Loss: 0.00091300 Test loss: 0.00114417\n",
      "Epoch 161/200, Loss: 0.00089861 Test loss: 0.00102699\n",
      "Epoch 162/200, Loss: 0.00089075 Test loss: 0.00140512\n",
      "Epoch 163/200, Loss: 0.00089703 Test loss: 0.00113875\n",
      "Epoch 164/200, Loss: 0.00091230 Test loss: 0.00114583\n",
      "Epoch 165/200, Loss: 0.00087314 Test loss: 0.00122338\n",
      "Epoch 166/200, Loss: 0.00090214 Test loss: 0.00124250\n",
      "Epoch 167/200, Loss: 0.00089658 Test loss: 0.00116897\n",
      "Epoch 168/200, Loss: 0.00089503 Test loss: 0.00115304\n",
      "Epoch 169/200, Loss: 0.00089855 Test loss: 0.00103871\n",
      "Epoch 170/200, Loss: 0.00087663 Test loss: 0.00129589\n",
      "Epoch 171/200, Loss: 0.00089904 Test loss: 0.00132385\n",
      "Epoch 172/200, Loss: 0.00086977 Test loss: 0.00107060\n",
      "Epoch 173/200, Loss: 0.00086786 Test loss: 0.00120980\n",
      "Epoch 174/200, Loss: 0.00093228 Test loss: 0.00108217\n",
      "Epoch 175/200, Loss: 0.00086241 Test loss: 0.00116313\n",
      "Epoch 176/200, Loss: 0.00087701 Test loss: 0.00122226\n",
      "Epoch 177/200, Loss: 0.00090321 Test loss: 0.00116434\n",
      "Epoch 178/200, Loss: 0.00086867 Test loss: 0.00118117\n",
      "Epoch 179/200, Loss: 0.00089804 Test loss: 0.00104471\n",
      "Epoch 180/200, Loss: 0.00086513 Test loss: 0.00107259\n",
      "Epoch 181/200, Loss: 0.00086391 Test loss: 0.00101108\n",
      "Epoch 182/200, Loss: 0.00087627 Test loss: 0.00108444\n",
      "Epoch 183/200, Loss: 0.00089084 Test loss: 0.00105750\n",
      "Epoch 184/200, Loss: 0.00089849 Test loss: 0.00115176\n",
      "Epoch 185/200, Loss: 0.00087378 Test loss: 0.00112397\n",
      "Epoch 186/200, Loss: 0.00087528 Test loss: 0.00115143\n",
      "Epoch 187/200, Loss: 0.00088249 Test loss: 0.00113385\n",
      "Epoch 188/200, Loss: 0.00095504 Test loss: 0.00107996\n",
      "Epoch 189/200, Loss: 0.00090822 Test loss: 0.00109057\n",
      "Epoch 190/200, Loss: 0.00090883 Test loss: 0.00172319\n",
      "Epoch 191/200, Loss: 0.00086889 Test loss: 0.00118573\n",
      "Epoch 192/200, Loss: 0.00087906 Test loss: 0.00129497\n",
      "Epoch 193/200, Loss: 0.00089900 Test loss: 0.00122300\n",
      "Epoch 194/200, Loss: 0.00088284 Test loss: 0.00129972\n",
      "Epoch 195/200, Loss: 0.00087765 Test loss: 0.00111747\n",
      "Epoch 196/200, Loss: 0.00088226 Test loss: 0.00126391\n",
      "Epoch 197/200, Loss: 0.00084706 Test loss: 0.00122383\n",
      "Epoch 198/200, Loss: 0.00087263 Test loss: 0.00122565\n",
      "Epoch 199/200, Loss: 0.00087742 Test loss: 0.00109009\n",
      "Epoch 200/200, Loss: 0.00087608 Test loss: 0.00129379\n"
     ]
    }
   ],
   "execution_count": 204
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-18T05:13:14.088688Z",
     "start_time": "2025-06-18T05:13:14.041504Z"
    }
   },
   "cell_type": "code",
   "source": [
    "plt.plot(loss_lst)\n",
    "plt.plot(vloss_lst)\n",
    "plt.show()"
   ],
   "id": "826613b63fcbd7f1",
   "outputs": [],
   "execution_count": 73
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-18T06:03:28.196844Z",
     "start_time": "2025-06-18T06:03:28.182843Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model_name = \"autoencoder_regressor_lognormal\"\n",
    "torch.save(model_PSD, f\"data/models/torch/{model_name}\")"
   ],
   "id": "8b63396d69bd925e",
   "outputs": [],
   "execution_count": 210
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-18T06:03:32.870355Z",
     "start_time": "2025-06-18T06:03:32.839353Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model_name = \"autoencoder_regressor_lognormal\"\n",
    "model_PSD = torch.load(f\"data/models/torch/{model_name}\", weights_only=False)"
   ],
   "id": "5df22fca0b595bb7",
   "outputs": [],
   "execution_count": 211
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-18T06:00:45.869706Z",
     "start_time": "2025-06-18T06:00:45.824710Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model_PSD.eval()\n",
    "y_train_PSD = model_PSD.model(torch.tensor(latent_vectors_train, dtype=torch.float32).to(device)).detach().cpu().numpy()\n",
    "y_test_PSD = model_PSD.model(torch.tensor(latent_vectors_test, dtype=torch.float32).to(device)).detach().cpu().numpy()\n",
    "y_test_exp_PSD = model_PSD.model(torch.tensor(latent_vectors_test_exp, dtype=torch.float32).to(device)).detach().cpu().numpy()"
   ],
   "id": "bd320d01dab9040b",
   "outputs": [],
   "execution_count": 205
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-18T04:30:46.317952Z",
     "start_time": "2025-06-18T04:30:44.087857Z"
    }
   },
   "cell_type": "code",
   "source": [
    "math_psds = [fit_linear(x_test_exp[i], data_sorb[:, :-10], 0).x for i in range(len(x_test_exp))]\n",
    "restored_isotherms = [np.dot(data_sorb[:, :-10].T, psd) for psd in math_psds]"
   ],
   "id": "6c93f1ac56f30964",
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[20], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m math_psds \u001B[38;5;241m=\u001B[39m [fit_linear(x_test_exp[i], data_sorb[:, :\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m10\u001B[39m], \u001B[38;5;241m0\u001B[39m)\u001B[38;5;241m.\u001B[39mx \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;28mlen\u001B[39m(x_test_exp))]\n\u001B[0;32m      2\u001B[0m restored_isotherms \u001B[38;5;241m=\u001B[39m [np\u001B[38;5;241m.\u001B[39mdot(data_sorb[:, :\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m10\u001B[39m]\u001B[38;5;241m.\u001B[39mT, psd) \u001B[38;5;28;01mfor\u001B[39;00m psd \u001B[38;5;129;01min\u001B[39;00m math_psds]\n",
      "Cell \u001B[1;32mIn[20], line 1\u001B[0m, in \u001B[0;36m<listcomp>\u001B[1;34m(.0)\u001B[0m\n\u001B[1;32m----> 1\u001B[0m math_psds \u001B[38;5;241m=\u001B[39m [\u001B[43mfit_linear\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx_test_exp\u001B[49m\u001B[43m[\u001B[49m\u001B[43mi\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdata_sorb\u001B[49m\u001B[43m[\u001B[49m\u001B[43m:\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m:\u001B[49m\u001B[38;5;241;43m-\u001B[39;49m\u001B[38;5;241;43m10\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m)\u001B[49m\u001B[38;5;241m.\u001B[39mx \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;28mlen\u001B[39m(x_test_exp))]\n\u001B[0;32m      2\u001B[0m restored_isotherms \u001B[38;5;241m=\u001B[39m [np\u001B[38;5;241m.\u001B[39mdot(data_sorb[:, :\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m10\u001B[39m]\u001B[38;5;241m.\u001B[39mT, psd) \u001B[38;5;28;01mfor\u001B[39;00m psd \u001B[38;5;129;01min\u001B[39;00m math_psds]\n",
      "File \u001B[1;32m~\\PycharmProjects\\isotherm\\inverse.py:76\u001B[0m, in \u001B[0;36mfit_linear\u001B[1;34m(adsorption, kernel, alpha)\u001B[0m\n\u001B[0;32m     74\u001B[0m A_augmented \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39mvstack([kernel\u001B[38;5;241m.\u001B[39mT, np\u001B[38;5;241m.\u001B[39msqrt(alpha) \u001B[38;5;241m*\u001B[39m np\u001B[38;5;241m.\u001B[39meye(\u001B[38;5;28mlen\u001B[39m(kernel))])\n\u001B[0;32m     75\u001B[0m b_augmented \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39mconcatenate([adsorption, np\u001B[38;5;241m.\u001B[39mzeros(\u001B[38;5;28mlen\u001B[39m(kernel))])\n\u001B[1;32m---> 76\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[43mlsq_linear\u001B[49m\u001B[43m(\u001B[49m\u001B[43mA_augmented\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mb_augmented\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbounds\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43minf\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     77\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m result\n",
      "File \u001B[1;32m~\\PycharmProjects\\isotherm\\.venv\\lib\\site-packages\\scipy\\optimize\\_lsq\\lsq_linear.py:333\u001B[0m, in \u001B[0;36mlsq_linear\u001B[1;34m(A, b, bounds, method, tol, lsq_solver, lsmr_tol, max_iter, verbose, lsmr_maxiter)\u001B[0m\n\u001B[0;32m    326\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m OptimizeResult(\n\u001B[0;32m    327\u001B[0m         x\u001B[38;5;241m=\u001B[39mx_lsq, fun\u001B[38;5;241m=\u001B[39mr, cost\u001B[38;5;241m=\u001B[39mcost, optimality\u001B[38;5;241m=\u001B[39mg_norm,\n\u001B[0;32m    328\u001B[0m         active_mask\u001B[38;5;241m=\u001B[39mnp\u001B[38;5;241m.\u001B[39mzeros(n), unbounded_sol\u001B[38;5;241m=\u001B[39munbd_lsq,\n\u001B[0;32m    329\u001B[0m         nit\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0\u001B[39m, status\u001B[38;5;241m=\u001B[39mtermination_status,\n\u001B[0;32m    330\u001B[0m         message\u001B[38;5;241m=\u001B[39mtermination_message, success\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[0;32m    332\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m method \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtrf\u001B[39m\u001B[38;5;124m'\u001B[39m:\n\u001B[1;32m--> 333\u001B[0m     res \u001B[38;5;241m=\u001B[39m \u001B[43mtrf_linear\u001B[49m\u001B[43m(\u001B[49m\u001B[43mA\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mb\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mx_lsq\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlb\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mub\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtol\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlsq_solver\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlsmr_tol\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    334\u001B[0m \u001B[43m                     \u001B[49m\u001B[43mmax_iter\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mverbose\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlsmr_maxiter\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mlsmr_maxiter\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    335\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m method \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mbvls\u001B[39m\u001B[38;5;124m'\u001B[39m:\n\u001B[0;32m    336\u001B[0m     res \u001B[38;5;241m=\u001B[39m bvls(A, b, x_lsq, lb, ub, tol, max_iter, verbose)\n",
      "File \u001B[1;32m~\\PycharmProjects\\isotherm\\.venv\\lib\\site-packages\\scipy\\optimize\\_lsq\\trf_linear.py:221\u001B[0m, in \u001B[0;36mtrf_linear\u001B[1;34m(A, b, x_lsq, lb, ub, tol, lsq_solver, lsmr_tol, max_iter, verbose, lsmr_maxiter)\u001B[0m\n\u001B[0;32m    218\u001B[0m     termination_status \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m\n\u001B[0;32m    220\u001B[0m theta \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m \u001B[38;5;241m-\u001B[39m \u001B[38;5;28mmin\u001B[39m(\u001B[38;5;241m0.005\u001B[39m, g_norm)\n\u001B[1;32m--> 221\u001B[0m step \u001B[38;5;241m=\u001B[39m \u001B[43mselect_step\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mA_h\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mg_h\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdiag_h\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mp\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mp_h\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43md\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlb\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mub\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtheta\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    222\u001B[0m cost_change \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m-\u001B[39mevaluate_quadratic(A, g, step)\n\u001B[0;32m    224\u001B[0m \u001B[38;5;66;03m# Perhaps almost never executed, the idea is that `p` is descent\u001B[39;00m\n\u001B[0;32m    225\u001B[0m \u001B[38;5;66;03m# direction thus we must find acceptable cost decrease using simple\u001B[39;00m\n\u001B[0;32m    226\u001B[0m \u001B[38;5;66;03m# \"backtracking\", otherwise the algorithm's logic would break.\u001B[39;00m\n",
      "File \u001B[1;32m~\\PycharmProjects\\isotherm\\.venv\\lib\\site-packages\\scipy\\optimize\\_lsq\\trf_linear.py:114\u001B[0m, in \u001B[0;36mselect_step\u001B[1;34m(x, A_h, g_h, c_h, p, p_h, d, lb, ub, theta)\u001B[0m\n\u001B[0;32m    111\u001B[0m r_stride_u \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m=\u001B[39m theta\n\u001B[0;32m    113\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m r_stride_u \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[1;32m--> 114\u001B[0m     a, b, c \u001B[38;5;241m=\u001B[39m \u001B[43mbuild_quadratic_1d\u001B[49m\u001B[43m(\u001B[49m\u001B[43mA_h\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mg_h\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mr_h\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43ms0\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mp_h\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdiag\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mc_h\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    115\u001B[0m     r_stride, r_value \u001B[38;5;241m=\u001B[39m minimize_quadratic_1d(\n\u001B[0;32m    116\u001B[0m         a, b, r_stride_l, r_stride_u, c\u001B[38;5;241m=\u001B[39mc)\n\u001B[0;32m    117\u001B[0m     r_h \u001B[38;5;241m=\u001B[39m p_h \u001B[38;5;241m+\u001B[39m r_h \u001B[38;5;241m*\u001B[39m r_stride\n",
      "File \u001B[1;32m~\\PycharmProjects\\isotherm\\.venv\\lib\\site-packages\\scipy\\optimize\\_lsq\\common.py:282\u001B[0m, in \u001B[0;36mbuild_quadratic_1d\u001B[1;34m(J, g, s, diag, s0)\u001B[0m\n\u001B[0;32m    251\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21mbuild_quadratic_1d\u001B[39m(J, g, s, diag\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, s0\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m):\n\u001B[0;32m    252\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Parameterize a multivariate quadratic function along a line.\u001B[39;00m\n\u001B[0;32m    253\u001B[0m \n\u001B[0;32m    254\u001B[0m \u001B[38;5;124;03m    The resulting univariate quadratic function is given as follows:\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    280\u001B[0m \u001B[38;5;124;03m        Free term. Returned only if `s0` is provided.\u001B[39;00m\n\u001B[0;32m    281\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m--> 282\u001B[0m     v \u001B[38;5;241m=\u001B[39m \u001B[43mJ\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdot\u001B[49m\u001B[43m(\u001B[49m\u001B[43ms\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    283\u001B[0m     a \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39mdot(v, v)\n\u001B[0;32m    284\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m diag \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "File \u001B[1;32m~\\PycharmProjects\\isotherm\\.venv\\lib\\site-packages\\scipy\\sparse\\linalg\\_interface.py:418\u001B[0m, in \u001B[0;36mLinearOperator.dot\u001B[1;34m(self, x)\u001B[0m\n\u001B[0;32m    415\u001B[0m x \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39masarray(x)\n\u001B[0;32m    417\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m x\u001B[38;5;241m.\u001B[39mndim \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m1\u001B[39m \u001B[38;5;129;01mor\u001B[39;00m x\u001B[38;5;241m.\u001B[39mndim \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m2\u001B[39m \u001B[38;5;129;01mand\u001B[39;00m x\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m1\u001B[39m] \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m1\u001B[39m:\n\u001B[1;32m--> 418\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmatvec\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    419\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m x\u001B[38;5;241m.\u001B[39mndim \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m2\u001B[39m:\n\u001B[0;32m    420\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmatmat(x)\n",
      "File \u001B[1;32m~\\PycharmProjects\\isotherm\\.venv\\lib\\site-packages\\scipy\\sparse\\linalg\\_interface.py:232\u001B[0m, in \u001B[0;36mLinearOperator.matvec\u001B[1;34m(self, x)\u001B[0m\n\u001B[0;32m    229\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m x\u001B[38;5;241m.\u001B[39mshape \u001B[38;5;241m!=\u001B[39m (N,) \u001B[38;5;129;01mand\u001B[39;00m x\u001B[38;5;241m.\u001B[39mshape \u001B[38;5;241m!=\u001B[39m (N,\u001B[38;5;241m1\u001B[39m):\n\u001B[0;32m    230\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mdimension mismatch\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m--> 232\u001B[0m y \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_matvec\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    234\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(x, np\u001B[38;5;241m.\u001B[39mmatrix):\n\u001B[0;32m    235\u001B[0m     y \u001B[38;5;241m=\u001B[39m asmatrix(y)\n",
      "File \u001B[1;32m~\\PycharmProjects\\isotherm\\.venv\\lib\\site-packages\\scipy\\sparse\\linalg\\_interface.py:530\u001B[0m, in \u001B[0;36m_CustomLinearOperator._matvec\u001B[1;34m(self, x)\u001B[0m\n\u001B[0;32m    529\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21m_matvec\u001B[39m(\u001B[38;5;28mself\u001B[39m, x):\n\u001B[1;32m--> 530\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m__matvec_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\PycharmProjects\\isotherm\\.venv\\lib\\site-packages\\scipy\\optimize\\_lsq\\common.py:638\u001B[0m, in \u001B[0;36mright_multiplied_operator.<locals>.matvec\u001B[1;34m(x)\u001B[0m\n\u001B[0;32m    637\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21mmatvec\u001B[39m(x):\n\u001B[1;32m--> 638\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mJ\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmatvec\u001B[49m\u001B[43m(\u001B[49m\u001B[43mnp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mravel\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43m \u001B[49m\u001B[43md\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\PycharmProjects\\isotherm\\.venv\\lib\\site-packages\\scipy\\sparse\\linalg\\_interface.py:232\u001B[0m, in \u001B[0;36mLinearOperator.matvec\u001B[1;34m(self, x)\u001B[0m\n\u001B[0;32m    229\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m x\u001B[38;5;241m.\u001B[39mshape \u001B[38;5;241m!=\u001B[39m (N,) \u001B[38;5;129;01mand\u001B[39;00m x\u001B[38;5;241m.\u001B[39mshape \u001B[38;5;241m!=\u001B[39m (N,\u001B[38;5;241m1\u001B[39m):\n\u001B[0;32m    230\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mdimension mismatch\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m--> 232\u001B[0m y \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_matvec\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    234\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(x, np\u001B[38;5;241m.\u001B[39mmatrix):\n\u001B[0;32m    235\u001B[0m     y \u001B[38;5;241m=\u001B[39m asmatrix(y)\n",
      "File \u001B[1;32m~\\PycharmProjects\\isotherm\\.venv\\lib\\site-packages\\scipy\\sparse\\linalg\\_interface.py:199\u001B[0m, in \u001B[0;36mLinearOperator._matvec\u001B[1;34m(self, x)\u001B[0m\n\u001B[0;32m    189\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21m_matvec\u001B[39m(\u001B[38;5;28mself\u001B[39m, x):\n\u001B[0;32m    190\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Default matrix-vector multiplication handler.\u001B[39;00m\n\u001B[0;32m    191\u001B[0m \n\u001B[0;32m    192\u001B[0m \u001B[38;5;124;03m    If self is a linear operator of shape (M, N), then this method will\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    197\u001B[0m \u001B[38;5;124;03m    will define matrix-vector multiplication as well.\u001B[39;00m\n\u001B[0;32m    198\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m--> 199\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmatmat\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mreshape\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m-\u001B[39;49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\PycharmProjects\\isotherm\\.venv\\lib\\site-packages\\scipy\\sparse\\linalg\\_interface.py:337\u001B[0m, in \u001B[0;36mLinearOperator.matmat\u001B[1;34m(self, X)\u001B[0m\n\u001B[0;32m    333\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m X\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m0\u001B[39m] \u001B[38;5;241m!=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m1\u001B[39m]:\n\u001B[0;32m    334\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mdimension mismatch: \u001B[39m\u001B[38;5;132;01m%r\u001B[39;00m\u001B[38;5;124m, \u001B[39m\u001B[38;5;132;01m%r\u001B[39;00m\u001B[38;5;124m'\u001B[39m\n\u001B[0;32m    335\u001B[0m                      \u001B[38;5;241m%\u001B[39m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mshape, X\u001B[38;5;241m.\u001B[39mshape))\n\u001B[1;32m--> 337\u001B[0m Y \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_matmat\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    339\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(Y, np\u001B[38;5;241m.\u001B[39mmatrix):\n\u001B[0;32m    340\u001B[0m     Y \u001B[38;5;241m=\u001B[39m asmatrix(Y)\n",
      "File \u001B[1;32m~\\PycharmProjects\\isotherm\\.venv\\lib\\site-packages\\scipy\\sparse\\linalg\\_interface.py:731\u001B[0m, in \u001B[0;36mMatrixLinearOperator._matmat\u001B[1;34m(self, X)\u001B[0m\n\u001B[0;32m    730\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21m_matmat\u001B[39m(\u001B[38;5;28mself\u001B[39m, X):\n\u001B[1;32m--> 731\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mA\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdot\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-18T04:36:44.955192Z",
     "start_time": "2025-06-18T04:36:44.923339Z"
    }
   },
   "cell_type": "code",
   "source": "model_tester.plot_testing_graphs(y_test_exp_PSD, x_test_exp, restored_isotherms, data_sorb[:, :-10].T, model_name)",
   "id": "dbc5b29070bcaec0",
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'restored_isotherms' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[42], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m model_tester\u001B[38;5;241m.\u001B[39mplot_testing_graphs(y_test_exp_PSD, x_test_exp, \u001B[43mrestored_isotherms\u001B[49m, data_sorb[:, :\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m10\u001B[39m]\u001B[38;5;241m.\u001B[39mT, model_name)\n",
      "\u001B[1;31mNameError\u001B[0m: name 'restored_isotherms' is not defined"
     ]
    }
   ],
   "execution_count": 42
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-18T06:02:57.592690Z",
     "start_time": "2025-06-18T06:02:57.050993Z"
    }
   },
   "cell_type": "code",
   "source": "plot_preds(x_test_exp, y_test_exp, y_test_exp_PSD)",
   "id": "424bd83f3fe2a9f9",
   "outputs": [],
   "execution_count": 209
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-18T06:04:46.305815Z",
     "start_time": "2025-06-18T06:04:46.275626Z"
    }
   },
   "cell_type": "code",
   "source": "np.savez(f\"data/models/metrics/{model_name}\", x=x_test_exp, y=y_test_exp_PSD)",
   "id": "b11eabe3085b3dc5",
   "outputs": [],
   "execution_count": 212
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "2304478b36149859"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
