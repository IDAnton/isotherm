{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-06-18T09:50:17.004808Z",
     "start_time": "2025-06-18T09:50:14.524482Z"
    }
   },
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from datasetLoader import load_dataset\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "from inverse import fit_linear\n",
    "from tools import model_tester\n",
    "%matplotlib qt"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-18T09:50:17.044149Z",
     "start_time": "2025-06-18T09:50:17.004808Z"
    }
   },
   "cell_type": "code",
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ],
   "id": "d322819b15356399",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-18T11:28:39.468276Z",
     "start_time": "2025-06-18T11:28:35.542870Z"
    }
   },
   "cell_type": "code",
   "source": [
    "pore_widths = np.load(\"data/initial kernels/Size_Kernel_Silica_Adsorption.npy\")\n",
    "pressures = np.load(\"data/initial kernels/Pressure_Silica.npy\")\n",
    "with open(\"data/initial kernels/Kernel_Silica_Adsorption.npy\", 'rb') as f:\n",
    "    data_sorb = np.load(f)\n",
    "\n",
    "x, y = load_dataset('data/datasets/silica_random.npz')\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.15, random_state=1)\n",
    "\n",
    "x_exp, y_exp = load_dataset('data/datasets/SMP_CUT_NOT_ZERO.npz')\n",
    "\n",
    "x_train_exp, x_test_exp, y_train_exp, y_test_exp = train_test_split(x_exp, y_exp, test_size=0.15, random_state=1)"
   ],
   "id": "d29956229295484c",
   "outputs": [],
   "execution_count": 183
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-18T11:43:53.991227Z",
     "start_time": "2025-06-18T11:43:53.917456Z"
    }
   },
   "cell_type": "code",
   "source": [
    "figure, axis = plt.subplots(3, 4)\n",
    "for i in range(3):\n",
    "    for j in range(4):\n",
    "        k = np.random.randint(0, len(x_train))\n",
    "        axis[i, j].plot(pore_widths, y_train[k], marker=\".\")\n",
    "        axis[i, j].grid()\n",
    "plt.show()"
   ],
   "id": "15f9ff24438da3f5",
   "outputs": [],
   "execution_count": 190
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-18T11:43:49.397871Z",
     "start_time": "2025-06-18T11:43:49.317259Z"
    }
   },
   "cell_type": "code",
   "source": "plt.plot(pore_widths, sum(y_train), marker=\".\")",
   "id": "d204e5d61104e3ed",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1e7b3b47a00>]"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 189
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-13T09:02:46.726188Z",
     "start_time": "2025-06-13T09:02:46.680189Z"
    }
   },
   "cell_type": "code",
   "source": [
    "i = np.random.randint(0, len(x_train))\n",
    "plt.plot(pressures[:-10], x_train[i], marker=\".\")\n",
    "plt.grid()\n",
    "plt.show()"
   ],
   "id": "3201b1f78762f4ff",
   "outputs": [],
   "execution_count": 272
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-18T09:53:13.185164Z",
     "start_time": "2025-06-18T09:53:13.076799Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class IsothermDataset(Dataset):\n",
    "    def __init__(self, isotherms, transform=None):\n",
    "        self.data = torch.tensor(isotherms, dtype=torch.float32).to(device)\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = self.data[idx]\n",
    "        if self.transform:\n",
    "            x = self.transform(x)\n",
    "        return x, x\n",
    "\n",
    "dataset = IsothermDataset(np.concatenate((x_train_exp, x_train_exp)))\n",
    "dataset_test = IsothermDataset(np.concatenate((x_test_exp, x_test_exp)))\n",
    "\n",
    "\n",
    "batch_size = 512\n",
    "loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "loader_test = DataLoader(dataset_test, batch_size=batch_size, shuffle=False)"
   ],
   "id": "5cefc1b65fc7fa63",
   "outputs": [],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-18T10:55:46.902708Z",
     "start_time": "2025-06-18T10:55:46.872528Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self, input_dim, latent_dim):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, latent_dim)\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(latent_dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, input_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        z = self.encoder(x)\n",
    "        x_recon = self.decoder(z)\n",
    "        return x_recon, z\n",
    "\n",
    "input_dim = 448\n",
    "latent_dim = 16\n",
    "epochs = 200\n",
    "learning_rate = 1e-3\n",
    "\n",
    "model = Autoencoder(input_dim=input_dim, latent_dim=latent_dim)\n",
    "model.to(device)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "def train_autoencoder(model, loader, loader_test):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    total_vloss = 0\n",
    "    for x, _ in loader:\n",
    "        optimizer.zero_grad()\n",
    "        x_recon, _ = model(x)\n",
    "        loss = criterion(x_recon, x)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for x, _ in loader_test:\n",
    "            x_recon, _  = model(x)\n",
    "            vloss = criterion(x_recon, x)\n",
    "            total_vloss += vloss.item()\n",
    "\n",
    "    return total_loss / len(loader.dataset), total_vloss / len(loader_test.dataset)\n",
    "\n",
    "\n",
    "# sample_z = model.encoder(torch.tensor(isotherms_np[0], dtype=torch.float32))"
   ],
   "id": "c960463d00766e7c",
   "outputs": [],
   "execution_count": 159
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-18T10:56:30.688021Z",
     "start_time": "2025-06-18T10:55:47.729533Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for epoch in range(1, epochs+1):\n",
    "    loss, vloss = train_autoencoder(model, loader,loader_test)\n",
    "    if epoch % 1 == 0:\n",
    "        print(f\"Epoch {epoch}/{epochs}, Loss: {loss*100:.8f} Test loss: {vloss*100:.8f}\")"
   ],
   "id": "6f625ed77c7b7164",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200, Loss: 0.01567786 Test loss: 0.00345204\n",
      "Epoch 2/200, Loss: 0.00310688 Test loss: 0.00251379\n",
      "Epoch 3/200, Loss: 0.00102629 Test loss: 0.00054073\n",
      "Epoch 4/200, Loss: 0.00042479 Test loss: 0.00032616\n",
      "Epoch 5/200, Loss: 0.00025846 Test loss: 0.00022357\n",
      "Epoch 6/200, Loss: 0.00019980 Test loss: 0.00018698\n",
      "Epoch 7/200, Loss: 0.00017159 Test loss: 0.00016422\n",
      "Epoch 8/200, Loss: 0.00014878 Test loss: 0.00014052\n",
      "Epoch 9/200, Loss: 0.00012857 Test loss: 0.00012015\n",
      "Epoch 10/200, Loss: 0.00011389 Test loss: 0.00011373\n",
      "Epoch 11/200, Loss: 0.00010235 Test loss: 0.00009978\n",
      "Epoch 12/200, Loss: 0.00009515 Test loss: 0.00009060\n",
      "Epoch 13/200, Loss: 0.00008999 Test loss: 0.00009503\n",
      "Epoch 14/200, Loss: 0.00008358 Test loss: 0.00008574\n",
      "Epoch 15/200, Loss: 0.00007723 Test loss: 0.00007401\n",
      "Epoch 16/200, Loss: 0.00007146 Test loss: 0.00007651\n",
      "Epoch 17/200, Loss: 0.00006734 Test loss: 0.00006669\n",
      "Epoch 18/200, Loss: 0.00006537 Test loss: 0.00006196\n",
      "Epoch 19/200, Loss: 0.00006016 Test loss: 0.00006206\n",
      "Epoch 20/200, Loss: 0.00005943 Test loss: 0.00005530\n",
      "Epoch 21/200, Loss: 0.00005550 Test loss: 0.00005511\n",
      "Epoch 22/200, Loss: 0.00005173 Test loss: 0.00005227\n",
      "Epoch 23/200, Loss: 0.00005109 Test loss: 0.00005098\n",
      "Epoch 24/200, Loss: 0.00004843 Test loss: 0.00004594\n",
      "Epoch 25/200, Loss: 0.00005101 Test loss: 0.00005211\n",
      "Epoch 26/200, Loss: 0.00004585 Test loss: 0.00004344\n",
      "Epoch 27/200, Loss: 0.00004407 Test loss: 0.00004332\n",
      "Epoch 28/200, Loss: 0.00004361 Test loss: 0.00005005\n",
      "Epoch 29/200, Loss: 0.00004325 Test loss: 0.00005043\n",
      "Epoch 30/200, Loss: 0.00004190 Test loss: 0.00003978\n",
      "Epoch 31/200, Loss: 0.00004182 Test loss: 0.00004207\n",
      "Epoch 32/200, Loss: 0.00003960 Test loss: 0.00004193\n",
      "Epoch 33/200, Loss: 0.00003736 Test loss: 0.00003900\n",
      "Epoch 34/200, Loss: 0.00003917 Test loss: 0.00003409\n",
      "Epoch 35/200, Loss: 0.00003726 Test loss: 0.00003447\n",
      "Epoch 36/200, Loss: 0.00003692 Test loss: 0.00003423\n",
      "Epoch 37/200, Loss: 0.00003643 Test loss: 0.00003540\n",
      "Epoch 38/200, Loss: 0.00003804 Test loss: 0.00003125\n",
      "Epoch 39/200, Loss: 0.00003558 Test loss: 0.00003385\n",
      "Epoch 40/200, Loss: 0.00003426 Test loss: 0.00003345\n",
      "Epoch 41/200, Loss: 0.00003550 Test loss: 0.00003173\n",
      "Epoch 42/200, Loss: 0.00003453 Test loss: 0.00004564\n",
      "Epoch 43/200, Loss: 0.00003509 Test loss: 0.00003136\n",
      "Epoch 44/200, Loss: 0.00003081 Test loss: 0.00002938\n",
      "Epoch 45/200, Loss: 0.00003314 Test loss: 0.00003238\n",
      "Epoch 46/200, Loss: 0.00003558 Test loss: 0.00003379\n",
      "Epoch 47/200, Loss: 0.00002977 Test loss: 0.00003519\n",
      "Epoch 48/200, Loss: 0.00003374 Test loss: 0.00002885\n",
      "Epoch 49/200, Loss: 0.00003089 Test loss: 0.00003399\n",
      "Epoch 50/200, Loss: 0.00003193 Test loss: 0.00002613\n",
      "Epoch 51/200, Loss: 0.00002977 Test loss: 0.00003442\n",
      "Epoch 52/200, Loss: 0.00003054 Test loss: 0.00002865\n",
      "Epoch 53/200, Loss: 0.00003207 Test loss: 0.00002666\n",
      "Epoch 54/200, Loss: 0.00002824 Test loss: 0.00002678\n",
      "Epoch 55/200, Loss: 0.00003003 Test loss: 0.00003616\n",
      "Epoch 56/200, Loss: 0.00002882 Test loss: 0.00002412\n",
      "Epoch 57/200, Loss: 0.00002934 Test loss: 0.00002731\n",
      "Epoch 58/200, Loss: 0.00002977 Test loss: 0.00002973\n",
      "Epoch 59/200, Loss: 0.00002561 Test loss: 0.00002546\n",
      "Epoch 60/200, Loss: 0.00003271 Test loss: 0.00002349\n",
      "Epoch 61/200, Loss: 0.00002709 Test loss: 0.00002510\n",
      "Epoch 62/200, Loss: 0.00002700 Test loss: 0.00002434\n",
      "Epoch 63/200, Loss: 0.00002839 Test loss: 0.00004574\n",
      "Epoch 64/200, Loss: 0.00002557 Test loss: 0.00002235\n",
      "Epoch 65/200, Loss: 0.00003019 Test loss: 0.00002279\n",
      "Epoch 66/200, Loss: 0.00002538 Test loss: 0.00002479\n",
      "Epoch 67/200, Loss: 0.00002736 Test loss: 0.00002341\n",
      "Epoch 68/200, Loss: 0.00002595 Test loss: 0.00007039\n",
      "Epoch 69/200, Loss: 0.00002623 Test loss: 0.00002211\n",
      "Epoch 70/200, Loss: 0.00002553 Test loss: 0.00002139\n",
      "Epoch 71/200, Loss: 0.00002310 Test loss: 0.00003619\n",
      "Epoch 72/200, Loss: 0.00002835 Test loss: 0.00002245\n",
      "Epoch 73/200, Loss: 0.00002643 Test loss: 0.00002122\n",
      "Epoch 74/200, Loss: 0.00002310 Test loss: 0.00004894\n",
      "Epoch 75/200, Loss: 0.00002501 Test loss: 0.00004068\n",
      "Epoch 76/200, Loss: 0.00002516 Test loss: 0.00001968\n",
      "Epoch 77/200, Loss: 0.00002257 Test loss: 0.00001940\n",
      "Epoch 78/200, Loss: 0.00003305 Test loss: 0.00002416\n",
      "Epoch 79/200, Loss: 0.00001986 Test loss: 0.00001897\n",
      "Epoch 80/200, Loss: 0.00002038 Test loss: 0.00002169\n",
      "Epoch 81/200, Loss: 0.00002234 Test loss: 0.00002232\n",
      "Epoch 82/200, Loss: 0.00002389 Test loss: 0.00002002\n",
      "Epoch 83/200, Loss: 0.00002066 Test loss: 0.00001884\n",
      "Epoch 84/200, Loss: 0.00002540 Test loss: 0.00003032\n",
      "Epoch 85/200, Loss: 0.00001936 Test loss: 0.00001750\n",
      "Epoch 86/200, Loss: 0.00002036 Test loss: 0.00001863\n",
      "Epoch 87/200, Loss: 0.00002892 Test loss: 0.00001685\n",
      "Epoch 88/200, Loss: 0.00001677 Test loss: 0.00001944\n",
      "Epoch 89/200, Loss: 0.00001923 Test loss: 0.00002288\n",
      "Epoch 90/200, Loss: 0.00002248 Test loss: 0.00001862\n",
      "Epoch 91/200, Loss: 0.00001964 Test loss: 0.00001924\n",
      "Epoch 92/200, Loss: 0.00002294 Test loss: 0.00001689\n",
      "Epoch 93/200, Loss: 0.00001915 Test loss: 0.00002201\n",
      "Epoch 94/200, Loss: 0.00001972 Test loss: 0.00001791\n",
      "Epoch 95/200, Loss: 0.00002288 Test loss: 0.00003151\n",
      "Epoch 96/200, Loss: 0.00001924 Test loss: 0.00001556\n",
      "Epoch 97/200, Loss: 0.00002025 Test loss: 0.00002016\n",
      "Epoch 98/200, Loss: 0.00001999 Test loss: 0.00001925\n",
      "Epoch 99/200, Loss: 0.00001882 Test loss: 0.00001610\n",
      "Epoch 100/200, Loss: 0.00002122 Test loss: 0.00004807\n",
      "Epoch 101/200, Loss: 0.00002054 Test loss: 0.00001539\n",
      "Epoch 102/200, Loss: 0.00001905 Test loss: 0.00001597\n",
      "Epoch 103/200, Loss: 0.00001822 Test loss: 0.00002639\n",
      "Epoch 104/200, Loss: 0.00002144 Test loss: 0.00001901\n",
      "Epoch 105/200, Loss: 0.00001808 Test loss: 0.00001537\n",
      "Epoch 106/200, Loss: 0.00002028 Test loss: 0.00009747\n",
      "Epoch 107/200, Loss: 0.00002199 Test loss: 0.00002029\n",
      "Epoch 108/200, Loss: 0.00001620 Test loss: 0.00001467\n",
      "Epoch 109/200, Loss: 0.00001831 Test loss: 0.00001732\n",
      "Epoch 110/200, Loss: 0.00002083 Test loss: 0.00001460\n",
      "Epoch 111/200, Loss: 0.00001518 Test loss: 0.00001462\n",
      "Epoch 112/200, Loss: 0.00002141 Test loss: 0.00001403\n",
      "Epoch 113/200, Loss: 0.00001679 Test loss: 0.00001664\n",
      "Epoch 114/200, Loss: 0.00002498 Test loss: 0.00002588\n",
      "Epoch 115/200, Loss: 0.00001652 Test loss: 0.00001352\n",
      "Epoch 116/200, Loss: 0.00001494 Test loss: 0.00001710\n",
      "Epoch 117/200, Loss: 0.00001640 Test loss: 0.00001344\n",
      "Epoch 118/200, Loss: 0.00001892 Test loss: 0.00001341\n",
      "Epoch 119/200, Loss: 0.00002221 Test loss: 0.00002486\n",
      "Epoch 120/200, Loss: 0.00001550 Test loss: 0.00001280\n",
      "Epoch 121/200, Loss: 0.00001509 Test loss: 0.00001622\n",
      "Epoch 122/200, Loss: 0.00001653 Test loss: 0.00002008\n",
      "Epoch 123/200, Loss: 0.00001542 Test loss: 0.00001400\n",
      "Epoch 124/200, Loss: 0.00001622 Test loss: 0.00001344\n",
      "Epoch 125/200, Loss: 0.00001616 Test loss: 0.00001492\n",
      "Epoch 126/200, Loss: 0.00001857 Test loss: 0.00001520\n",
      "Epoch 127/200, Loss: 0.00001501 Test loss: 0.00002545\n",
      "Epoch 128/200, Loss: 0.00001527 Test loss: 0.00002309\n",
      "Epoch 129/200, Loss: 0.00001813 Test loss: 0.00001387\n",
      "Epoch 130/200, Loss: 0.00001605 Test loss: 0.00001321\n",
      "Epoch 131/200, Loss: 0.00001770 Test loss: 0.00001262\n",
      "Epoch 132/200, Loss: 0.00001505 Test loss: 0.00001367\n",
      "Epoch 133/200, Loss: 0.00002256 Test loss: 0.00001806\n",
      "Epoch 134/200, Loss: 0.00001278 Test loss: 0.00001284\n",
      "Epoch 135/200, Loss: 0.00001370 Test loss: 0.00001461\n",
      "Epoch 136/200, Loss: 0.00001640 Test loss: 0.00001312\n",
      "Epoch 137/200, Loss: 0.00001392 Test loss: 0.00001392\n",
      "Epoch 138/200, Loss: 0.00001561 Test loss: 0.00001237\n",
      "Epoch 139/200, Loss: 0.00001975 Test loss: 0.00003779\n",
      "Epoch 140/200, Loss: 0.00001638 Test loss: 0.00001223\n",
      "Epoch 141/200, Loss: 0.00001270 Test loss: 0.00001148\n",
      "Epoch 142/200, Loss: 0.00001489 Test loss: 0.00001131\n",
      "Epoch 143/200, Loss: 0.00001498 Test loss: 0.00001172\n",
      "Epoch 144/200, Loss: 0.00001327 Test loss: 0.00003992\n",
      "Epoch 145/200, Loss: 0.00001657 Test loss: 0.00001128\n",
      "Epoch 146/200, Loss: 0.00001539 Test loss: 0.00001487\n",
      "Epoch 147/200, Loss: 0.00001475 Test loss: 0.00001369\n",
      "Epoch 148/200, Loss: 0.00001335 Test loss: 0.00001205\n",
      "Epoch 149/200, Loss: 0.00001919 Test loss: 0.00001178\n",
      "Epoch 150/200, Loss: 0.00001295 Test loss: 0.00001361\n",
      "Epoch 151/200, Loss: 0.00001304 Test loss: 0.00003308\n",
      "Epoch 152/200, Loss: 0.00001442 Test loss: 0.00001184\n",
      "Epoch 153/200, Loss: 0.00001666 Test loss: 0.00001070\n",
      "Epoch 154/200, Loss: 0.00001534 Test loss: 0.00001156\n",
      "Epoch 155/200, Loss: 0.00001193 Test loss: 0.00001840\n",
      "Epoch 156/200, Loss: 0.00001493 Test loss: 0.00001088\n",
      "Epoch 157/200, Loss: 0.00001780 Test loss: 0.00001056\n",
      "Epoch 158/200, Loss: 0.00001300 Test loss: 0.00002165\n",
      "Epoch 159/200, Loss: 0.00001224 Test loss: 0.00001076\n",
      "Epoch 160/200, Loss: 0.00001454 Test loss: 0.00002643\n",
      "Epoch 161/200, Loss: 0.00001315 Test loss: 0.00002107\n",
      "Epoch 162/200, Loss: 0.00001467 Test loss: 0.00001213\n",
      "Epoch 163/200, Loss: 0.00001309 Test loss: 0.00001372\n",
      "Epoch 164/200, Loss: 0.00001706 Test loss: 0.00001034\n",
      "Epoch 165/200, Loss: 0.00001234 Test loss: 0.00001153\n",
      "Epoch 166/200, Loss: 0.00001573 Test loss: 0.00002230\n",
      "Epoch 167/200, Loss: 0.00001146 Test loss: 0.00001077\n",
      "Epoch 168/200, Loss: 0.00001309 Test loss: 0.00001080\n",
      "Epoch 169/200, Loss: 0.00001360 Test loss: 0.00001773\n",
      "Epoch 170/200, Loss: 0.00001357 Test loss: 0.00001298\n",
      "Epoch 171/200, Loss: 0.00001248 Test loss: 0.00002078\n",
      "Epoch 172/200, Loss: 0.00001267 Test loss: 0.00001994\n",
      "Epoch 173/200, Loss: 0.00001369 Test loss: 0.00001007\n",
      "Epoch 174/200, Loss: 0.00001393 Test loss: 0.00004311\n",
      "Epoch 175/200, Loss: 0.00001448 Test loss: 0.00001165\n",
      "Epoch 176/200, Loss: 0.00001257 Test loss: 0.00001145\n",
      "Epoch 177/200, Loss: 0.00001300 Test loss: 0.00002625\n",
      "Epoch 178/200, Loss: 0.00001152 Test loss: 0.00001065\n",
      "Epoch 179/200, Loss: 0.00001366 Test loss: 0.00001097\n",
      "Epoch 180/200, Loss: 0.00001549 Test loss: 0.00001229\n",
      "Epoch 181/200, Loss: 0.00001122 Test loss: 0.00001211\n",
      "Epoch 182/200, Loss: 0.00001130 Test loss: 0.00001479\n",
      "Epoch 183/200, Loss: 0.00001293 Test loss: 0.00001044\n",
      "Epoch 184/200, Loss: 0.00001391 Test loss: 0.00001025\n",
      "Epoch 185/200, Loss: 0.00001203 Test loss: 0.00000948\n",
      "Epoch 186/200, Loss: 0.00001483 Test loss: 0.00000912\n",
      "Epoch 187/200, Loss: 0.00001238 Test loss: 0.00001203\n",
      "Epoch 188/200, Loss: 0.00001033 Test loss: 0.00001322\n",
      "Epoch 189/200, Loss: 0.00001221 Test loss: 0.00000934\n",
      "Epoch 190/200, Loss: 0.00001640 Test loss: 0.00000913\n",
      "Epoch 191/200, Loss: 0.00001131 Test loss: 0.00001567\n",
      "Epoch 192/200, Loss: 0.00001056 Test loss: 0.00000940\n",
      "Epoch 193/200, Loss: 0.00001540 Test loss: 0.00001024\n",
      "Epoch 194/200, Loss: 0.00000972 Test loss: 0.00000948\n",
      "Epoch 195/200, Loss: 0.00001106 Test loss: 0.00000947\n",
      "Epoch 196/200, Loss: 0.00001330 Test loss: 0.00001074\n",
      "Epoch 197/200, Loss: 0.00001497 Test loss: 0.00001139\n",
      "Epoch 198/200, Loss: 0.00000960 Test loss: 0.00001102\n",
      "Epoch 199/200, Loss: 0.00001176 Test loss: 0.00000939\n",
      "Epoch 200/200, Loss: 0.00001159 Test loss: 0.00004322\n"
     ]
    }
   ],
   "execution_count": 160
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-18T09:30:34.136669Z",
     "start_time": "2025-06-18T09:30:34.110776Z"
    }
   },
   "cell_type": "code",
   "source": "torch.save(model, \"data/models/torch/autoencoder_exp.pt\")",
   "id": "6e5a9e294045c74c",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-18T11:28:45.986048Z",
     "start_time": "2025-06-18T11:28:45.957583Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model = torch.load(\"data/models/torch/autoencoder_exp.pt\", weights_only=False)\n",
    "model.eval()"
   ],
   "id": "f8af0cb897395d7c",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Autoencoder(\n",
       "  (encoder): Sequential(\n",
       "    (0): Linear(in_features=448, out_features=128, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=128, out_features=64, bias=True)\n",
       "    (3): ReLU()\n",
       "    (4): Linear(in_features=64, out_features=16, bias=True)\n",
       "  )\n",
       "  (decoder): Sequential(\n",
       "    (0): Linear(in_features=16, out_features=64, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=64, out_features=128, bias=True)\n",
       "    (3): ReLU()\n",
       "    (4): Linear(in_features=128, out_features=448, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 184
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-18T11:28:48.213951Z",
     "start_time": "2025-06-18T11:28:47.930524Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model.eval()\n",
    "latent_vectors_train = model.encoder(torch.tensor(x_train, dtype=torch.float32).to(device)).detach().cpu().numpy()\n",
    "latent_vectors_test = model.encoder(torch.tensor(x_test, dtype=torch.float32).to(device)).detach().cpu().numpy()\n",
    "latent_vectors_test_exp = model.encoder(torch.tensor(x_test_exp, dtype=torch.float32).to(device)).detach().cpu().numpy()"
   ],
   "id": "59307777a8d5284",
   "outputs": [],
   "execution_count": 185
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-18T10:56:37.150653Z",
     "start_time": "2025-06-18T10:56:37.137838Z"
    }
   },
   "cell_type": "code",
   "source": "decoded = model.decoder(model.encoder(torch.tensor(x_test_exp, dtype=torch.float32).to(device))).detach().cpu().numpy()",
   "id": "a95fe3351e57df09",
   "outputs": [],
   "execution_count": 162
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-18T10:56:37.650886Z",
     "start_time": "2025-06-18T10:56:37.556350Z"
    }
   },
   "cell_type": "code",
   "source": [
    "figure, axis = plt.subplots(3, 3)\n",
    "for i in range(3):\n",
    "    for j in range(3):\n",
    "        k=np.random.randint(0, len(decoded))\n",
    "        axis[i, j].plot(pressures[:-10], x_test_exp[k], marker=\".\", label = \"origin\")\n",
    "        axis[i, j].plot(pressures[:-10], decoded[k], marker=\".\", label = \"decoded\")\n",
    "        axis[i, j].grid(True)\n",
    "axis[i, j].legend()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# k=np.random.randint(0, len(decoded))\n",
    "# plt.plot(pressures[:-10], x_test_exp[k], marker=\".\", label = \"origin\")\n",
    "# plt.plot(pressures[:-10], decoded[k], marker=\".\", label = \"decoded\")\n",
    "# plt.legend()\n",
    "# plt.grid(True)\n",
    "# plt.show()"
   ],
   "id": "dbe5444885056917",
   "outputs": [],
   "execution_count": 163
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-18T10:55:32.947447Z",
     "start_time": "2025-06-18T10:55:32.395234Z"
    }
   },
   "cell_type": "code",
   "source": [
    "np.random.seed(0)\n",
    "labels = None \n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "latent_pca = pca.fit_transform(latent_vectors_train[:100])\n",
    "latent_pca_exp = pca.fit_transform(latent_vectors_test[:100])\n",
    "\n",
    "tsne = TSNE(n_components=2, init='pca', random_state=0)\n",
    "latent_tsne = tsne.fit_transform(latent_vectors_train[:100])\n",
    "latent_tsne_exp = tsne.fit_transform(latent_vectors_test[:100])\n",
    "\n",
    "plt.figure()\n",
    "plt.scatter(latent_pca[:, 0], latent_pca[:, 1], label=\"train\")\n",
    "plt.scatter(latent_pca_exp[:, 0], latent_pca_exp[:, 1], label=\"exp\")\n",
    "plt.title(\"PCA of Latent Space\")\n",
    "plt.xlabel(\"PC1\")\n",
    "plt.ylabel(\"PC2\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "plt.scatter(latent_tsne[:, 0], latent_tsne[:, 1], label=\"train\")\n",
    "plt.scatter(latent_tsne_exp[:, 0], latent_tsne_exp[:, 1], label=\"exp\")\n",
    "for i in range(latent_tsne_exp.shape[0]):\n",
    "        plt.text(latent_tsne_exp[i, 0], latent_tsne_exp[i, 1], str(i), fontsize=8, ha='center', va='center')\n",
    "plt.title(\"t-SNE of Latent Space\")\n",
    "plt.xlabel(\"Dim 1\")\n",
    "plt.ylabel(\"Dim 2\")\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "id": "f578073be61beebe",
   "outputs": [],
   "execution_count": 158
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-09T12:16:27.311899Z",
     "start_time": "2025-06-09T12:16:27.266277Z"
    }
   },
   "cell_type": "code",
   "source": [
    "plt.plot(accuracies, marker=\".\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ],
   "id": "ef1c6a399de52286",
   "outputs": [],
   "execution_count": 34
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-18T04:26:01.151259Z",
     "start_time": "2025-06-18T04:26:01.119586Z"
    }
   },
   "cell_type": "code",
   "source": "preds = bst.predict(latent_vectors_test_exp)",
   "id": "f0c2f20f26efef7d",
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'bst' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[12], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m preds \u001B[38;5;241m=\u001B[39m \u001B[43mbst\u001B[49m\u001B[38;5;241m.\u001B[39mpredict(latent_vectors_test_exp)\n",
      "\u001B[1;31mNameError\u001B[0m: name 'bst' is not defined"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-18T09:55:12.695824Z",
     "start_time": "2025-06-18T09:55:12.680863Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def plot_preds(x, y, preds): \n",
    "    NX, NY = 3, 4\n",
    "    figure, axis = plt.subplots(NX, NY)\n",
    "    for i in range(NX):\n",
    "        for j in range(NY):\n",
    "            k = np.random.randint(0, len(preds)) \n",
    "            iso_axis = axis[i, j].twiny()\n",
    "            iso_axis.set_xlabel(\"P/P$^0$\",fontsize=8)\n",
    "            iso_axis.plot(pressures[:-10], x[k], label=\"Isotherm\", color = 'green')\n",
    "            kernel = (data_sorb.T[:-10])\n",
    "            iso_axis.plot(pressures[:-10], np.dot(kernel, preds[k][:128]), label=\"Isotherm by model\", color=\"red\")\n",
    "            axis[i, j].set_title(f\"â„– {k}\")\n",
    "            axis[i, j].title.set_size(10)\n",
    "            axis[i, j].grid()\n",
    "            axis[i, j].set_xlabel(\"nm\",fontsize=8)\n",
    "            axis[i, j].plot(pore_widths, (preds[k]), marker=\".\", label=f\"Model PSD\")\n",
    "            axis[i, j].plot(pore_widths, y[k], marker=\".\", label=\"PSD\")\n",
    "    plt.subplots_adjust(hspace=0.6, right=0.95, left=0.05, bottom=0.05, top=0.9)\n",
    "    plt.legend()\n",
    "    axis[0, 0].legend()\n",
    "    plt.show()"
   ],
   "id": "a07cbe66d234c80f",
   "outputs": [],
   "execution_count": 32
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-11T07:54:34.696430Z",
     "start_time": "2025-06-11T07:54:34.349935Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from tools import model_tester\n",
    "from inverse import fit_linear\n",
    "\n",
    "error_lst, roughness_lst = model_tester.test_model_predictions(preds, x_test_exp, kernel=data_sorb[:, :-10])\n",
    "kde_x, kde_error, kde_fun = model_tester.calculate_kde_data(error_lst, stop=150)\n",
    "print(\"average error:\", np.mean(error_lst))\n",
    "plt.plot(kde_x, kde_error, label=model_name)\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.plot()"
   ],
   "id": "1316a794fbe60afa",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average error: 19.576227837865584\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 56
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-18T10:31:57.908940Z",
     "start_time": "2025-06-18T10:31:57.834628Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class PSD_model(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(PSD_model, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_dim, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.15),\n",
    "            nn.Linear(32, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.15),\n",
    "            nn.Linear(64, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.15),\n",
    "            nn.Linear(128, output_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        psd = self.model(x)\n",
    "        return psd\n",
    "\n",
    "class Isotherm_PSD_Dataset(Dataset):\n",
    "    def __init__(self, x, y, transform=None):\n",
    "        self.x = torch.tensor(x, dtype=torch.float32).to(device)\n",
    "        self.y = torch.tensor(y, dtype=torch.float32).to(device)\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = self.x[idx]\n",
    "        y = self.y[idx]\n",
    "        if self.transform:\n",
    "            x = self.transform(x)\n",
    "        return x, y\n",
    "\n",
    "train_PSD = Isotherm_PSD_Dataset(latent_vectors_train, (y_train))\n",
    "test_PSD = Isotherm_PSD_Dataset(latent_vectors_test, (y_test))\n",
    "\n",
    "batch_size = 512\n",
    "PSD_loader = DataLoader(train_PSD, batch_size=batch_size, shuffle=True)\n",
    "PSD_loader_test = DataLoader(test_PSD, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "\n",
    "model_PSD = PSD_model(input_dim=latent_dim, output_dim=128)\n",
    "model_PSD.to(device)\n",
    "\n",
    "optimizer = optim.Adam(model_PSD.parameters(), lr=learning_rate)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "def train_PSD_model(model, loader, loader_test):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    total_vloss = 0\n",
    "    for x, y in loader:\n",
    "        optimizer.zero_grad()\n",
    "        y_recon = model(x)\n",
    "        loss = criterion(y_recon, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for x, y in loader_test:\n",
    "            y_recon  = model(x)\n",
    "            vloss = criterion(y_recon, y)\n",
    "            total_vloss += vloss.item()\n",
    "\n",
    "    return total_loss / len(loader.dataset), total_vloss / len(loader_test.dataset)\n"
   ],
   "id": "e5e1fac9c7ab87a1",
   "outputs": [],
   "execution_count": 107
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-18T10:32:58.242966Z",
     "start_time": "2025-06-18T10:31:58.867730Z"
    }
   },
   "cell_type": "code",
   "source": [
    "epochs = 100\n",
    "loss_lst = []\n",
    "vloss_lst = []\n",
    "for epoch in range(1, epochs+1):\n",
    "    loss, vloss = train_PSD_model(model_PSD, PSD_loader, PSD_loader_test)\n",
    "    loss_lst.append(loss)\n",
    "    vloss_lst.append(vloss)\n",
    "    if epoch % 1 == 0:\n",
    "        print(f\"Epoch {epoch}/{epochs}, Loss: {loss*100:.8f} Test loss: {vloss*100:.8f}\")"
   ],
   "id": "2fd72aae7361b860",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100, Loss: 0.00176239 Test loss: 0.00079351\n",
      "Epoch 2/100, Loss: 0.00078187 Test loss: 0.00057135\n",
      "Epoch 3/100, Loss: 0.00063825 Test loss: 0.00056636\n",
      "Epoch 4/100, Loss: 0.00057047 Test loss: 0.00046190\n",
      "Epoch 5/100, Loss: 0.00053094 Test loss: 0.00049900\n",
      "Epoch 6/100, Loss: 0.00049706 Test loss: 0.00045661\n",
      "Epoch 7/100, Loss: 0.00047023 Test loss: 0.00041262\n",
      "Epoch 8/100, Loss: 0.00044908 Test loss: 0.00042090\n",
      "Epoch 9/100, Loss: 0.00043458 Test loss: 0.00040962\n",
      "Epoch 10/100, Loss: 0.00042521 Test loss: 0.00044432\n",
      "Epoch 11/100, Loss: 0.00040947 Test loss: 0.00042271\n",
      "Epoch 12/100, Loss: 0.00039602 Test loss: 0.00041472\n",
      "Epoch 13/100, Loss: 0.00038916 Test loss: 0.00036237\n",
      "Epoch 14/100, Loss: 0.00039080 Test loss: 0.00037144\n",
      "Epoch 15/100, Loss: 0.00038547 Test loss: 0.00038281\n",
      "Epoch 16/100, Loss: 0.00037768 Test loss: 0.00036573\n",
      "Epoch 17/100, Loss: 0.00037764 Test loss: 0.00038030\n",
      "Epoch 18/100, Loss: 0.00036761 Test loss: 0.00045962\n",
      "Epoch 19/100, Loss: 0.00037904 Test loss: 0.00037099\n",
      "Epoch 20/100, Loss: 0.00037042 Test loss: 0.00038497\n",
      "Epoch 21/100, Loss: 0.00036270 Test loss: 0.00037395\n",
      "Epoch 22/100, Loss: 0.00035545 Test loss: 0.00036197\n",
      "Epoch 23/100, Loss: 0.00034824 Test loss: 0.00037636\n",
      "Epoch 24/100, Loss: 0.00034991 Test loss: 0.00043195\n",
      "Epoch 25/100, Loss: 0.00035189 Test loss: 0.00035875\n",
      "Epoch 26/100, Loss: 0.00034190 Test loss: 0.00040777\n",
      "Epoch 27/100, Loss: 0.00035500 Test loss: 0.00035314\n",
      "Epoch 28/100, Loss: 0.00034197 Test loss: 0.00038610\n",
      "Epoch 29/100, Loss: 0.00034047 Test loss: 0.00035755\n",
      "Epoch 30/100, Loss: 0.00033447 Test loss: 0.00035882\n",
      "Epoch 31/100, Loss: 0.00033395 Test loss: 0.00036270\n",
      "Epoch 32/100, Loss: 0.00033646 Test loss: 0.00034080\n",
      "Epoch 33/100, Loss: 0.00033115 Test loss: 0.00035396\n",
      "Epoch 34/100, Loss: 0.00033764 Test loss: 0.00061442\n",
      "Epoch 35/100, Loss: 0.00039769 Test loss: 0.00042383\n",
      "Epoch 36/100, Loss: 0.00035840 Test loss: 0.00060228\n",
      "Epoch 37/100, Loss: 0.00039546 Test loss: 0.00037013\n",
      "Epoch 38/100, Loss: 0.00034384 Test loss: 0.00038494\n",
      "Epoch 39/100, Loss: 0.00033868 Test loss: 0.00038797\n",
      "Epoch 40/100, Loss: 0.00032383 Test loss: 0.00036311\n",
      "Epoch 41/100, Loss: 0.00032642 Test loss: 0.00038122\n",
      "Epoch 42/100, Loss: 0.00032418 Test loss: 0.00036114\n",
      "Epoch 43/100, Loss: 0.00032570 Test loss: 0.00038464\n",
      "Epoch 44/100, Loss: 0.00032171 Test loss: 0.00038148\n",
      "Epoch 45/100, Loss: 0.00032039 Test loss: 0.00037457\n",
      "Epoch 46/100, Loss: 0.00032743 Test loss: 0.00033757\n",
      "Epoch 47/100, Loss: 0.00032610 Test loss: 0.00036165\n",
      "Epoch 48/100, Loss: 0.00031852 Test loss: 0.00032739\n",
      "Epoch 49/100, Loss: 0.00031588 Test loss: 0.00034736\n",
      "Epoch 50/100, Loss: 0.00031353 Test loss: 0.00035689\n",
      "Epoch 51/100, Loss: 0.00031138 Test loss: 0.00036327\n",
      "Epoch 52/100, Loss: 0.00031039 Test loss: 0.00034652\n",
      "Epoch 53/100, Loss: 0.00030637 Test loss: 0.00033839\n",
      "Epoch 54/100, Loss: 0.00031462 Test loss: 0.00038418\n",
      "Epoch 55/100, Loss: 0.00030531 Test loss: 0.00035078\n",
      "Epoch 56/100, Loss: 0.00031129 Test loss: 0.00036559\n",
      "Epoch 57/100, Loss: 0.00030893 Test loss: 0.00037826\n",
      "Epoch 58/100, Loss: 0.00030758 Test loss: 0.00039464\n",
      "Epoch 59/100, Loss: 0.00030477 Test loss: 0.00035647\n",
      "Epoch 60/100, Loss: 0.00030868 Test loss: 0.00033055\n",
      "Epoch 61/100, Loss: 0.00030611 Test loss: 0.00035544\n",
      "Epoch 62/100, Loss: 0.00030453 Test loss: 0.00035008\n",
      "Epoch 63/100, Loss: 0.00029913 Test loss: 0.00037559\n",
      "Epoch 64/100, Loss: 0.00030732 Test loss: 0.00034906\n",
      "Epoch 65/100, Loss: 0.00029983 Test loss: 0.00034850\n",
      "Epoch 66/100, Loss: 0.00030024 Test loss: 0.00035534\n",
      "Epoch 67/100, Loss: 0.00031057 Test loss: 0.00036889\n",
      "Epoch 68/100, Loss: 0.00030448 Test loss: 0.00033895\n",
      "Epoch 69/100, Loss: 0.00030121 Test loss: 0.00033918\n",
      "Epoch 70/100, Loss: 0.00029548 Test loss: 0.00036180\n",
      "Epoch 71/100, Loss: 0.00030128 Test loss: 0.00034300\n",
      "Epoch 72/100, Loss: 0.00029224 Test loss: 0.00033507\n",
      "Epoch 73/100, Loss: 0.00029319 Test loss: 0.00040422\n",
      "Epoch 74/100, Loss: 0.00029720 Test loss: 0.00039401\n",
      "Epoch 75/100, Loss: 0.00029465 Test loss: 0.00035681\n",
      "Epoch 76/100, Loss: 0.00029191 Test loss: 0.00031559\n",
      "Epoch 77/100, Loss: 0.00029733 Test loss: 0.00035745\n",
      "Epoch 78/100, Loss: 0.00029182 Test loss: 0.00037162\n",
      "Epoch 79/100, Loss: 0.00028819 Test loss: 0.00037378\n",
      "Epoch 80/100, Loss: 0.00029002 Test loss: 0.00037767\n",
      "Epoch 81/100, Loss: 0.00028997 Test loss: 0.00034717\n",
      "Epoch 82/100, Loss: 0.00029027 Test loss: 0.00035997\n",
      "Epoch 83/100, Loss: 0.00029197 Test loss: 0.00037453\n",
      "Epoch 84/100, Loss: 0.00028927 Test loss: 0.00032781\n",
      "Epoch 85/100, Loss: 0.00028401 Test loss: 0.00034231\n",
      "Epoch 86/100, Loss: 0.00029083 Test loss: 0.00035038\n",
      "Epoch 87/100, Loss: 0.00028065 Test loss: 0.00031303\n",
      "Epoch 88/100, Loss: 0.00029123 Test loss: 0.00036997\n",
      "Epoch 89/100, Loss: 0.00029043 Test loss: 0.00036747\n",
      "Epoch 90/100, Loss: 0.00028475 Test loss: 0.00035352\n",
      "Epoch 91/100, Loss: 0.00029656 Test loss: 0.00034711\n",
      "Epoch 92/100, Loss: 0.00028902 Test loss: 0.00033824\n",
      "Epoch 93/100, Loss: 0.00029447 Test loss: 0.00037815\n",
      "Epoch 94/100, Loss: 0.00028700 Test loss: 0.00032472\n",
      "Epoch 95/100, Loss: 0.00028942 Test loss: 0.00039608\n",
      "Epoch 96/100, Loss: 0.00029307 Test loss: 0.00036241\n",
      "Epoch 97/100, Loss: 0.00028818 Test loss: 0.00032606\n",
      "Epoch 98/100, Loss: 0.00028441 Test loss: 0.00032130\n",
      "Epoch 99/100, Loss: 0.00028244 Test loss: 0.00032782\n",
      "Epoch 100/100, Loss: 0.00028454 Test loss: 0.00032195\n"
     ]
    }
   ],
   "execution_count": 108
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-18T10:32:59.923174Z",
     "start_time": "2025-06-18T10:32:59.883331Z"
    }
   },
   "cell_type": "code",
   "source": [
    "plt.plot(loss_lst)\n",
    "plt.plot(vloss_lst)\n",
    "plt.show()"
   ],
   "id": "826613b63fcbd7f1",
   "outputs": [],
   "execution_count": 109
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-18T10:01:49.937669Z",
     "start_time": "2025-06-18T10:01:49.922015Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model_name = \"autoencoder_regressor_lognormal\"\n",
    "torch.save(model_PSD, f\"data/models/torch/{model_name}\") "
   ],
   "id": "8b63396d69bd925e",
   "outputs": [],
   "execution_count": 49
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-18T11:27:27.826683Z",
     "start_time": "2025-06-18T11:27:27.814216Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model_name = \"autoencoder_regressor\"\n",
    "model_PSD = torch.load(f\"data/models/torch/{model_name}\", weights_only=False)"
   ],
   "id": "5df22fca0b595bb7",
   "outputs": [],
   "execution_count": 177
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-18T11:28:54.173732Z",
     "start_time": "2025-06-18T11:28:53.985262Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model_PSD.eval()\n",
    "y_train_PSD = model_PSD.model(torch.tensor(latent_vectors_train, dtype=torch.float32).to(device)).detach().cpu().numpy()\n",
    "y_test_PSD = model_PSD.model(torch.tensor(latent_vectors_test, dtype=torch.float32).to(device)).detach().cpu().numpy()\n",
    "y_test_exp_PSD = model_PSD.model(torch.tensor(latent_vectors_test_exp, dtype=torch.float32).to(device)).detach().cpu().numpy()"
   ],
   "id": "bd320d01dab9040b",
   "outputs": [],
   "execution_count": 186
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-18T11:39:01.695464Z",
     "start_time": "2025-06-18T11:39:01.552808Z"
    }
   },
   "cell_type": "code",
   "source": "plot_preds(x_test_exp, y_test_exp, y_test_exp_PSD)",
   "id": "424bd83f3fe2a9f9",
   "outputs": [],
   "execution_count": 188
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-18T10:33:34.191424Z",
     "start_time": "2025-06-18T10:33:34.175738Z"
    }
   },
   "cell_type": "code",
   "source": [
    "np.savez(f\"data/models/metrics/{model_name}\", x=x_test_exp, y=y_test_exp_PSD)\n",
    "model_name\n"
   ],
   "id": "b11eabe3085b3dc5",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'autoencoder_regressor_lognormal'"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 112
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "89e0e8e7540ebf2e"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
