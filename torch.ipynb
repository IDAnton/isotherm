{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-07-10T06:20:58.981888Z",
     "start_time": "2025-07-10T06:20:53.939844Z"
    }
   },
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from datasetLoader import load_dataset\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "from inverse import fit_linear\n",
    "from tools import model_tester\n",
    "%matplotlib qt"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-10T06:20:58.997854Z",
     "start_time": "2025-07-10T06:20:58.981888Z"
    }
   },
   "cell_type": "code",
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ],
   "id": "d322819b15356399",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-10T07:33:04.479656Z",
     "start_time": "2025-07-10T07:32:59.947197Z"
    }
   },
   "cell_type": "code",
   "source": [
    "pore_widths = np.load(\"data/initial kernels/Size_Kernel_Silica_Adsorption.npy\")\n",
    "pressures = np.load(\"data/initial kernels/Pressure_Silica.npy\")\n",
    "with open(\"data/initial kernels/Kernel_Silica_Adsorption.npy\", 'rb') as f:\n",
    "    data_sorb = np.load(f)\n",
    "\n",
    "x, y = load_dataset('data/datasets/silica_random_combined.npz')\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.15, random_state=1)\n",
    "\n",
    "x_exp, y_exp = load_dataset('data/datasets/exp.npz')\n",
    "\n",
    "x_train_exp, x_test_exp, y_train_exp, y_test_exp = train_test_split(x_exp, y_exp, test_size=0.15, random_state=1)"
   ],
   "id": "d29956229295484c",
   "outputs": [],
   "execution_count": 105
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-10T07:12:22.732474Z",
     "start_time": "2025-07-10T07:12:22.621874Z"
    }
   },
   "cell_type": "code",
   "source": [
    "figure, axis = plt.subplots(3, 4)\n",
    "for i in range(3):\n",
    "    for j in range(4):\n",
    "        k = np.random.randint(0, len(x_train_exp))\n",
    "        axis[i, j].plot(pressures[:-10], x_train_exp[k], marker=\".\")\n",
    "        axis[i, j].grid()\n",
    "plt.show()"
   ],
   "id": "56cb2ebf581218f4",
   "outputs": [],
   "execution_count": 103
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-10T07:12:26.066223Z",
     "start_time": "2025-07-10T07:12:25.959080Z"
    }
   },
   "cell_type": "code",
   "source": [
    "figure, axis = plt.subplots(3, 4)\n",
    "for i in range(3):\n",
    "    for j in range(4):\n",
    "        k = np.random.randint(0, len(y_train_exp))\n",
    "        axis[i, j].plot(pore_widths, y_train_exp[k], marker=\".\")\n",
    "        axis[i, j].grid()\n",
    "plt.show()"
   ],
   "id": "b92642b436f09b6e",
   "outputs": [],
   "execution_count": 104
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-10T07:10:38.203706Z",
     "start_time": "2025-07-10T07:10:37.934928Z"
    }
   },
   "cell_type": "code",
   "source": [
    "figure, axis = plt.subplots(3, 4)\n",
    "for i in range(3):\n",
    "    for j in range(4):\n",
    "        k = np.random.randint(0, len(x_train))\n",
    "        axis[i, j].plot(pore_widths, y_train[k], marker=\".\")\n",
    "        axis[i, j].grid()\n",
    "plt.show()"
   ],
   "id": "15f9ff24438da3f5",
   "outputs": [],
   "execution_count": 100
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-09T09:49:44.819944Z",
     "start_time": "2025-07-09T09:49:44.756113Z"
    }
   },
   "cell_type": "code",
   "source": "plt.plot(pore_widths, sum(y_train), marker=\".\")",
   "id": "d204e5d61104e3ed",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x20c1e23cd30>]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-09T09:49:44.913859Z",
     "start_time": "2025-07-09T09:49:44.898572Z"
    }
   },
   "cell_type": "code",
   "source": [
    "i = np.random.randint(0, len(x_train))\n",
    "plt.plot(pressures[:-10], x_train[i], marker=\".\")\n",
    "plt.grid()\n",
    "plt.show()"
   ],
   "id": "3201b1f78762f4ff",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-10T06:53:59.845062Z",
     "start_time": "2025-07-10T06:53:59.561455Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class IsothermDataset(Dataset):\n",
    "    def __init__(self, isotherms, transform=None):\n",
    "        self.data = torch.tensor(isotherms, dtype=torch.float32).to(device)\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = self.data[idx]\n",
    "        if self.transform:\n",
    "            x = self.transform(x)\n",
    "        return x, x\n",
    "\n",
    "x_mixed_train = np.concatenate((x_train_exp, x_train))\n",
    "x_mixed_test = np.concatenate((x_test_exp, x_test))\n",
    "\n",
    "# dataset = IsothermDataset(np.concatenate((x_train_exp, x_train_exp)))\n",
    "# dataset_test = IsothermDataset(np.concatenate((x_test_exp, x_test_exp)))\n",
    "dataset = IsothermDataset(np.concatenate((x_mixed_train, x_mixed_train)))\n",
    "dataset_test = IsothermDataset(np.concatenate((x_mixed_test, x_mixed_test)))\n",
    "\n",
    "\n",
    "batch_size = 512\n",
    "loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "loader_test = DataLoader(dataset_test, batch_size=batch_size, shuffle=False)"
   ],
   "id": "5cefc1b65fc7fa63",
   "outputs": [],
   "execution_count": 58
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-10T06:54:02.020539Z",
     "start_time": "2025-07-10T06:54:00.196670Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self, input_dim, latent_dim):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, latent_dim)\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(latent_dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, input_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        z = self.encoder(x)\n",
    "        x_recon = self.decoder(z)\n",
    "        return x_recon, z\n",
    "\n",
    "input_dim = 448\n",
    "latent_dim = 16\n",
    "epochs = 200\n",
    "learning_rate = 1e-3\n",
    "\n",
    "model = Autoencoder(input_dim=input_dim, latent_dim=latent_dim)\n",
    "model.to(device)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "def train_autoencoder(model, loader, loader_test):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    total_vloss = 0\n",
    "    for x, _ in loader:\n",
    "        optimizer.zero_grad()\n",
    "        x_recon, _ = model(x)\n",
    "        loss = criterion(x_recon, x)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for x, _ in loader_test:\n",
    "            x_recon, _  = model(x)\n",
    "            vloss = criterion(x_recon, x)\n",
    "            total_vloss += vloss.item()\n",
    "\n",
    "    return total_loss / len(loader.dataset), total_vloss / len(loader_test.dataset)\n",
    "\n",
    "\n",
    "# sample_z = model.encoder(torch.tensor(isotherms_np[0], dtype=torch.float32))"
   ],
   "id": "c960463d00766e7c",
   "outputs": [],
   "execution_count": 59
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-10T06:57:17.049942Z",
     "start_time": "2025-07-10T06:54:04.115947Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for epoch in range(1, epochs+1):\n",
    "    loss, vloss = train_autoencoder(model, loader,loader_test)\n",
    "    if epoch % 1 == 0:\n",
    "        print(f\"Epoch {epoch}/{epochs}, Loss: {loss*100:.8f} Test loss: {vloss*100:.8f}\")"
   ],
   "id": "6f625ed77c7b7164",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200, Loss: 0.00250238 Test loss: 0.00012563\n",
      "Epoch 2/200, Loss: 0.00009064 Test loss: 0.00005624\n",
      "Epoch 3/200, Loss: 0.00004870 Test loss: 0.00004471\n",
      "Epoch 4/200, Loss: 0.00004029 Test loss: 0.00004083\n",
      "Epoch 5/200, Loss: 0.00003512 Test loss: 0.00003395\n",
      "Epoch 6/200, Loss: 0.00002848 Test loss: 0.00002491\n",
      "Epoch 7/200, Loss: 0.00002537 Test loss: 0.00002465\n",
      "Epoch 8/200, Loss: 0.00002375 Test loss: 0.00002189\n",
      "Epoch 9/200, Loss: 0.00002244 Test loss: 0.00002019\n",
      "Epoch 10/200, Loss: 0.00002127 Test loss: 0.00001826\n",
      "Epoch 11/200, Loss: 0.00001936 Test loss: 0.00001751\n",
      "Epoch 12/200, Loss: 0.00001834 Test loss: 0.00001899\n",
      "Epoch 13/200, Loss: 0.00001600 Test loss: 0.00001472\n",
      "Epoch 14/200, Loss: 0.00001587 Test loss: 0.00001311\n",
      "Epoch 15/200, Loss: 0.00001408 Test loss: 0.00001933\n",
      "Epoch 16/200, Loss: 0.00001433 Test loss: 0.00001377\n",
      "Epoch 17/200, Loss: 0.00001343 Test loss: 0.00001107\n",
      "Epoch 18/200, Loss: 0.00001258 Test loss: 0.00001111\n",
      "Epoch 19/200, Loss: 0.00001199 Test loss: 0.00001225\n",
      "Epoch 20/200, Loss: 0.00001187 Test loss: 0.00001292\n",
      "Epoch 21/200, Loss: 0.00001204 Test loss: 0.00000968\n",
      "Epoch 22/200, Loss: 0.00001126 Test loss: 0.00000980\n",
      "Epoch 23/200, Loss: 0.00001088 Test loss: 0.00001133\n",
      "Epoch 24/200, Loss: 0.00001075 Test loss: 0.00002352\n",
      "Epoch 25/200, Loss: 0.00001058 Test loss: 0.00000976\n",
      "Epoch 26/200, Loss: 0.00001055 Test loss: 0.00001059\n",
      "Epoch 27/200, Loss: 0.00001001 Test loss: 0.00001386\n",
      "Epoch 28/200, Loss: 0.00000985 Test loss: 0.00000856\n",
      "Epoch 29/200, Loss: 0.00000941 Test loss: 0.00001175\n",
      "Epoch 30/200, Loss: 0.00000940 Test loss: 0.00000902\n",
      "Epoch 31/200, Loss: 0.00000889 Test loss: 0.00000949\n",
      "Epoch 32/200, Loss: 0.00000883 Test loss: 0.00000773\n",
      "Epoch 33/200, Loss: 0.00000929 Test loss: 0.00000711\n",
      "Epoch 34/200, Loss: 0.00000812 Test loss: 0.00000729\n",
      "Epoch 35/200, Loss: 0.00000831 Test loss: 0.00000967\n",
      "Epoch 36/200, Loss: 0.00000807 Test loss: 0.00000698\n",
      "Epoch 37/200, Loss: 0.00000812 Test loss: 0.00000721\n",
      "Epoch 38/200, Loss: 0.00000800 Test loss: 0.00001159\n",
      "Epoch 39/200, Loss: 0.00000770 Test loss: 0.00000712\n",
      "Epoch 40/200, Loss: 0.00000753 Test loss: 0.00000791\n",
      "Epoch 41/200, Loss: 0.00000730 Test loss: 0.00000745\n",
      "Epoch 42/200, Loss: 0.00000722 Test loss: 0.00000783\n",
      "Epoch 43/200, Loss: 0.00000723 Test loss: 0.00000650\n",
      "Epoch 44/200, Loss: 0.00000701 Test loss: 0.00000573\n",
      "Epoch 45/200, Loss: 0.00000715 Test loss: 0.00000621\n",
      "Epoch 46/200, Loss: 0.00000679 Test loss: 0.00000555\n",
      "Epoch 47/200, Loss: 0.00000697 Test loss: 0.00001268\n",
      "Epoch 48/200, Loss: 0.00000666 Test loss: 0.00000562\n",
      "Epoch 49/200, Loss: 0.00000668 Test loss: 0.00000618\n",
      "Epoch 50/200, Loss: 0.00000638 Test loss: 0.00001412\n",
      "Epoch 51/200, Loss: 0.00000645 Test loss: 0.00000674\n",
      "Epoch 52/200, Loss: 0.00000627 Test loss: 0.00000619\n",
      "Epoch 53/200, Loss: 0.00000642 Test loss: 0.00000557\n",
      "Epoch 54/200, Loss: 0.00000604 Test loss: 0.00000558\n",
      "Epoch 55/200, Loss: 0.00000634 Test loss: 0.00000492\n",
      "Epoch 56/200, Loss: 0.00000574 Test loss: 0.00000526\n",
      "Epoch 57/200, Loss: 0.00000568 Test loss: 0.00000605\n",
      "Epoch 58/200, Loss: 0.00000561 Test loss: 0.00000489\n",
      "Epoch 59/200, Loss: 0.00000552 Test loss: 0.00000551\n",
      "Epoch 60/200, Loss: 0.00000582 Test loss: 0.00000560\n",
      "Epoch 61/200, Loss: 0.00000510 Test loss: 0.00000698\n",
      "Epoch 62/200, Loss: 0.00000553 Test loss: 0.00000456\n",
      "Epoch 63/200, Loss: 0.00000534 Test loss: 0.00000458\n",
      "Epoch 64/200, Loss: 0.00000524 Test loss: 0.00001061\n",
      "Epoch 65/200, Loss: 0.00000530 Test loss: 0.00000521\n",
      "Epoch 66/200, Loss: 0.00000500 Test loss: 0.00000545\n",
      "Epoch 67/200, Loss: 0.00000513 Test loss: 0.00000446\n",
      "Epoch 68/200, Loss: 0.00000504 Test loss: 0.00000465\n",
      "Epoch 69/200, Loss: 0.00000504 Test loss: 0.00000544\n",
      "Epoch 70/200, Loss: 0.00000495 Test loss: 0.00001017\n",
      "Epoch 71/200, Loss: 0.00000493 Test loss: 0.00000484\n",
      "Epoch 72/200, Loss: 0.00000491 Test loss: 0.00000517\n",
      "Epoch 73/200, Loss: 0.00000496 Test loss: 0.00000411\n",
      "Epoch 74/200, Loss: 0.00000473 Test loss: 0.00000434\n",
      "Epoch 75/200, Loss: 0.00000467 Test loss: 0.00000383\n",
      "Epoch 76/200, Loss: 0.00000500 Test loss: 0.00000555\n",
      "Epoch 77/200, Loss: 0.00000442 Test loss: 0.00000752\n",
      "Epoch 78/200, Loss: 0.00000468 Test loss: 0.00000626\n",
      "Epoch 79/200, Loss: 0.00000453 Test loss: 0.00000653\n",
      "Epoch 80/200, Loss: 0.00000460 Test loss: 0.00000451\n",
      "Epoch 81/200, Loss: 0.00000453 Test loss: 0.00000375\n",
      "Epoch 82/200, Loss: 0.00000457 Test loss: 0.00000550\n",
      "Epoch 83/200, Loss: 0.00000437 Test loss: 0.00000471\n",
      "Epoch 84/200, Loss: 0.00000440 Test loss: 0.00000374\n",
      "Epoch 85/200, Loss: 0.00000457 Test loss: 0.00000389\n",
      "Epoch 86/200, Loss: 0.00000420 Test loss: 0.00000394\n",
      "Epoch 87/200, Loss: 0.00000432 Test loss: 0.00000359\n",
      "Epoch 88/200, Loss: 0.00000409 Test loss: 0.00000709\n",
      "Epoch 89/200, Loss: 0.00000420 Test loss: 0.00000374\n",
      "Epoch 90/200, Loss: 0.00000433 Test loss: 0.00000441\n",
      "Epoch 91/200, Loss: 0.00000403 Test loss: 0.00000358\n",
      "Epoch 92/200, Loss: 0.00000406 Test loss: 0.00000634\n",
      "Epoch 93/200, Loss: 0.00000422 Test loss: 0.00000399\n",
      "Epoch 94/200, Loss: 0.00000405 Test loss: 0.00000378\n",
      "Epoch 95/200, Loss: 0.00000407 Test loss: 0.00000360\n",
      "Epoch 96/200, Loss: 0.00000420 Test loss: 0.00000606\n",
      "Epoch 97/200, Loss: 0.00000364 Test loss: 0.00000764\n",
      "Epoch 98/200, Loss: 0.00000392 Test loss: 0.00000776\n",
      "Epoch 99/200, Loss: 0.00000394 Test loss: 0.00000494\n",
      "Epoch 100/200, Loss: 0.00000381 Test loss: 0.00000352\n",
      "Epoch 101/200, Loss: 0.00000374 Test loss: 0.00000365\n",
      "Epoch 102/200, Loss: 0.00000386 Test loss: 0.00000363\n",
      "Epoch 103/200, Loss: 0.00000368 Test loss: 0.00000334\n",
      "Epoch 104/200, Loss: 0.00000356 Test loss: 0.00000354\n",
      "Epoch 105/200, Loss: 0.00000361 Test loss: 0.00000852\n",
      "Epoch 106/200, Loss: 0.00000364 Test loss: 0.00000726\n",
      "Epoch 107/200, Loss: 0.00000371 Test loss: 0.00000308\n",
      "Epoch 108/200, Loss: 0.00000360 Test loss: 0.00000381\n",
      "Epoch 109/200, Loss: 0.00000344 Test loss: 0.00000303\n",
      "Epoch 110/200, Loss: 0.00000353 Test loss: 0.00000422\n",
      "Epoch 111/200, Loss: 0.00000352 Test loss: 0.00000354\n",
      "Epoch 112/200, Loss: 0.00000345 Test loss: 0.00000319\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[60], line 2\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m epoch \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;241m1\u001B[39m, epochs\u001B[38;5;241m+\u001B[39m\u001B[38;5;241m1\u001B[39m):\n\u001B[1;32m----> 2\u001B[0m     loss, vloss \u001B[38;5;241m=\u001B[39m \u001B[43mtrain_autoencoder\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mloader\u001B[49m\u001B[43m,\u001B[49m\u001B[43mloader_test\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m      3\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m epoch \u001B[38;5;241m%\u001B[39m \u001B[38;5;241m1\u001B[39m \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[0;32m      4\u001B[0m         \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mEpoch \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mepoch\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m/\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mepochs\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m, Loss: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mloss\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m100\u001B[39m\u001B[38;5;132;01m:\u001B[39;00m\u001B[38;5;124m.8f\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m Test loss: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mvloss\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m100\u001B[39m\u001B[38;5;132;01m:\u001B[39;00m\u001B[38;5;124m.8f\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n",
      "Cell \u001B[1;32mIn[59], line 39\u001B[0m, in \u001B[0;36mtrain_autoencoder\u001B[1;34m(model, loader, loader_test)\u001B[0m\n\u001B[0;32m     37\u001B[0m total_loss \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0\u001B[39m\n\u001B[0;32m     38\u001B[0m total_vloss \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0\u001B[39m\n\u001B[1;32m---> 39\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m x, _ \u001B[38;5;129;01min\u001B[39;00m loader:\n\u001B[0;32m     40\u001B[0m     optimizer\u001B[38;5;241m.\u001B[39mzero_grad()\n\u001B[0;32m     41\u001B[0m     x_recon, _ \u001B[38;5;241m=\u001B[39m model(x)\n",
      "File \u001B[1;32m~\\PycharmProjects\\isotherm\\.venv\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:733\u001B[0m, in \u001B[0;36m_BaseDataLoaderIter.__next__\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    730\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_sampler_iter \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    731\u001B[0m     \u001B[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001B[39;00m\n\u001B[0;32m    732\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_reset()  \u001B[38;5;66;03m# type: ignore[call-arg]\u001B[39;00m\n\u001B[1;32m--> 733\u001B[0m data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_next_data\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    734\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_yielded \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[0;32m    735\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m (\n\u001B[0;32m    736\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_dataset_kind \u001B[38;5;241m==\u001B[39m _DatasetKind\u001B[38;5;241m.\u001B[39mIterable\n\u001B[0;32m    737\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_IterableDataset_len_called \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m    738\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_yielded \u001B[38;5;241m>\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_IterableDataset_len_called\n\u001B[0;32m    739\u001B[0m ):\n",
      "File \u001B[1;32m~\\PycharmProjects\\isotherm\\.venv\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:789\u001B[0m, in \u001B[0;36m_SingleProcessDataLoaderIter._next_data\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    787\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21m_next_data\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[0;32m    788\u001B[0m     index \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_next_index()  \u001B[38;5;66;03m# may raise StopIteration\u001B[39;00m\n\u001B[1;32m--> 789\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_dataset_fetcher\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfetch\u001B[49m\u001B[43m(\u001B[49m\u001B[43mindex\u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# may raise StopIteration\u001B[39;00m\n\u001B[0;32m    790\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_pin_memory:\n\u001B[0;32m    791\u001B[0m         data \u001B[38;5;241m=\u001B[39m _utils\u001B[38;5;241m.\u001B[39mpin_memory\u001B[38;5;241m.\u001B[39mpin_memory(data, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_pin_memory_device)\n",
      "File \u001B[1;32m~\\PycharmProjects\\isotherm\\.venv\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:55\u001B[0m, in \u001B[0;36m_MapDatasetFetcher.fetch\u001B[1;34m(self, possibly_batched_index)\u001B[0m\n\u001B[0;32m     53\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m     54\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset[possibly_batched_index]\n\u001B[1;32m---> 55\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcollate_fn\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdata\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\PycharmProjects\\isotherm\\.venv\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:398\u001B[0m, in \u001B[0;36mdefault_collate\u001B[1;34m(batch)\u001B[0m\n\u001B[0;32m    337\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21mdefault_collate\u001B[39m(batch):\n\u001B[0;32m    338\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124mr\u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    339\u001B[0m \u001B[38;5;124;03m    Take in a batch of data and put the elements within the batch into a tensor with an additional outer dimension - batch size.\u001B[39;00m\n\u001B[0;32m    340\u001B[0m \n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    396\u001B[0m \u001B[38;5;124;03m        >>> default_collate(batch)  # Handle `CustomType` automatically\u001B[39;00m\n\u001B[0;32m    397\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m--> 398\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mcollate\u001B[49m\u001B[43m(\u001B[49m\u001B[43mbatch\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcollate_fn_map\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdefault_collate_fn_map\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\PycharmProjects\\isotherm\\.venv\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:211\u001B[0m, in \u001B[0;36mcollate\u001B[1;34m(batch, collate_fn_map)\u001B[0m\n\u001B[0;32m    208\u001B[0m transposed \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlist\u001B[39m(\u001B[38;5;28mzip\u001B[39m(\u001B[38;5;241m*\u001B[39mbatch))  \u001B[38;5;66;03m# It may be accessed twice, so we use a list.\u001B[39;00m\n\u001B[0;32m    210\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(elem, \u001B[38;5;28mtuple\u001B[39m):\n\u001B[1;32m--> 211\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m [\n\u001B[0;32m    212\u001B[0m         collate(samples, collate_fn_map\u001B[38;5;241m=\u001B[39mcollate_fn_map)\n\u001B[0;32m    213\u001B[0m         \u001B[38;5;28;01mfor\u001B[39;00m samples \u001B[38;5;129;01min\u001B[39;00m transposed\n\u001B[0;32m    214\u001B[0m     ]  \u001B[38;5;66;03m# Backwards compatibility.\u001B[39;00m\n\u001B[0;32m    215\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    216\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n",
      "File \u001B[1;32m~\\PycharmProjects\\isotherm\\.venv\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:212\u001B[0m, in \u001B[0;36m<listcomp>\u001B[1;34m(.0)\u001B[0m\n\u001B[0;32m    208\u001B[0m transposed \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlist\u001B[39m(\u001B[38;5;28mzip\u001B[39m(\u001B[38;5;241m*\u001B[39mbatch))  \u001B[38;5;66;03m# It may be accessed twice, so we use a list.\u001B[39;00m\n\u001B[0;32m    210\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(elem, \u001B[38;5;28mtuple\u001B[39m):\n\u001B[0;32m    211\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m [\n\u001B[1;32m--> 212\u001B[0m         \u001B[43mcollate\u001B[49m\u001B[43m(\u001B[49m\u001B[43msamples\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcollate_fn_map\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcollate_fn_map\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    213\u001B[0m         \u001B[38;5;28;01mfor\u001B[39;00m samples \u001B[38;5;129;01min\u001B[39;00m transposed\n\u001B[0;32m    214\u001B[0m     ]  \u001B[38;5;66;03m# Backwards compatibility.\u001B[39;00m\n\u001B[0;32m    215\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    216\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n",
      "File \u001B[1;32m~\\PycharmProjects\\isotherm\\.venv\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:155\u001B[0m, in \u001B[0;36mcollate\u001B[1;34m(batch, collate_fn_map)\u001B[0m\n\u001B[0;32m    153\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m collate_fn_map \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    154\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m elem_type \u001B[38;5;129;01min\u001B[39;00m collate_fn_map:\n\u001B[1;32m--> 155\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mcollate_fn_map\u001B[49m\u001B[43m[\u001B[49m\u001B[43melem_type\u001B[49m\u001B[43m]\u001B[49m\u001B[43m(\u001B[49m\u001B[43mbatch\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcollate_fn_map\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcollate_fn_map\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    157\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m collate_type \u001B[38;5;129;01min\u001B[39;00m collate_fn_map:\n\u001B[0;32m    158\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(elem, collate_type):\n",
      "File \u001B[1;32m~\\PycharmProjects\\isotherm\\.venv\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:272\u001B[0m, in \u001B[0;36mcollate_tensor_fn\u001B[1;34m(batch, collate_fn_map)\u001B[0m\n\u001B[0;32m    270\u001B[0m     storage \u001B[38;5;241m=\u001B[39m elem\u001B[38;5;241m.\u001B[39m_typed_storage()\u001B[38;5;241m.\u001B[39m_new_shared(numel, device\u001B[38;5;241m=\u001B[39melem\u001B[38;5;241m.\u001B[39mdevice)\n\u001B[0;32m    271\u001B[0m     out \u001B[38;5;241m=\u001B[39m elem\u001B[38;5;241m.\u001B[39mnew(storage)\u001B[38;5;241m.\u001B[39mresize_(\u001B[38;5;28mlen\u001B[39m(batch), \u001B[38;5;241m*\u001B[39m\u001B[38;5;28mlist\u001B[39m(elem\u001B[38;5;241m.\u001B[39msize()))\n\u001B[1;32m--> 272\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstack\u001B[49m\u001B[43m(\u001B[49m\u001B[43mbatch\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mout\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mout\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 60
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-10T06:57:20.624025Z",
     "start_time": "2025-07-10T06:57:20.593020Z"
    }
   },
   "cell_type": "code",
   "source": "torch.save(model, \"data/models/torch/autoencoder_exp.pt\")",
   "id": "6e5a9e294045c74c",
   "outputs": [],
   "execution_count": 61
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-10T06:57:21.113112Z",
     "start_time": "2025-07-10T06:57:21.083114Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model = torch.load(\"data/models/torch/autoencoder_exp.pt\", weights_only=False)\n",
    "model.eval()"
   ],
   "id": "f8af0cb897395d7c",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Autoencoder(\n",
       "  (encoder): Sequential(\n",
       "    (0): Linear(in_features=448, out_features=128, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=128, out_features=64, bias=True)\n",
       "    (3): ReLU()\n",
       "    (4): Linear(in_features=64, out_features=16, bias=True)\n",
       "  )\n",
       "  (decoder): Sequential(\n",
       "    (0): Linear(in_features=16, out_features=64, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=64, out_features=128, bias=True)\n",
       "    (3): ReLU()\n",
       "    (4): Linear(in_features=128, out_features=448, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 62
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-10T07:33:48.892751Z",
     "start_time": "2025-07-10T07:33:48.782753Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model.eval()\n",
    "latent_vectors_train = model.encoder(torch.tensor(x_train, dtype=torch.float32).to(device)).detach().cpu().numpy()\n",
    "latent_vectors_test = model.encoder(torch.tensor(x_test, dtype=torch.float32).to(device)).detach().cpu().numpy()\n",
    "latent_vectors_test_exp = model.encoder(torch.tensor(x_test_exp, dtype=torch.float32).to(device)).detach().cpu().numpy()"
   ],
   "id": "59307777a8d5284",
   "outputs": [],
   "execution_count": 108
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-10T07:33:53.684374Z",
     "start_time": "2025-07-10T07:33:53.653377Z"
    }
   },
   "cell_type": "code",
   "source": "decoded = model.decoder(model.encoder(torch.tensor(x_test_exp, dtype=torch.float32).to(device))).detach().cpu().numpy()",
   "id": "a95fe3351e57df09",
   "outputs": [],
   "execution_count": 109
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-10T06:57:27.468493Z",
     "start_time": "2025-07-10T06:57:27.358967Z"
    }
   },
   "cell_type": "code",
   "source": [
    "figure, axis = plt.subplots(3, 3)\n",
    "for i in range(3):\n",
    "    for j in range(3):\n",
    "        k=np.random.randint(0, len(decoded))\n",
    "        axis[i, j].plot(pressures[:-10], x_test_exp[k], marker=\".\", label = \"origin\")\n",
    "        axis[i, j].plot(pressures[:-10], decoded[k], marker=\".\", label = \"decoded\")\n",
    "        axis[i, j].grid(True)\n",
    "axis[i, j].legend()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# k=np.random.randint(0, len(decoded))\n",
    "# plt.plot(pressures[:-10], x_test_exp[k], marker=\".\", label = \"origin\")\n",
    "# plt.plot(pressures[:-10], decoded[k], marker=\".\", label = \"decoded\")\n",
    "# plt.legend()\n",
    "# plt.grid(True)\n",
    "# plt.show()"
   ],
   "id": "dbe5444885056917",
   "outputs": [],
   "execution_count": 65
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-09T09:49:59.729171Z",
     "start_time": "2025-07-09T09:49:59.069100Z"
    }
   },
   "cell_type": "code",
   "source": [
    "np.random.seed(0)\n",
    "labels = None \n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "latent_pca = pca.fit_transform(latent_vectors_train[:100])\n",
    "latent_pca_exp = pca.fit_transform(latent_vectors_test[:100])\n",
    "\n",
    "tsne = TSNE(n_components=2, init='pca', random_state=0)\n",
    "latent_tsne = tsne.fit_transform(latent_vectors_train[:100])\n",
    "latent_tsne_exp = tsne.fit_transform(latent_vectors_test[:100])\n",
    "\n",
    "plt.figure()\n",
    "plt.scatter(latent_pca[:, 0], latent_pca[:, 1], label=\"train\")\n",
    "plt.scatter(latent_pca_exp[:, 0], latent_pca_exp[:, 1], label=\"exp\")\n",
    "plt.title(\"PCA of Latent Space\")\n",
    "plt.xlabel(\"PC1\")\n",
    "plt.ylabel(\"PC2\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "plt.scatter(latent_tsne[:, 0], latent_tsne[:, 1], label=\"train\")\n",
    "plt.scatter(latent_tsne_exp[:, 0], latent_tsne_exp[:, 1], label=\"exp\")\n",
    "for i in range(latent_tsne_exp.shape[0]):\n",
    "        plt.text(latent_tsne_exp[i, 0], latent_tsne_exp[i, 1], str(i), fontsize=8, ha='center', va='center')\n",
    "plt.title(\"t-SNE of Latent Space\")\n",
    "plt.xlabel(\"Dim 1\")\n",
    "plt.ylabel(\"Dim 2\")\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "id": "f578073be61beebe",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\anton\\PycharmProjects\\isotherm\\.venv\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:810: FutureWarning: The default learning rate in TSNE will change from 200.0 to 'auto' in 1.2.\n",
      "  warnings.warn(\n",
      "C:\\Users\\anton\\PycharmProjects\\isotherm\\.venv\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:996: FutureWarning: The PCA initialization in TSNE will change to have the standard deviation of PC1 equal to 1e-4 in 1.2. This will ensure better convergence.\n",
      "  warnings.warn(\n",
      "C:\\Users\\anton\\PycharmProjects\\isotherm\\.venv\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:810: FutureWarning: The default learning rate in TSNE will change from 200.0 to 'auto' in 1.2.\n",
      "  warnings.warn(\n",
      "C:\\Users\\anton\\PycharmProjects\\isotherm\\.venv\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:996: FutureWarning: The PCA initialization in TSNE will change to have the standard deviation of PC1 equal to 1e-4 in 1.2. This will ensure better convergence.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-10T06:58:16.360986Z",
     "start_time": "2025-07-10T06:58:16.347474Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def plot_preds(x, y, preds): \n",
    "    NX, NY = 3, 4\n",
    "    figure, axis = plt.subplots(NX, NY)\n",
    "    for i in range(NX):\n",
    "        for j in range(NY):\n",
    "            k = np.random.randint(0, len(preds)) \n",
    "            iso_axis = axis[i, j].twiny()\n",
    "            iso_axis.set_xlabel(\"P/P$^0$\",fontsize=8)\n",
    "            iso_axis.plot(pressures[:-10], x[k], label=\"Isotherm\", color = 'green')\n",
    "            kernel = (data_sorb.T[:-10])\n",
    "            iso_axis.plot(pressures[:-10], np.dot(kernel, preds[k][:128]), label=\"Isotherm by model\", color=\"red\")\n",
    "            axis[i, j].set_title(f\"№ {k}\")\n",
    "            axis[i, j].title.set_size(10)\n",
    "            axis[i, j].grid()\n",
    "            axis[i, j].set_xlabel(\"nm\",fontsize=8)\n",
    "            axis[i, j].plot(pore_widths, (preds[k]), marker=\".\", label=f\"Model PSD\")\n",
    "            axis[i, j].plot(pore_widths, y[k], marker=\".\", label=\"PSD\")\n",
    "    plt.subplots_adjust(hspace=0.6, right=0.95, left=0.05, bottom=0.05, top=0.9)\n",
    "    plt.legend()\n",
    "    axis[0, 0].legend()\n",
    "    plt.show()"
   ],
   "id": "a07cbe66d234c80f",
   "outputs": [],
   "execution_count": 66
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-11T07:54:34.696430Z",
     "start_time": "2025-06-11T07:54:34.349935Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from tools import model_tester\n",
    "from inverse import fit_linear\n",
    "\n",
    "error_lst, roughness_lst = model_tester.test_model_predictions(preds, x_test_exp, kernel=data_sorb[:, :-10])\n",
    "kde_x, kde_error, kde_fun = model_tester.calculate_kde_data(error_lst, stop=150)\n",
    "print(\"average error:\", np.mean(error_lst))\n",
    "plt.plot(kde_x, kde_error, label=model_name)\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.plot()"
   ],
   "id": "1316a794fbe60afa",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average error: 19.576227837865584\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 56
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-10T06:58:20.037104Z",
     "start_time": "2025-07-10T06:58:20.022105Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class DynamicWeightAveraging:\n",
    "    def __init__(self, num_tasks, T=2.0):\n",
    "        self.num_tasks = num_tasks\n",
    "        self.T = T\n",
    "        self.loss_history = []  # список списков: [ [L1_1, L1_2, ...], [L2_1, L2_2, ...], ... ]\n",
    "\n",
    "    def update_weights(self):\n",
    "        # Требуются как минимум 2 эпохи\n",
    "        if len(self.loss_history[0]) < 2:\n",
    "            return np.ones(self.num_tasks) / self.num_tasks  # равномерные веса\n",
    "\n",
    "        r = []\n",
    "        for i in range(self.num_tasks):\n",
    "            li = self.loss_history[i]\n",
    "            r_i = li[-1] / (li[-2] + 1e-8)\n",
    "            r.append(r_i)\n",
    "\n",
    "        r = np.array(r)\n",
    "        weights = self.T * np.exp(r / self.T)\n",
    "        weights /= weights.sum()\n",
    "        return weights\n",
    "\n",
    "    def append_losses(self, losses):  # losses — список текущих значений потерь [L1, L2, ...]\n",
    "        if not self.loss_history:\n",
    "            self.loss_history = [[] for _ in range(len(losses))]\n",
    "        for i, l in enumerate(losses):\n",
    "            self.loss_history[i].append(l)\n",
    "dwa = DynamicWeightAveraging(num_tasks=2)"
   ],
   "id": "8a3f50c19ae75247",
   "outputs": [],
   "execution_count": 67
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-10T07:37:59.969207Z",
     "start_time": "2025-07-10T07:37:59.919209Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class PSD_model(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(PSD_model, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_dim, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(32, 128),\n",
    "            nn.ReLU(),\n",
    "            # nn.Dropout(0.5),\n",
    "            # nn.Linear(64, 128),\n",
    "            # nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(128, output_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        psd = self.model(x)\n",
    "        return psd\n",
    "\n",
    "class Isotherm_PSD_Dataset(Dataset):\n",
    "    def __init__(self, x, y, original_x, transform=None):\n",
    "        self.x = torch.tensor(x, dtype=torch.float32).to(device)\n",
    "        self.y = torch.tensor(y, dtype=torch.float32).to(device)\n",
    "        self.original_x = torch.tensor(original_x, dtype=torch.float32).to(device)\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = self.x[idx]\n",
    "        y = self.y[idx]\n",
    "        original_x = self.original_x[idx]\n",
    "        if self.transform:\n",
    "            x = self.transform(x)\n",
    "        return x, y, original_x\n",
    "\n",
    "train_PSD = Isotherm_PSD_Dataset(latent_vectors_train, (y_train), x_train)\n",
    "test_PSD = Isotherm_PSD_Dataset(latent_vectors_test, (y_test), x_test)\n",
    "\n",
    "batch_size = 512\n",
    "PSD_loader = DataLoader(train_PSD, batch_size=batch_size, shuffle=True)\n",
    "PSD_loader_test = DataLoader(test_PSD, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "\n",
    "model_PSD = PSD_model(input_dim=latent_dim, output_dim=128)\n",
    "model_PSD.to(device)\n",
    "\n",
    "optimizer = optim.Adam(model_PSD.parameters(), lr=learning_rate)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "\n",
    "with open(\"data/initial kernels/Kernel_Silica_Adsorption.npy\", 'rb') as f:\n",
    "    data_sorb_torch = torch.tensor(np.load(f)[:, :-10])\n",
    "    data_sorb_torch = data_sorb_torch.to(torch.float32)\n",
    "\n",
    "def isoterm_loss(predicted_y, x):\n",
    "    restored_isotherm = torch.matmul(predicted_y, data_sorb_torch)\n",
    "    loss = torch.mean((x - restored_isotherm) ** 2)\n",
    "    return loss\n",
    "\n",
    "def train_PSD_model(model, loader, loader_test):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    total_vloss = 0\n",
    "    for x, y, original_x in loader:\n",
    "        optimizer.zero_grad()\n",
    "        y_recon = model(x)\n",
    "        loss = criterion(y_recon, y)\n",
    "        iso_loss = isoterm_loss(y_recon, original_x)\n",
    "        dwa.append_losses([loss.item(), iso_loss.item()])\n",
    "        weights = dwa.update_weights()\n",
    "        loss = weights[0] * loss + weights[1] * iso_loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for x, y, original_x in loader_test:\n",
    "            y_recon  = model(x)\n",
    "            loss = criterion(y_recon, y)\n",
    "            iso_loss = isoterm_loss(y_recon, original_x)\n",
    "            vloss = weights[0] * loss + weights[1] * iso_loss\n",
    "            total_vloss += vloss.item()\n",
    "\n",
    "    return total_loss / len(loader.dataset), total_vloss / len(loader_test.dataset)\n"
   ],
   "id": "e5e1fac9c7ab87a1",
   "outputs": [],
   "execution_count": 124
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-10T07:39:51.335002Z",
     "start_time": "2025-07-10T07:38:00.749028Z"
    }
   },
   "cell_type": "code",
   "source": [
    "epochs = 100\n",
    "loss_lst = []\n",
    "vloss_lst = []\n",
    "for epoch in range(1, epochs+1):\n",
    "    loss, vloss = train_PSD_model(model_PSD, PSD_loader, PSD_loader_test)\n",
    "    loss_lst.append(loss)\n",
    "    vloss_lst.append(vloss)\n",
    "    if epoch % 1 == 0:\n",
    "        print(f\"Epoch {epoch}/{epochs}, Loss: {loss*100:.8f} Test loss: {vloss*100:.8f}\")"
   ],
   "id": "2fd72aae7361b860",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100, Loss: 0.00178949 Test loss: 0.00074177\n",
      "Epoch 2/100, Loss: 0.00085159 Test loss: 0.00037687\n",
      "Epoch 3/100, Loss: 0.00072723 Test loss: 0.00037235\n",
      "Epoch 4/100, Loss: 0.00066786 Test loss: 0.00038169\n",
      "Epoch 5/100, Loss: 0.00062097 Test loss: 0.00033301\n",
      "Epoch 6/100, Loss: 0.00058336 Test loss: 0.00034946\n",
      "Epoch 7/100, Loss: 0.00055189 Test loss: 0.00034665\n",
      "Epoch 8/100, Loss: 0.00054093 Test loss: 0.00033054\n",
      "Epoch 9/100, Loss: 0.00051792 Test loss: 0.00034681\n",
      "Epoch 10/100, Loss: 0.00051313 Test loss: 0.00031940\n",
      "Epoch 11/100, Loss: 0.00049437 Test loss: 0.00033276\n",
      "Epoch 12/100, Loss: 0.00049415 Test loss: 0.00028986\n",
      "Epoch 13/100, Loss: 0.00047247 Test loss: 0.00030633\n",
      "Epoch 14/100, Loss: 0.00044605 Test loss: 0.00034328\n",
      "Epoch 15/100, Loss: 0.00043887 Test loss: 0.00046527\n",
      "Epoch 16/100, Loss: 0.00046346 Test loss: 0.00032395\n",
      "Epoch 17/100, Loss: 0.00046872 Test loss: 0.00035225\n",
      "Epoch 18/100, Loss: 0.00042937 Test loss: 0.00034018\n",
      "Epoch 19/100, Loss: 0.00042055 Test loss: 0.00032560\n",
      "Epoch 20/100, Loss: 0.00040234 Test loss: 0.00034809\n",
      "Epoch 21/100, Loss: 0.00040625 Test loss: 0.00035077\n",
      "Epoch 22/100, Loss: 0.00037264 Test loss: 0.00034321\n",
      "Epoch 23/100, Loss: 0.00034262 Test loss: 0.00029304\n",
      "Epoch 24/100, Loss: 0.00040005 Test loss: 0.00033268\n",
      "Epoch 25/100, Loss: 0.00038265 Test loss: 0.00033510\n",
      "Epoch 26/100, Loss: 0.00035796 Test loss: 0.00033059\n",
      "Epoch 27/100, Loss: 0.00038500 Test loss: 0.00048714\n",
      "Epoch 28/100, Loss: 0.00033439 Test loss: 0.00036003\n",
      "Epoch 29/100, Loss: 0.00035445 Test loss: 0.00031211\n",
      "Epoch 30/100, Loss: 0.00037143 Test loss: 0.00023906\n",
      "Epoch 31/100, Loss: 0.00035979 Test loss: 0.00031835\n",
      "Epoch 32/100, Loss: 0.00032940 Test loss: 0.00037781\n",
      "Epoch 33/100, Loss: 0.00032080 Test loss: 0.00031591\n",
      "Epoch 34/100, Loss: 0.00035276 Test loss: 0.00030968\n",
      "Epoch 35/100, Loss: 0.00029576 Test loss: 0.00032161\n",
      "Epoch 36/100, Loss: 0.00026048 Test loss: 0.00032346\n",
      "Epoch 37/100, Loss: 0.00040626 Test loss: 0.00032248\n",
      "Epoch 38/100, Loss: 0.00034178 Test loss: 0.00033635\n",
      "Epoch 39/100, Loss: 0.00032336 Test loss: 0.00038236\n",
      "Epoch 40/100, Loss: 0.00033275 Test loss: 0.00045080\n",
      "Epoch 41/100, Loss: 0.00029333 Test loss: 0.00031055\n",
      "Epoch 42/100, Loss: 0.00027951 Test loss: 0.00031848\n",
      "Epoch 43/100, Loss: 0.00029873 Test loss: 0.00036078\n",
      "Epoch 44/100, Loss: 0.00033666 Test loss: 0.00033555\n",
      "Epoch 45/100, Loss: 0.00031066 Test loss: 0.00039088\n",
      "Epoch 46/100, Loss: 0.00032941 Test loss: 0.00035624\n",
      "Epoch 47/100, Loss: 0.00033214 Test loss: 0.00031927\n",
      "Epoch 48/100, Loss: 0.00029790 Test loss: 0.00035575\n",
      "Epoch 49/100, Loss: 0.00027743 Test loss: 0.00031685\n",
      "Epoch 50/100, Loss: 0.00027135 Test loss: 0.00032891\n",
      "Epoch 51/100, Loss: 0.00028960 Test loss: 0.00028753\n",
      "Epoch 52/100, Loss: 0.00026883 Test loss: 0.00034044\n",
      "Epoch 53/100, Loss: 0.00023783 Test loss: 0.00034195\n",
      "Epoch 54/100, Loss: 0.00029162 Test loss: 0.00039205\n",
      "Epoch 55/100, Loss: 0.00025414 Test loss: 0.00039571\n",
      "Epoch 56/100, Loss: 0.00035349 Test loss: 0.00029965\n",
      "Epoch 57/100, Loss: 0.00031225 Test loss: 0.00033289\n",
      "Epoch 58/100, Loss: 0.00024750 Test loss: 0.00037278\n",
      "Epoch 59/100, Loss: 0.00034174 Test loss: 0.00030803\n",
      "Epoch 60/100, Loss: 0.00023198 Test loss: 0.00037748\n",
      "Epoch 61/100, Loss: 0.00030503 Test loss: 0.00031827\n",
      "Epoch 62/100, Loss: 0.00022468 Test loss: 0.00035908\n",
      "Epoch 63/100, Loss: 0.00023600 Test loss: 0.00032478\n",
      "Epoch 64/100, Loss: 0.00028593 Test loss: 0.00029047\n",
      "Epoch 65/100, Loss: 0.00022418 Test loss: 0.00033451\n",
      "Epoch 66/100, Loss: 0.00025653 Test loss: 0.00031171\n",
      "Epoch 67/100, Loss: 0.00025566 Test loss: 0.00033615\n",
      "Epoch 68/100, Loss: 0.00026146 Test loss: 0.00032466\n",
      "Epoch 69/100, Loss: 0.00023992 Test loss: 0.00034533\n",
      "Epoch 70/100, Loss: 0.00023741 Test loss: 0.00032559\n",
      "Epoch 71/100, Loss: 0.00033725 Test loss: 0.00034112\n",
      "Epoch 72/100, Loss: 0.00023767 Test loss: 0.00033597\n",
      "Epoch 73/100, Loss: 0.00023031 Test loss: 0.00030606\n",
      "Epoch 74/100, Loss: 0.00027338 Test loss: 0.00033806\n",
      "Epoch 75/100, Loss: 0.00021672 Test loss: 0.00035398\n",
      "Epoch 76/100, Loss: 0.00022017 Test loss: 0.00029385\n",
      "Epoch 77/100, Loss: 0.00021404 Test loss: 0.00034926\n",
      "Epoch 78/100, Loss: 0.00021089 Test loss: 0.00033708\n",
      "Epoch 79/100, Loss: 0.00022498 Test loss: 0.00032472\n",
      "Epoch 80/100, Loss: 0.00022020 Test loss: 0.00033885\n",
      "Epoch 81/100, Loss: 0.00021965 Test loss: 0.00028417\n",
      "Epoch 82/100, Loss: 0.00022814 Test loss: 0.00031363\n",
      "Epoch 83/100, Loss: 0.00020761 Test loss: 0.00033994\n",
      "Epoch 84/100, Loss: 0.00025361 Test loss: 0.00038757\n",
      "Epoch 85/100, Loss: 0.00023834 Test loss: 0.00033333\n",
      "Epoch 86/100, Loss: 0.00023359 Test loss: 0.00029282\n",
      "Epoch 87/100, Loss: 0.00024617 Test loss: 0.00032361\n",
      "Epoch 88/100, Loss: 0.00023523 Test loss: 0.00035112\n",
      "Epoch 89/100, Loss: 0.00021123 Test loss: 0.00033177\n",
      "Epoch 90/100, Loss: 0.00027033 Test loss: 0.00032508\n",
      "Epoch 91/100, Loss: 0.00025970 Test loss: 0.00031922\n",
      "Epoch 92/100, Loss: 0.00022192 Test loss: 0.00031547\n",
      "Epoch 93/100, Loss: 0.00021458 Test loss: 0.00029207\n",
      "Epoch 94/100, Loss: 0.00020894 Test loss: 0.00031778\n",
      "Epoch 95/100, Loss: 0.00025687 Test loss: 0.00036376\n",
      "Epoch 96/100, Loss: 0.00021582 Test loss: 0.00030906\n",
      "Epoch 97/100, Loss: 0.00020127 Test loss: 0.00032162\n",
      "Epoch 98/100, Loss: 0.00020409 Test loss: 0.00033830\n",
      "Epoch 99/100, Loss: 0.00021151 Test loss: 0.00028227\n",
      "Epoch 100/100, Loss: 0.00025829 Test loss: 0.00031241\n"
     ]
    }
   ],
   "execution_count": 125
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-10T07:40:03.722428Z",
     "start_time": "2025-07-10T07:40:03.676588Z"
    }
   },
   "cell_type": "code",
   "source": [
    "plt.plot(loss_lst)\n",
    "plt.plot(vloss_lst)\n",
    "plt.show()"
   ],
   "id": "826613b63fcbd7f1",
   "outputs": [],
   "execution_count": 126
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-10T07:40:06.630580Z",
     "start_time": "2025-07-10T07:40:06.615581Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model_name = \"autoencoder_regressor_combined\"\n",
    "torch.save(model_PSD, f\"data/models/torch/{model_name}\") "
   ],
   "id": "8b63396d69bd925e",
   "outputs": [],
   "execution_count": 127
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-10T07:40:06.945260Z",
     "start_time": "2025-07-10T07:40:06.916255Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model_name = \"autoencoder_regressor_combined\"\n",
    "model_PSD = torch.load(f\"data/models/torch/{model_name}\", weights_only=False)"
   ],
   "id": "5df22fca0b595bb7",
   "outputs": [],
   "execution_count": 128
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-10T07:40:45.532106Z",
     "start_time": "2025-07-10T07:40:45.485587Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model_PSD.eval()\n",
    "y_train_PSD = model_PSD.model(torch.tensor(latent_vectors_train, dtype=torch.float32).to(device)).detach().cpu().numpy()\n",
    "y_test_PSD = model_PSD.model(torch.tensor(latent_vectors_test, dtype=torch.float32).to(device)).detach().cpu().numpy()\n",
    "y_test_exp_PSD = model_PSD.model(torch.tensor(latent_vectors_test_exp, dtype=torch.float32).to(device)).detach().cpu().numpy()"
   ],
   "id": "bd320d01dab9040b",
   "outputs": [],
   "execution_count": 132
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-10T07:41:34.798999Z",
     "start_time": "2025-07-10T07:41:34.593065Z"
    }
   },
   "cell_type": "code",
   "source": "plot_preds(x_test_exp, y_test_exp, y_test_exp_PSD)",
   "id": "424bd83f3fe2a9f9",
   "outputs": [],
   "execution_count": 135
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-10T07:40:41.166755Z",
     "start_time": "2025-07-10T07:40:41.138746Z"
    }
   },
   "cell_type": "code",
   "source": [
    "np.savez(f\"data/models/metrics/{model_name}\", x=x_test_exp, y=y_test_exp_PSD)\n",
    "model_name\n"
   ],
   "id": "b11eabe3085b3dc5",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'autoencoder_regressor_combined'"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 131
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "89e0e8e7540ebf2e"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
