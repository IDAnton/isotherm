{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-07-10T08:52:35.561094Z",
     "start_time": "2025-07-10T08:52:35.547754Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sympy import false\n",
    "from sympy.abc import alpha\n",
    "from tensorflow.python.ops.linalg.linear_operator_algebra import inverse\n",
    "\n",
    "from datasetLoader import load_dataset\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "from inverse import fit_linear\n",
    "from tools import model_tester\n",
    "from tools.reportParser import find_nearest\n",
    "%matplotlib qt"
   ],
   "outputs": [],
   "execution_count": 33
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-10T08:52:36.210200Z",
     "start_time": "2025-07-10T08:52:36.200451Z"
    }
   },
   "cell_type": "code",
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ],
   "id": "d322819b15356399",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "execution_count": 34
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-10T08:52:41.102013Z",
     "start_time": "2025-07-10T08:52:36.826756Z"
    }
   },
   "cell_type": "code",
   "source": [
    "pore_widths = np.load(\"data/initial kernels/Size_Kernel_Silica_Adsorption.npy\")\n",
    "pressures = np.load(\"data/initial kernels/Pressure_Silica.npy\")\n",
    "with open(\"data/initial kernels/Kernel_Silica_Adsorption.npy\", 'rb') as f:\n",
    "    data_sorb = np.load(f)\n",
    "\n",
    "x, y = load_dataset('data/datasets/silica_random.npz')\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.15, random_state=1)\n",
    "\n",
    "x_exp, y_exp = load_dataset('data/datasets/exp.npz')\n",
    "\n",
    "x_train_exp, x_test_exp, y_train_exp, y_test_exp = train_test_split(x_exp, y_exp, test_size=0.15, random_state=1)"
   ],
   "id": "d29956229295484c",
   "outputs": [],
   "execution_count": 35
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-10T08:49:15.899568Z",
     "start_time": "2025-07-10T08:49:15.806999Z"
    }
   },
   "cell_type": "code",
   "source": [
    "figure, axis = plt.subplots(3, 4)\n",
    "for i in range(3):\n",
    "    for j in range(4):\n",
    "        k = np.random.randint(0, len(x_train_exp))\n",
    "        axis[i, j].plot(pressures[:-10], x_train_exp[k], marker=\".\")\n",
    "        axis[i, j].grid()\n",
    "plt.show()"
   ],
   "id": "56cb2ebf581218f4",
   "outputs": [],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-10T08:49:55.139790Z",
     "start_time": "2025-07-10T08:49:55.043357Z"
    }
   },
   "cell_type": "code",
   "source": [
    "figure, axis = plt.subplots(3, 4)\n",
    "for i in range(3):\n",
    "    for j in range(4):\n",
    "        k = np.random.randint(0, len(y_train_exp))\n",
    "        axis[i, j].plot(pore_widths, y_train_exp[k], marker=\".\")\n",
    "        axis[i, j].grid()\n",
    "plt.show()"
   ],
   "id": "b92642b436f09b6e",
   "outputs": [],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-10T08:50:02.300214Z",
     "start_time": "2025-07-10T08:50:02.227249Z"
    }
   },
   "cell_type": "code",
   "source": [
    "figure, axis = plt.subplots(3, 4)\n",
    "for i in range(3):\n",
    "    for j in range(4):\n",
    "        k = np.random.randint(0, len(x_train))\n",
    "        axis[i, j].plot(pore_widths, y_train[k], marker=\".\")\n",
    "        axis[i, j].grid()\n",
    "plt.show()"
   ],
   "id": "15f9ff24438da3f5",
   "outputs": [],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-09T09:49:44.819944Z",
     "start_time": "2025-07-09T09:49:44.756113Z"
    }
   },
   "cell_type": "code",
   "source": "plt.plot(pore_widths, sum(y_train), marker=\".\")",
   "id": "d204e5d61104e3ed",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x20c1e23cd30>]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-09T09:49:44.913859Z",
     "start_time": "2025-07-09T09:49:44.898572Z"
    }
   },
   "cell_type": "code",
   "source": [
    "i = np.random.randint(0, len(x_train))\n",
    "plt.plot(pressures[:-10], x_train[i], marker=\".\")\n",
    "plt.grid()\n",
    "plt.show()"
   ],
   "id": "3201b1f78762f4ff",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-10T08:52:48.630391Z",
     "start_time": "2025-07-10T08:52:48.019262Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class IsothermDataset(Dataset):\n",
    "    def __init__(self, isotherms, transform=None):\n",
    "        self.data = torch.tensor(isotherms, dtype=torch.float32).to(device)\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = self.data[idx]\n",
    "        if self.transform:\n",
    "            x = self.transform(x)\n",
    "        return x, x\n",
    "\n",
    "x_mixed_train = np.concatenate((x_train_exp, x_train))\n",
    "x_mixed_test = np.concatenate((x_test_exp, x_test))\n",
    "\n",
    "# dataset = IsothermDataset(np.concatenate((x_train_exp, x_train_exp)))\n",
    "# dataset_test = IsothermDataset(np.concatenate((x_test_exp, x_test_exp)))\n",
    "dataset = IsothermDataset(np.concatenate((x_mixed_train, x_mixed_train)))\n",
    "dataset_test = IsothermDataset(np.concatenate((x_mixed_test, x_mixed_test)))\n",
    "\n",
    "\n",
    "batch_size = 512\n",
    "loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "loader_test = DataLoader(dataset_test, batch_size=batch_size, shuffle=False)"
   ],
   "id": "5cefc1b65fc7fa63",
   "outputs": [],
   "execution_count": 36
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-10T08:52:53.616352Z",
     "start_time": "2025-07-10T08:52:53.597947Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self, input_dim, latent_dim):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, latent_dim)\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(latent_dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, input_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        z = self.encoder(x)\n",
    "        x_recon = self.decoder(z)\n",
    "        return x_recon, z\n",
    "\n",
    "input_dim = 448\n",
    "latent_dim = 16\n",
    "epochs = 200\n",
    "learning_rate = 1e-3\n",
    "\n",
    "model = Autoencoder(input_dim=input_dim, latent_dim=latent_dim)\n",
    "model.to(device)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "def train_autoencoder(model, loader, loader_test):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    total_vloss = 0\n",
    "    for x, _ in loader:\n",
    "        optimizer.zero_grad()\n",
    "        x_recon, _ = model(x)\n",
    "        loss = criterion(x_recon, x)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for x, _ in loader_test:\n",
    "            x_recon, _  = model(x)\n",
    "            vloss = criterion(x_recon, x)\n",
    "            total_vloss += vloss.item()\n",
    "\n",
    "    return total_loss / len(loader.dataset), total_vloss / len(loader_test.dataset)\n",
    "\n",
    "\n",
    "# sample_z = model.encoder(torch.tensor(isotherms_np[0], dtype=torch.float32))"
   ],
   "id": "c960463d00766e7c",
   "outputs": [],
   "execution_count": 37
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-10T08:55:31.467563Z",
     "start_time": "2025-07-10T08:52:57.956985Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for epoch in range(1, epochs+1):\n",
    "    loss, vloss = train_autoencoder(model, loader,loader_test)\n",
    "    if epoch % 1 == 0:\n",
    "        print(f\"Epoch {epoch}/{epochs}, Loss: {loss*100:.8f} Test loss: {vloss*100:.8f}\")"
   ],
   "id": "6f625ed77c7b7164",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200, Loss: 0.00295697 Test loss: 0.00021172\n",
      "Epoch 2/200, Loss: 0.00015777 Test loss: 0.00011480\n",
      "Epoch 3/200, Loss: 0.00009958 Test loss: 0.00009277\n",
      "Epoch 4/200, Loss: 0.00007486 Test loss: 0.00006260\n",
      "Epoch 5/200, Loss: 0.00005777 Test loss: 0.00004983\n",
      "Epoch 6/200, Loss: 0.00004702 Test loss: 0.00004064\n",
      "Epoch 7/200, Loss: 0.00003855 Test loss: 0.00004000\n",
      "Epoch 8/200, Loss: 0.00003495 Test loss: 0.00002958\n",
      "Epoch 9/200, Loss: 0.00003093 Test loss: 0.00003215\n",
      "Epoch 10/200, Loss: 0.00002823 Test loss: 0.00002556\n",
      "Epoch 11/200, Loss: 0.00002570 Test loss: 0.00002458\n",
      "Epoch 12/200, Loss: 0.00002357 Test loss: 0.00001912\n",
      "Epoch 13/200, Loss: 0.00002128 Test loss: 0.00002003\n",
      "Epoch 14/200, Loss: 0.00001957 Test loss: 0.00002058\n",
      "Epoch 15/200, Loss: 0.00001914 Test loss: 0.00001791\n",
      "Epoch 16/200, Loss: 0.00001730 Test loss: 0.00001649\n",
      "Epoch 17/200, Loss: 0.00001764 Test loss: 0.00001296\n",
      "Epoch 18/200, Loss: 0.00001698 Test loss: 0.00001385\n",
      "Epoch 19/200, Loss: 0.00001484 Test loss: 0.00001414\n",
      "Epoch 20/200, Loss: 0.00001555 Test loss: 0.00001815\n",
      "Epoch 21/200, Loss: 0.00001544 Test loss: 0.00003022\n",
      "Epoch 22/200, Loss: 0.00001455 Test loss: 0.00001524\n",
      "Epoch 23/200, Loss: 0.00001455 Test loss: 0.00001144\n",
      "Epoch 24/200, Loss: 0.00001444 Test loss: 0.00001039\n",
      "Epoch 25/200, Loss: 0.00001317 Test loss: 0.00001215\n",
      "Epoch 26/200, Loss: 0.00001325 Test loss: 0.00001007\n",
      "Epoch 27/200, Loss: 0.00001294 Test loss: 0.00001081\n",
      "Epoch 28/200, Loss: 0.00001252 Test loss: 0.00001450\n",
      "Epoch 29/200, Loss: 0.00001233 Test loss: 0.00001153\n",
      "Epoch 30/200, Loss: 0.00001232 Test loss: 0.00001026\n",
      "Epoch 31/200, Loss: 0.00001209 Test loss: 0.00000888\n",
      "Epoch 32/200, Loss: 0.00001161 Test loss: 0.00001151\n",
      "Epoch 33/200, Loss: 0.00001151 Test loss: 0.00001314\n",
      "Epoch 34/200, Loss: 0.00001163 Test loss: 0.00000927\n",
      "Epoch 35/200, Loss: 0.00001107 Test loss: 0.00001093\n",
      "Epoch 36/200, Loss: 0.00001122 Test loss: 0.00000893\n",
      "Epoch 37/200, Loss: 0.00001098 Test loss: 0.00000799\n",
      "Epoch 38/200, Loss: 0.00001084 Test loss: 0.00000764\n",
      "Epoch 39/200, Loss: 0.00001038 Test loss: 0.00000755\n",
      "Epoch 40/200, Loss: 0.00001021 Test loss: 0.00000874\n",
      "Epoch 41/200, Loss: 0.00001002 Test loss: 0.00000961\n",
      "Epoch 42/200, Loss: 0.00001046 Test loss: 0.00000752\n",
      "Epoch 43/200, Loss: 0.00000992 Test loss: 0.00000886\n",
      "Epoch 44/200, Loss: 0.00001020 Test loss: 0.00000737\n",
      "Epoch 45/200, Loss: 0.00000973 Test loss: 0.00000785\n",
      "Epoch 46/200, Loss: 0.00000951 Test loss: 0.00000812\n",
      "Epoch 47/200, Loss: 0.00000944 Test loss: 0.00000718\n",
      "Epoch 48/200, Loss: 0.00000943 Test loss: 0.00000782\n",
      "Epoch 49/200, Loss: 0.00000935 Test loss: 0.00000699\n",
      "Epoch 50/200, Loss: 0.00000911 Test loss: 0.00000727\n",
      "Epoch 51/200, Loss: 0.00000870 Test loss: 0.00000641\n",
      "Epoch 52/200, Loss: 0.00000873 Test loss: 0.00000916\n",
      "Epoch 53/200, Loss: 0.00000845 Test loss: 0.00000650\n",
      "Epoch 54/200, Loss: 0.00000841 Test loss: 0.00000704\n",
      "Epoch 55/200, Loss: 0.00000841 Test loss: 0.00000648\n",
      "Epoch 56/200, Loss: 0.00000820 Test loss: 0.00000656\n",
      "Epoch 57/200, Loss: 0.00000840 Test loss: 0.00000565\n",
      "Epoch 58/200, Loss: 0.00000777 Test loss: 0.00000785\n",
      "Epoch 59/200, Loss: 0.00000811 Test loss: 0.00000776\n",
      "Epoch 60/200, Loss: 0.00000797 Test loss: 0.00000666\n",
      "Epoch 61/200, Loss: 0.00000757 Test loss: 0.00000590\n",
      "Epoch 62/200, Loss: 0.00000762 Test loss: 0.00000712\n",
      "Epoch 63/200, Loss: 0.00000757 Test loss: 0.00000683\n",
      "Epoch 64/200, Loss: 0.00000723 Test loss: 0.00000753\n",
      "Epoch 65/200, Loss: 0.00000770 Test loss: 0.00000544\n",
      "Epoch 66/200, Loss: 0.00000738 Test loss: 0.00000548\n",
      "Epoch 67/200, Loss: 0.00000725 Test loss: 0.00000608\n",
      "Epoch 68/200, Loss: 0.00000713 Test loss: 0.00001084\n",
      "Epoch 69/200, Loss: 0.00000713 Test loss: 0.00000487\n",
      "Epoch 70/200, Loss: 0.00000691 Test loss: 0.00000570\n",
      "Epoch 71/200, Loss: 0.00000719 Test loss: 0.00000605\n",
      "Epoch 72/200, Loss: 0.00000698 Test loss: 0.00000640\n",
      "Epoch 73/200, Loss: 0.00000681 Test loss: 0.00000630\n",
      "Epoch 74/200, Loss: 0.00000678 Test loss: 0.00001675\n",
      "Epoch 75/200, Loss: 0.00000681 Test loss: 0.00000537\n",
      "Epoch 76/200, Loss: 0.00000655 Test loss: 0.00000512\n",
      "Epoch 77/200, Loss: 0.00000664 Test loss: 0.00000517\n",
      "Epoch 78/200, Loss: 0.00000679 Test loss: 0.00000648\n",
      "Epoch 79/200, Loss: 0.00000640 Test loss: 0.00000769\n",
      "Epoch 80/200, Loss: 0.00000636 Test loss: 0.00000497\n",
      "Epoch 81/200, Loss: 0.00000624 Test loss: 0.00000576\n",
      "Epoch 82/200, Loss: 0.00000613 Test loss: 0.00000467\n",
      "Epoch 83/200, Loss: 0.00000629 Test loss: 0.00000906\n",
      "Epoch 84/200, Loss: 0.00000612 Test loss: 0.00000465\n",
      "Epoch 85/200, Loss: 0.00000627 Test loss: 0.00000496\n",
      "Epoch 86/200, Loss: 0.00000578 Test loss: 0.00000893\n",
      "Epoch 87/200, Loss: 0.00000600 Test loss: 0.00000587\n",
      "Epoch 88/200, Loss: 0.00000614 Test loss: 0.00000437\n",
      "Epoch 89/200, Loss: 0.00000574 Test loss: 0.00001141\n",
      "Epoch 90/200, Loss: 0.00000577 Test loss: 0.00000406\n",
      "Epoch 91/200, Loss: 0.00000586 Test loss: 0.00000507\n",
      "Epoch 92/200, Loss: 0.00000551 Test loss: 0.00000977\n",
      "Epoch 93/200, Loss: 0.00000570 Test loss: 0.00000410\n",
      "Epoch 94/200, Loss: 0.00000545 Test loss: 0.00000398\n",
      "Epoch 95/200, Loss: 0.00000574 Test loss: 0.00000766\n",
      "Epoch 96/200, Loss: 0.00000530 Test loss: 0.00000655\n",
      "Epoch 97/200, Loss: 0.00000539 Test loss: 0.00000384\n",
      "Epoch 98/200, Loss: 0.00000564 Test loss: 0.00000415\n",
      "Epoch 99/200, Loss: 0.00000561 Test loss: 0.00000377\n",
      "Epoch 100/200, Loss: 0.00000493 Test loss: 0.00000609\n",
      "Epoch 101/200, Loss: 0.00000518 Test loss: 0.00000414\n",
      "Epoch 102/200, Loss: 0.00000520 Test loss: 0.00000649\n",
      "Epoch 103/200, Loss: 0.00000510 Test loss: 0.00000398\n",
      "Epoch 104/200, Loss: 0.00000525 Test loss: 0.00000430\n",
      "Epoch 105/200, Loss: 0.00000496 Test loss: 0.00000720\n",
      "Epoch 106/200, Loss: 0.00000512 Test loss: 0.00000528\n",
      "Epoch 107/200, Loss: 0.00000502 Test loss: 0.00000381\n",
      "Epoch 108/200, Loss: 0.00000482 Test loss: 0.00000479\n",
      "Epoch 109/200, Loss: 0.00000478 Test loss: 0.00000374\n",
      "Epoch 110/200, Loss: 0.00000480 Test loss: 0.00000541\n",
      "Epoch 111/200, Loss: 0.00000461 Test loss: 0.00000461\n",
      "Epoch 112/200, Loss: 0.00000480 Test loss: 0.00001833\n",
      "Epoch 113/200, Loss: 0.00000466 Test loss: 0.00000602\n",
      "Epoch 114/200, Loss: 0.00000486 Test loss: 0.00000793\n",
      "Epoch 115/200, Loss: 0.00000448 Test loss: 0.00000332\n",
      "Epoch 116/200, Loss: 0.00000465 Test loss: 0.00000399\n",
      "Epoch 117/200, Loss: 0.00000439 Test loss: 0.00000355\n",
      "Epoch 118/200, Loss: 0.00000431 Test loss: 0.00000613\n",
      "Epoch 119/200, Loss: 0.00000494 Test loss: 0.00000451\n",
      "Epoch 120/200, Loss: 0.00000420 Test loss: 0.00000379\n",
      "Epoch 121/200, Loss: 0.00000438 Test loss: 0.00000496\n",
      "Epoch 122/200, Loss: 0.00000466 Test loss: 0.00000330\n",
      "Epoch 123/200, Loss: 0.00000398 Test loss: 0.00000345\n",
      "Epoch 124/200, Loss: 0.00000426 Test loss: 0.00000386\n",
      "Epoch 125/200, Loss: 0.00000400 Test loss: 0.00000356\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[38], line 2\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m epoch \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;241m1\u001B[39m, epochs\u001B[38;5;241m+\u001B[39m\u001B[38;5;241m1\u001B[39m):\n\u001B[1;32m----> 2\u001B[0m     loss, vloss \u001B[38;5;241m=\u001B[39m \u001B[43mtrain_autoencoder\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mloader\u001B[49m\u001B[43m,\u001B[49m\u001B[43mloader_test\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m      3\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m epoch \u001B[38;5;241m%\u001B[39m \u001B[38;5;241m1\u001B[39m \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[0;32m      4\u001B[0m         \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mEpoch \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mepoch\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m/\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mepochs\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m, Loss: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mloss\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m100\u001B[39m\u001B[38;5;132;01m:\u001B[39;00m\u001B[38;5;124m.8f\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m Test loss: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mvloss\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m100\u001B[39m\u001B[38;5;132;01m:\u001B[39;00m\u001B[38;5;124m.8f\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n",
      "Cell \u001B[1;32mIn[37], line 43\u001B[0m, in \u001B[0;36mtrain_autoencoder\u001B[1;34m(model, loader, loader_test)\u001B[0m\n\u001B[0;32m     41\u001B[0m x_recon, _ \u001B[38;5;241m=\u001B[39m model(x)\n\u001B[0;32m     42\u001B[0m loss \u001B[38;5;241m=\u001B[39m criterion(x_recon, x)\n\u001B[1;32m---> 43\u001B[0m \u001B[43mloss\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     44\u001B[0m optimizer\u001B[38;5;241m.\u001B[39mstep()\n\u001B[0;32m     45\u001B[0m total_loss \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m loss\u001B[38;5;241m.\u001B[39mitem()\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\py310\\lib\\site-packages\\torch\\_tensor.py:648\u001B[0m, in \u001B[0;36mTensor.backward\u001B[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001B[0m\n\u001B[0;32m    638\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m has_torch_function_unary(\u001B[38;5;28mself\u001B[39m):\n\u001B[0;32m    639\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m handle_torch_function(\n\u001B[0;32m    640\u001B[0m         Tensor\u001B[38;5;241m.\u001B[39mbackward,\n\u001B[0;32m    641\u001B[0m         (\u001B[38;5;28mself\u001B[39m,),\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    646\u001B[0m         inputs\u001B[38;5;241m=\u001B[39minputs,\n\u001B[0;32m    647\u001B[0m     )\n\u001B[1;32m--> 648\u001B[0m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mautograd\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    649\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgradient\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minputs\u001B[49m\n\u001B[0;32m    650\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\py310\\lib\\site-packages\\torch\\autograd\\__init__.py:353\u001B[0m, in \u001B[0;36mbackward\u001B[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001B[0m\n\u001B[0;32m    348\u001B[0m     retain_graph \u001B[38;5;241m=\u001B[39m create_graph\n\u001B[0;32m    350\u001B[0m \u001B[38;5;66;03m# The reason we repeat the same comment below is that\u001B[39;00m\n\u001B[0;32m    351\u001B[0m \u001B[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001B[39;00m\n\u001B[0;32m    352\u001B[0m \u001B[38;5;66;03m# calls in the traceback and some print out the last line\u001B[39;00m\n\u001B[1;32m--> 353\u001B[0m \u001B[43m_engine_run_backward\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    354\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtensors\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    355\u001B[0m \u001B[43m    \u001B[49m\u001B[43mgrad_tensors_\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    356\u001B[0m \u001B[43m    \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    357\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    358\u001B[0m \u001B[43m    \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    359\u001B[0m \u001B[43m    \u001B[49m\u001B[43mallow_unreachable\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[0;32m    360\u001B[0m \u001B[43m    \u001B[49m\u001B[43maccumulate_grad\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[0;32m    361\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\py310\\lib\\site-packages\\torch\\autograd\\graph.py:824\u001B[0m, in \u001B[0;36m_engine_run_backward\u001B[1;34m(t_outputs, *args, **kwargs)\u001B[0m\n\u001B[0;32m    822\u001B[0m     unregister_hooks \u001B[38;5;241m=\u001B[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001B[0;32m    823\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m--> 824\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m Variable\u001B[38;5;241m.\u001B[39m_execution_engine\u001B[38;5;241m.\u001B[39mrun_backward(  \u001B[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001B[39;00m\n\u001B[0;32m    825\u001B[0m         t_outputs, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs\n\u001B[0;32m    826\u001B[0m     )  \u001B[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001B[39;00m\n\u001B[0;32m    827\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[0;32m    828\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m attach_logging_hooks:\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 38
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-10T08:55:37.488845Z",
     "start_time": "2025-07-10T08:55:37.470852Z"
    }
   },
   "cell_type": "code",
   "source": "torch.save(model, \"data/models/torch/autoencoder_exp.pt\")",
   "id": "6e5a9e294045c74c",
   "outputs": [],
   "execution_count": 39
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-10T08:55:37.776910Z",
     "start_time": "2025-07-10T08:55:37.746207Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model = torch.load(\"data/models/torch/autoencoder_exp.pt\", weights_only=False)\n",
    "model.eval()"
   ],
   "id": "f8af0cb897395d7c",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Autoencoder(\n",
       "  (encoder): Sequential(\n",
       "    (0): Linear(in_features=448, out_features=128, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=128, out_features=64, bias=True)\n",
       "    (3): ReLU()\n",
       "    (4): Linear(in_features=64, out_features=16, bias=True)\n",
       "  )\n",
       "  (decoder): Sequential(\n",
       "    (0): Linear(in_features=16, out_features=64, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=64, out_features=128, bias=True)\n",
       "    (3): ReLU()\n",
       "    (4): Linear(in_features=128, out_features=448, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 40
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-10T08:56:25.576221Z",
     "start_time": "2025-07-10T08:56:25.290518Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model.eval()\n",
    "latent_vectors_train = model.encoder(torch.tensor(x_train, dtype=torch.float32).to(device)).detach().cpu().numpy()\n",
    "latent_vectors_test = model.encoder(torch.tensor(x_test, dtype=torch.float32).to(device)).detach().cpu().numpy()\n",
    "latent_vectors_test_exp = model.encoder(torch.tensor(x_test_exp, dtype=torch.float32).to(device)).detach().cpu().numpy()"
   ],
   "id": "59307777a8d5284",
   "outputs": [],
   "execution_count": 44
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-10T08:56:25.623413Z",
     "start_time": "2025-07-10T08:56:25.576221Z"
    }
   },
   "cell_type": "code",
   "source": "decoded = model.decoder(model.encoder(torch.tensor(x_test_exp, dtype=torch.float32).to(device))).detach().cpu().numpy()",
   "id": "a95fe3351e57df09",
   "outputs": [],
   "execution_count": 45
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-10T08:51:11.499708Z",
     "start_time": "2025-07-10T08:51:11.416566Z"
    }
   },
   "cell_type": "code",
   "source": [
    "figure, axis = plt.subplots(3, 3)\n",
    "for i in range(3):\n",
    "    for j in range(3):\n",
    "        k=np.random.randint(0, len(decoded))\n",
    "        axis[i, j].plot(pressures[:-10], x_test_exp[k], marker=\".\", label = \"origin\")\n",
    "        axis[i, j].plot(pressures[:-10], decoded[k], marker=\".\", label = \"decoded\")\n",
    "        axis[i, j].grid(True)\n",
    "axis[i, j].legend()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# k=np.random.randint(0, len(decoded))\n",
    "# plt.plot(pressures[:-10], x_test_exp[k], marker=\".\", label = \"origin\")\n",
    "# plt.plot(pressures[:-10], decoded[k], marker=\".\", label = \"decoded\")\n",
    "# plt.legend()\n",
    "# plt.grid(True)\n",
    "# plt.show()"
   ],
   "id": "dbe5444885056917",
   "outputs": [],
   "execution_count": 29
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-09T09:49:59.729171Z",
     "start_time": "2025-07-09T09:49:59.069100Z"
    }
   },
   "cell_type": "code",
   "source": [
    "np.random.seed(0)\n",
    "labels = None \n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "latent_pca = pca.fit_transform(latent_vectors_train[:100])\n",
    "latent_pca_exp = pca.fit_transform(latent_vectors_test[:100])\n",
    "\n",
    "tsne = TSNE(n_components=2, init='pca', random_state=0)\n",
    "latent_tsne = tsne.fit_transform(latent_vectors_train[:100])\n",
    "latent_tsne_exp = tsne.fit_transform(latent_vectors_test[:100])\n",
    "\n",
    "plt.figure()\n",
    "plt.scatter(latent_pca[:, 0], latent_pca[:, 1], label=\"train\")\n",
    "plt.scatter(latent_pca_exp[:, 0], latent_pca_exp[:, 1], label=\"exp\")\n",
    "plt.title(\"PCA of Latent Space\")\n",
    "plt.xlabel(\"PC1\")\n",
    "plt.ylabel(\"PC2\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "plt.scatter(latent_tsne[:, 0], latent_tsne[:, 1], label=\"train\")\n",
    "plt.scatter(latent_tsne_exp[:, 0], latent_tsne_exp[:, 1], label=\"exp\")\n",
    "for i in range(latent_tsne_exp.shape[0]):\n",
    "        plt.text(latent_tsne_exp[i, 0], latent_tsne_exp[i, 1], str(i), fontsize=8, ha='center', va='center')\n",
    "plt.title(\"t-SNE of Latent Space\")\n",
    "plt.xlabel(\"Dim 1\")\n",
    "plt.ylabel(\"Dim 2\")\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "id": "f578073be61beebe",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\anton\\PycharmProjects\\isotherm\\.venv\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:810: FutureWarning: The default learning rate in TSNE will change from 200.0 to 'auto' in 1.2.\n",
      "  warnings.warn(\n",
      "C:\\Users\\anton\\PycharmProjects\\isotherm\\.venv\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:996: FutureWarning: The PCA initialization in TSNE will change to have the standard deviation of PC1 equal to 1e-4 in 1.2. This will ensure better convergence.\n",
      "  warnings.warn(\n",
      "C:\\Users\\anton\\PycharmProjects\\isotherm\\.venv\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:810: FutureWarning: The default learning rate in TSNE will change from 200.0 to 'auto' in 1.2.\n",
      "  warnings.warn(\n",
      "C:\\Users\\anton\\PycharmProjects\\isotherm\\.venv\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:996: FutureWarning: The PCA initialization in TSNE will change to have the standard deviation of PC1 equal to 1e-4 in 1.2. This will ensure better convergence.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-10T11:51:37.321538Z",
     "start_time": "2025-07-10T11:51:37.289895Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def find_exact_idx_last(array, value):\n",
    "    flag = False\n",
    "    for i in range(len(array)):\n",
    "        if array[i] == value:\n",
    "            flag = True\n",
    "        if flag and array[i] != value:\n",
    "            return i\n",
    "def find_alpha(x_exp):\n",
    "    def triangle_method(log_residuals, log_solution_norms):\n",
    "        points = np.column_stack((log_residuals, log_solution_norms))\n",
    "        A = points[0]\n",
    "        B = points[-1]\n",
    "        AB = B - A\n",
    "        AB_norm = np.linalg.norm(AB)\n",
    "        dist = []\n",
    "        for i in range(0, len(points)):\n",
    "            P = points[i]\n",
    "            AP = P - A\n",
    "            dist.append(np.linalg.norm(np.cross(AB, AP)) / AB_norm)\n",
    "    \n",
    "        return dist\n",
    "    def calculate_roughness(psd, ord=2):\n",
    "        return np.linalg.norm(psd, ord=ord)\n",
    "    \n",
    "    alpha_list = np.logspace(-5, 0, 40)\n",
    "    error_lst = []\n",
    "    roughness_lst = []\n",
    "    restored_isotherms = []\n",
    "    for alpha in alpha_list:\n",
    "        start_idx = find_exact_idx_last(x_exp, 0)\n",
    "        y = fit_linear(x_exp[start_idx:], kernel=data_sorb[:, start_idx:-10], alpha=alpha).x\n",
    "        restored_isotherm = np.dot(y, data_sorb)\n",
    "        restored_isotherms.append(restored_isotherm)\n",
    "        error_lst.append(np.linalg.norm((x_exp[start_idx:] - restored_isotherm[start_idx:-10]), ord=2))\n",
    "        roughness_lst.append(calculate_roughness(y))\n",
    "\n",
    "    dist = triangle_method(np.log(error_lst), np.log(roughness_lst))\n",
    "    alpha = alpha_list[np.argmax(dist)]\n",
    "    return alpha\n",
    "\n",
    "\n",
    "\n",
    "def plot_preds(x, y, preds): \n",
    "    NX, NY = 3, 4\n",
    "    figure, axis = plt.subplots(NX, NY)\n",
    "    for i in range(NX):\n",
    "        for j in range(NY):\n",
    "            \n",
    "            low_p = False\n",
    "            while low_p == False:\n",
    "                k = np.random.randint(0, len(preds))\n",
    "                for l in range(len(x[k])):\n",
    "                    if x[k][l]!=0:\n",
    "                        if pressures[l] < 1e-2:\n",
    "                            low_p = True\n",
    "                        break\n",
    "            \n",
    "            iso_axis = axis[i, j].twiny()\n",
    "            iso_axis.set_xlabel(\"P/P$^0$\",fontsize=8)\n",
    "            iso_axis.plot(pressures[:-10], x[k], label=\"Isotherm\", color = 'green')\n",
    "            kernel = (data_sorb.T[:-10])\n",
    "            iso_axis.plot(pressures[:-10], np.dot(kernel, preds[k][:128]), label=\"Isotherm by model\", color=\"red\")\n",
    "            axis[i, j].set_title(f\"№ {k}\")\n",
    "            axis[i, j].title.set_size(10)\n",
    "            axis[i, j].grid()\n",
    "            axis[i, j].set_xlabel(\"nm\",fontsize=8)\n",
    "            axis[i, j].plot(pore_widths, (preds[k]), marker=\".\", label=f\"Model PSD\")\n",
    "            #axis[i, j].plot(pore_widths, y[k], marker=\".\", label=\"PSD\")\n",
    "            alpha = find_alpha(x[k])\n",
    "            start_idx = find_exact_idx_last(x[k], 0)\n",
    "            L_curve = fit_linear(x[k][start_idx:], kernel=data_sorb[:, start_idx:-10], alpha=alpha).x\n",
    "            axis[i, j].plot(pore_widths, L_curve, marker=\".\", label=\"L_curve\")\n",
    "            iso_axis.plot(pressures[:-10], np.dot(kernel, L_curve), label=\"Isotherm by L_curve\", color=\"yellow\")\n",
    "    plt.subplots_adjust(hspace=0.6, right=0.95, left=0.05, bottom=0.05, top=0.9)\n",
    "    plt.legend()\n",
    "    axis[0, 0].legend()\n",
    "    plt.show()"
   ],
   "id": "a07cbe66d234c80f",
   "outputs": [],
   "execution_count": 136
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-11T07:54:34.696430Z",
     "start_time": "2025-06-11T07:54:34.349935Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from tools import model_tester\n",
    "from inverse import fit_linear\n",
    "\n",
    "error_lst, roughness_lst = model_tester.test_model_predictions(preds, x_test_exp, kernel=data_sorb[:, :-10])\n",
    "kde_x, kde_error, kde_fun = model_tester.calculate_kde_data(error_lst, stop=150)\n",
    "print(\"average error:\", np.mean(error_lst))\n",
    "plt.plot(kde_x, kde_error, label=model_name)\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.plot()"
   ],
   "id": "1316a794fbe60afa",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average error: 19.576227837865584\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 56
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-10T08:51:18.524077Z",
     "start_time": "2025-07-10T08:51:18.508233Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class DynamicWeightAveraging:\n",
    "    def __init__(self, num_tasks, T=2.0):\n",
    "        self.num_tasks = num_tasks\n",
    "        self.T = T\n",
    "        self.loss_history = []  # список списков: [ [L1_1, L1_2, ...], [L2_1, L2_2, ...], ... ]\n",
    "\n",
    "    def update_weights(self):\n",
    "        if len(self.loss_history[0]) < 2:\n",
    "            return np.ones(self.num_tasks) / self.num_tasks\n",
    "\n",
    "        r = []\n",
    "        for i in range(self.num_tasks):\n",
    "            li = self.loss_history[i]\n",
    "            r_i = li[-1] / (li[-2] + 1e-8)\n",
    "            r.append(r_i)\n",
    "\n",
    "        r = np.array(r)\n",
    "        weights = self.T * np.exp(r / self.T)\n",
    "        weights /= weights.sum()\n",
    "        return weights\n",
    "\n",
    "    def append_losses(self, losses):  # losses — список текущих значений потерь [L1, L2, ...]\n",
    "        if not self.loss_history:\n",
    "            self.loss_history = [[] for _ in range(len(losses))]\n",
    "        for i, l in enumerate(losses):\n",
    "            self.loss_history[i].append(l)\n",
    "dwa = DynamicWeightAveraging(num_tasks=2)"
   ],
   "id": "8a3f50c19ae75247",
   "outputs": [],
   "execution_count": 30
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-10T09:02:12.099942Z",
     "start_time": "2025-07-10T09:02:12.084202Z"
    }
   },
   "cell_type": "code",
   "source": "dwa.update_weights()",
   "id": "64fc19dc8691f1e",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.49557976, 0.50442024])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 58
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-10T09:05:53.191119Z",
     "start_time": "2025-07-10T09:05:52.986046Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class PSD_model(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(PSD_model, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_dim, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(32, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(64, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(128, output_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        psd = self.model(x)\n",
    "        return psd\n",
    "\n",
    "class Isotherm_PSD_Dataset(Dataset):\n",
    "    def __init__(self, x, y, original_x, transform=None):\n",
    "        self.x = torch.tensor(x, dtype=torch.float32).to(device)\n",
    "        self.y = torch.tensor(y, dtype=torch.float32).to(device)\n",
    "        self.original_x = torch.tensor(original_x, dtype=torch.float32).to(device)\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = self.x[idx]\n",
    "        y = self.y[idx]\n",
    "        original_x = self.original_x[idx]\n",
    "        if self.transform:\n",
    "            x = self.transform(x)\n",
    "        return x, y, original_x\n",
    "\n",
    "train_PSD = Isotherm_PSD_Dataset(latent_vectors_train, (y_train), x_train)\n",
    "test_PSD = Isotherm_PSD_Dataset(latent_vectors_test, (y_test), x_test)\n",
    "\n",
    "batch_size = 512\n",
    "PSD_loader = DataLoader(train_PSD, batch_size=batch_size, shuffle=True)\n",
    "PSD_loader_test = DataLoader(test_PSD, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "\n",
    "model_PSD = PSD_model(input_dim=latent_dim, output_dim=128)\n",
    "model_PSD.to(device)\n",
    "\n",
    "optimizer = optim.Adam(model_PSD.parameters(), lr=learning_rate)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "\n",
    "with open(\"data/initial kernels/Kernel_Silica_Adsorption.npy\", 'rb') as f:\n",
    "    data_sorb_torch = torch.tensor(np.load(f)[:, :-10])\n",
    "    data_sorb_torch = data_sorb_torch.to(torch.float32).to(device)\n",
    "\n",
    "def isoterm_loss(predicted_y, x):\n",
    "    restored_isotherm = torch.matmul(predicted_y, data_sorb_torch)\n",
    "    loss = torch.mean((x - restored_isotherm) ** 2)\n",
    "    return loss\n",
    "\n",
    "def train_PSD_model(model, loader, loader_test):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    total_vloss = 0\n",
    "    for x, y, original_x in loader:\n",
    "        optimizer.zero_grad()\n",
    "        y_recon = model(x)\n",
    "        loss = criterion(y_recon, y)\n",
    "        iso_loss = isoterm_loss(y_recon, original_x)\n",
    "        dwa.append_losses([loss.item(), iso_loss.item()])\n",
    "        weights = dwa.update_weights()\n",
    "        loss = weights[0] * loss + weights[1] * iso_loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for x, y, original_x in loader_test:\n",
    "            y_recon  = model(x)\n",
    "            loss = criterion(y_recon, y)\n",
    "            iso_loss = isoterm_loss(y_recon, original_x)\n",
    "            vloss = weights[0] * loss + weights[1] * iso_loss\n",
    "            total_vloss += vloss.item()\n",
    "\n",
    "    return total_loss / len(loader.dataset), total_vloss / len(loader_test.dataset)\n"
   ],
   "id": "e5e1fac9c7ab87a1",
   "outputs": [],
   "execution_count": 61
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-10T09:07:20.389214Z",
     "start_time": "2025-07-10T09:05:53.786096Z"
    }
   },
   "cell_type": "code",
   "source": [
    "epochs = 100\n",
    "loss_lst = []\n",
    "vloss_lst = []\n",
    "for epoch in range(1, epochs+1):\n",
    "    loss, vloss = train_PSD_model(model_PSD, PSD_loader, PSD_loader_test)\n",
    "    loss_lst.append(loss)\n",
    "    vloss_lst.append(vloss)\n",
    "    if epoch % 1 == 0:\n",
    "        print(f\"Epoch {epoch}/{epochs}, Loss: {loss*100:.8f} Test loss: {vloss*100:.8f}\")"
   ],
   "id": "2fd72aae7361b860",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100, Loss: 0.00383810 Test loss: 0.00195509\n",
      "Epoch 2/100, Loss: 0.00195923 Test loss: 0.00162222\n",
      "Epoch 3/100, Loss: 0.00167126 Test loss: 0.00141452\n",
      "Epoch 4/100, Loss: 0.00158459 Test loss: 0.00142282\n",
      "Epoch 5/100, Loss: 0.00148877 Test loss: 0.00129145\n",
      "Epoch 6/100, Loss: 0.00139659 Test loss: 0.00116572\n",
      "Epoch 7/100, Loss: 0.00137226 Test loss: 0.00118691\n",
      "Epoch 8/100, Loss: 0.00131694 Test loss: 0.00122773\n",
      "Epoch 9/100, Loss: 0.00131264 Test loss: 0.00101837\n",
      "Epoch 10/100, Loss: 0.00126100 Test loss: 0.00111655\n",
      "Epoch 11/100, Loss: 0.00121896 Test loss: 0.00105156\n",
      "Epoch 12/100, Loss: 0.00120544 Test loss: 0.00109989\n",
      "Epoch 13/100, Loss: 0.00117798 Test loss: 0.00100141\n",
      "Epoch 14/100, Loss: 0.00119106 Test loss: 0.00094200\n",
      "Epoch 15/100, Loss: 0.00113441 Test loss: 0.00100388\n",
      "Epoch 16/100, Loss: 0.00113051 Test loss: 0.00114247\n",
      "Epoch 17/100, Loss: 0.00113529 Test loss: 0.00112608\n",
      "Epoch 18/100, Loss: 0.00111494 Test loss: 0.00107649\n",
      "Epoch 19/100, Loss: 0.00112374 Test loss: 0.00104921\n",
      "Epoch 20/100, Loss: 0.00108585 Test loss: 0.00098769\n",
      "Epoch 21/100, Loss: 0.00105111 Test loss: 0.00105726\n",
      "Epoch 22/100, Loss: 0.00140548 Test loss: 0.00177825\n",
      "Epoch 23/100, Loss: 0.00124393 Test loss: 0.00099842\n",
      "Epoch 24/100, Loss: 0.00117413 Test loss: 0.00095782\n",
      "Epoch 25/100, Loss: 0.00111361 Test loss: 0.00106867\n",
      "Epoch 26/100, Loss: 0.00100517 Test loss: 0.00109455\n",
      "Epoch 27/100, Loss: 0.00106477 Test loss: 0.00104811\n",
      "Epoch 28/100, Loss: 0.00101402 Test loss: 0.00105655\n",
      "Epoch 29/100, Loss: 0.00098587 Test loss: 0.00081714\n",
      "Epoch 30/100, Loss: 0.00106503 Test loss: 0.00093334\n",
      "Epoch 31/100, Loss: 0.00101536 Test loss: 0.00092874\n",
      "Epoch 32/100, Loss: 0.00098215 Test loss: 0.00070913\n",
      "Epoch 33/100, Loss: 0.00099842 Test loss: 0.00095475\n",
      "Epoch 34/100, Loss: 0.00098783 Test loss: 0.00090348\n",
      "Epoch 35/100, Loss: 0.00109947 Test loss: 0.00167074\n",
      "Epoch 36/100, Loss: 0.00108586 Test loss: 0.00095658\n",
      "Epoch 37/100, Loss: 0.00104463 Test loss: 0.00090451\n",
      "Epoch 38/100, Loss: 0.00098537 Test loss: 0.00083732\n",
      "Epoch 39/100, Loss: 0.00100350 Test loss: 0.00084529\n",
      "Epoch 40/100, Loss: 0.00095667 Test loss: 0.00087997\n",
      "Epoch 41/100, Loss: 0.00094790 Test loss: 0.00091886\n",
      "Epoch 42/100, Loss: 0.00093915 Test loss: 0.00086945\n",
      "Epoch 43/100, Loss: 0.00096777 Test loss: 0.00089443\n",
      "Epoch 44/100, Loss: 0.00093559 Test loss: 0.00094289\n",
      "Epoch 45/100, Loss: 0.00094958 Test loss: 0.00087516\n",
      "Epoch 46/100, Loss: 0.00095998 Test loss: 0.00096977\n",
      "Epoch 47/100, Loss: 0.00093621 Test loss: 0.00082830\n",
      "Epoch 48/100, Loss: 0.00088636 Test loss: 0.00085491\n",
      "Epoch 49/100, Loss: 0.00091275 Test loss: 0.00076742\n",
      "Epoch 50/100, Loss: 0.00092238 Test loss: 0.00082324\n",
      "Epoch 51/100, Loss: 0.00093076 Test loss: 0.00089875\n",
      "Epoch 52/100, Loss: 0.00091299 Test loss: 0.00086554\n",
      "Epoch 53/100, Loss: 0.00092923 Test loss: 0.00083387\n",
      "Epoch 54/100, Loss: 0.00087506 Test loss: 0.00084791\n",
      "Epoch 55/100, Loss: 0.00088261 Test loss: 0.00079621\n",
      "Epoch 56/100, Loss: 0.00089576 Test loss: 0.00097144\n",
      "Epoch 57/100, Loss: 0.00089781 Test loss: 0.00091467\n",
      "Epoch 58/100, Loss: 0.00090640 Test loss: 0.00068806\n",
      "Epoch 59/100, Loss: 0.00094179 Test loss: 0.00067249\n",
      "Epoch 60/100, Loss: 0.00089485 Test loss: 0.00082882\n",
      "Epoch 61/100, Loss: 0.00092372 Test loss: 0.00079616\n",
      "Epoch 62/100, Loss: 0.00089586 Test loss: 0.00089067\n",
      "Epoch 63/100, Loss: 0.00085845 Test loss: 0.00089623\n",
      "Epoch 64/100, Loss: 0.00088232 Test loss: 0.00083571\n",
      "Epoch 65/100, Loss: 0.00090477 Test loss: 0.00078992\n",
      "Epoch 66/100, Loss: 0.00092593 Test loss: 0.00077666\n",
      "Epoch 67/100, Loss: 0.00094052 Test loss: 0.00104771\n",
      "Epoch 68/100, Loss: 0.00094741 Test loss: 0.00077212\n",
      "Epoch 69/100, Loss: 0.00091027 Test loss: 0.00084644\n",
      "Epoch 70/100, Loss: 0.00089912 Test loss: 0.00079930\n",
      "Epoch 71/100, Loss: 0.00087422 Test loss: 0.00081803\n",
      "Epoch 72/100, Loss: 0.00089960 Test loss: 0.00087207\n",
      "Epoch 73/100, Loss: 0.00085482 Test loss: 0.00092637\n",
      "Epoch 74/100, Loss: 0.00086142 Test loss: 0.00079086\n",
      "Epoch 75/100, Loss: 0.00084282 Test loss: 0.00086271\n",
      "Epoch 76/100, Loss: 0.00089104 Test loss: 0.00077431\n",
      "Epoch 77/100, Loss: 0.00086871 Test loss: 0.00093052\n",
      "Epoch 78/100, Loss: 0.00085988 Test loss: 0.00089731\n",
      "Epoch 79/100, Loss: 0.00088836 Test loss: 0.00083677\n",
      "Epoch 80/100, Loss: 0.00084539 Test loss: 0.00082190\n",
      "Epoch 81/100, Loss: 0.00086360 Test loss: 0.00085248\n",
      "Epoch 82/100, Loss: 0.00089209 Test loss: 0.00080999\n",
      "Epoch 83/100, Loss: 0.00087241 Test loss: 0.00090191\n",
      "Epoch 84/100, Loss: 0.00082617 Test loss: 0.00088250\n",
      "Epoch 85/100, Loss: 0.00087075 Test loss: 0.00086890\n",
      "Epoch 86/100, Loss: 0.00085216 Test loss: 0.00101079\n",
      "Epoch 87/100, Loss: 0.00084356 Test loss: 0.00083562\n",
      "Epoch 88/100, Loss: 0.00083176 Test loss: 0.00097999\n",
      "Epoch 89/100, Loss: 0.00089503 Test loss: 0.00092606\n",
      "Epoch 90/100, Loss: 0.00087299 Test loss: 0.00078321\n",
      "Epoch 91/100, Loss: 0.00091408 Test loss: 0.00078093\n",
      "Epoch 92/100, Loss: 0.00085027 Test loss: 0.00097483\n",
      "Epoch 93/100, Loss: 0.00088142 Test loss: 0.00094014\n",
      "Epoch 94/100, Loss: 0.00083466 Test loss: 0.00082904\n",
      "Epoch 95/100, Loss: 0.00085688 Test loss: 0.00082087\n",
      "Epoch 96/100, Loss: 0.00086490 Test loss: 0.00090796\n",
      "Epoch 97/100, Loss: 0.00085112 Test loss: 0.00090295\n",
      "Epoch 98/100, Loss: 0.00081937 Test loss: 0.00094311\n",
      "Epoch 99/100, Loss: 0.00080532 Test loss: 0.00097079\n",
      "Epoch 100/100, Loss: 0.00086299 Test loss: 0.00091426\n"
     ]
    }
   ],
   "execution_count": 62
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-10T09:00:12.895585Z",
     "start_time": "2025-07-10T09:00:12.851352Z"
    }
   },
   "cell_type": "code",
   "source": [
    "plt.plot(loss_lst)\n",
    "plt.plot(vloss_lst)\n",
    "plt.show()"
   ],
   "id": "826613b63fcbd7f1",
   "outputs": [],
   "execution_count": 52
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-10T07:40:06.630580Z",
     "start_time": "2025-07-10T07:40:06.615581Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model_name = \"autoencoder_regressor_combined\"\n",
    "torch.save(model_PSD, f\"data/models/torch/{model_name}\") "
   ],
   "id": "8b63396d69bd925e",
   "outputs": [],
   "execution_count": 127
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-10T08:47:49.601637Z",
     "start_time": "2025-07-10T08:47:49.561905Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model_name = \"autoencoder_regressor\"\n",
    "model_PSD = torch.load(f\"data/models/torch/{model_name}\", weights_only=False)"
   ],
   "id": "5df22fca0b595bb7",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-10T09:07:44.819657Z",
     "start_time": "2025-07-10T09:07:44.625531Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model_PSD.eval()\n",
    "y_train_PSD = model_PSD.model(torch.tensor(latent_vectors_train, dtype=torch.float32).to(device)).detach().cpu().numpy()\n",
    "y_test_PSD = model_PSD.model(torch.tensor(latent_vectors_test, dtype=torch.float32).to(device)).detach().cpu().numpy()\n",
    "y_test_exp_PSD = model_PSD.model(torch.tensor(latent_vectors_test_exp, dtype=torch.float32).to(device)).detach().cpu().numpy()"
   ],
   "id": "bd320d01dab9040b",
   "outputs": [],
   "execution_count": 63
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-10T10:54:08.856426Z",
     "start_time": "2025-07-10T10:54:02.182277Z"
    }
   },
   "cell_type": "code",
   "source": "plot_preds(x_test_exp, y_test_exp, y_test_exp_PSD)",
   "id": "424bd83f3fe2a9f9",
   "outputs": [],
   "execution_count": 117
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-10T07:40:41.166755Z",
     "start_time": "2025-07-10T07:40:41.138746Z"
    }
   },
   "cell_type": "code",
   "source": [
    "np.savez(f\"data/models/metrics/{model_name}\", x=x_test_exp, y=y_test_exp_PSD)\n",
    "model_name\n"
   ],
   "id": "b11eabe3085b3dc5",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'autoencoder_regressor_combined'"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 131
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "89e0e8e7540ebf2e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "3b5400f1a2ea29b1"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-10T09:56:27.468085Z",
     "start_time": "2025-07-10T09:56:27.447714Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model2 = \"autoencoder_regressor_combined\"\n",
    "model2_data = np.load(f\"data/models/metrics/{model2}.npz\")\n",
    "model2_x = model2_data[\"x\"]\n",
    "model2_y = model2_data[\"y\"]"
   ],
   "id": "ff4850d3c12aad63",
   "outputs": [],
   "execution_count": 72
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-10T09:57:49.497143Z",
     "start_time": "2025-07-10T09:56:28.082908Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "model2_y_math = [fit_linear(model2_x[i], data_sorb[:, :-10], 0).x for i in range(len(model2_x))]"
   ],
   "id": "d9638b31eec76efc",
   "outputs": [],
   "execution_count": 73
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-10T11:51:47.565063Z",
     "start_time": "2025-07-10T11:51:43.072642Z"
    }
   },
   "cell_type": "code",
   "source": "plot_preds(model2_x, model2_y_math, model2_y)",
   "id": "190a38ad01d9974d",
   "outputs": [],
   "execution_count": 137
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "b57282e80f242f36"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-10T10:22:53.791632Z",
     "start_time": "2025-07-10T10:22:53.759898Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv(\"data/initial kernels/excel/kernel_N2_77K_2koza.csv\")"
   ],
   "id": "7d218eed32a5f654",
   "outputs": [],
   "execution_count": 83
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-10T10:27:01.102818Z",
     "start_time": "2025-07-10T10:27:01.071533Z"
    }
   },
   "cell_type": "code",
   "source": "pressures = data[\"# P/P0\\\\H[nm]\"]",
   "id": "6329324e18f5cd71",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      1.268479e-09\n",
       "1      1.596543e-09\n",
       "2      2.009431e-09\n",
       "3      2.529027e-09\n",
       "4      3.183027e-09\n",
       "           ...     \n",
       "190    9.925630e-01\n",
       "191    9.941513e-01\n",
       "192    9.957396e-01\n",
       "193    9.973292e-01\n",
       "194    9.989173e-01\n",
       "Name: # P/P0\\H[nm], Length: 195, dtype: float64"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 98
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-10T11:14:55.238080Z",
     "start_time": "2025-07-10T11:14:55.227497Z"
    }
   },
   "cell_type": "code",
   "source": [
    "a_array = []\n",
    "for i in (data.columns[1:]).to_numpy():\n",
    "    a_array.append(float(i))\n",
    "a_array = np.array(a_array)"
   ],
   "id": "adba2d4b571ef4d7",
   "outputs": [],
   "execution_count": 126
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-10T11:22:14.248195Z",
     "start_time": "2025-07-10T11:22:14.239260Z"
    }
   },
   "cell_type": "code",
   "source": "new_kernel = data.to_numpy()",
   "id": "16cd3fe05cb482f9",
   "outputs": [],
   "execution_count": 132
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-10T11:22:16.944540Z",
     "start_time": "2025-07-10T11:22:16.928711Z"
    }
   },
   "cell_type": "code",
   "source": [
    "np.save(\"data/initial kernels/new_kernel/pressure.npy\", pressures)\n",
    "np.save(\"data/initial kernels/new_kernel/pore_sizes.npy\", a_array)\n",
    "np.save(\"data/initial kernels/new_kernel/kernel.npy\", new_kernel)"
   ],
   "id": "40dc6c7db073b0e0",
   "outputs": [],
   "execution_count": 133
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-10T11:55:43.132943Z",
     "start_time": "2025-07-10T11:55:43.114867Z"
    }
   },
   "cell_type": "code",
   "source": "data ",
   "id": "b55bbb0915d15841",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "     # P/P0\\H[nm]        0.7     0.7125      0.725     0.7375       0.75  \\\n",
       "0    1.268479e-09   3.952327   3.731933   3.321363   2.863562   2.381306   \n",
       "1    1.596543e-09   4.199571   3.943151   3.553391   3.067191   2.574083   \n",
       "2    2.009431e-09   4.423346   4.192714   3.780905   3.265074   2.716655   \n",
       "3    2.529027e-09   4.691274   4.406331   3.926953   3.426152   2.883507   \n",
       "4    3.183027e-09   4.942917   4.662738   4.235102   3.658560   3.120700   \n",
       "..            ...        ...        ...        ...        ...        ...   \n",
       "190  9.925630e-01  15.390863  14.933137  14.587888  14.248162  13.888123   \n",
       "191  9.941513e-01  15.379747  14.929881  14.587508  14.257587  13.900280   \n",
       "192  9.957396e-01  15.391051  14.943118  14.597172  14.266703  13.898679   \n",
       "193  9.973292e-01  15.399291  14.961349  14.586886  14.267135  13.899183   \n",
       "194  9.989173e-01  15.399970  14.958560  14.574360  14.264961  13.893692   \n",
       "\n",
       "        0.7625      0.775     0.7875        0.8  ...       48.0       50.0  \\\n",
       "0     1.914137   1.479571   1.141257   0.876086  ...   0.000332   0.000319   \n",
       "1     2.027395   1.598272   1.233990   0.946521  ...   0.000383   0.000367   \n",
       "2     2.194424   1.744386   1.338909   1.023296  ...   0.000482   0.000463   \n",
       "3     2.367929   1.866774   1.438704   1.099539  ...   0.000562   0.000539   \n",
       "4     2.524314   2.017452   1.560537   1.210825  ...   0.000677   0.000650   \n",
       "..         ...        ...        ...        ...  ...        ...        ...   \n",
       "190  13.531749  13.305604  12.982052  12.763135  ...  17.311887  17.311887   \n",
       "191  13.541826  13.308271  13.003020  12.756473  ...  17.295862  17.295862   \n",
       "192  13.542538  13.311109  12.992426  12.759832  ...  17.294825  17.294825   \n",
       "193  13.522256  13.298693  13.010123  12.778168  ...  17.309949  17.309949   \n",
       "194  13.529513  13.316876  12.998196  12.775074  ...  17.311599  17.311599   \n",
       "\n",
       "          55.0       60.0       65.0       70.0       75.0       80.0  \\\n",
       "0     0.000290   0.000266   0.000245   0.000228   0.000212   0.000199   \n",
       "1     0.000334   0.000306   0.000282   0.000262   0.000245   0.000229   \n",
       "2     0.000420   0.000385   0.000356   0.000330   0.000308   0.000289   \n",
       "3     0.000490   0.000449   0.000414   0.000385   0.000359   0.000336   \n",
       "4     0.000591   0.000541   0.000500   0.000464   0.000433   0.000406   \n",
       "..         ...        ...        ...        ...        ...        ...   \n",
       "190  17.311887  17.311887  17.311887  17.311887  17.311887  17.311887   \n",
       "191  17.295862  17.295862  17.295862  17.295862  17.295862  17.295862   \n",
       "192  17.294825  17.294825  17.294825  17.294825  17.294825  17.294825   \n",
       "193  17.309949  17.309949  17.309949  17.309949  17.309949  17.309949   \n",
       "194  17.311599  17.311599  17.311599  17.311599  17.311599  17.311599   \n",
       "\n",
       "          90.0      100.0  \n",
       "0     0.000177   0.000159  \n",
       "1     0.000204   0.000183  \n",
       "2     0.000257   0.000231  \n",
       "3     0.000299   0.000269  \n",
       "4     0.000360   0.000324  \n",
       "..         ...        ...  \n",
       "190  17.311887  17.311887  \n",
       "191  17.295862  17.295862  \n",
       "192  17.294825  17.294825  \n",
       "193  17.309949  17.309949  \n",
       "194  17.311599  17.311599  \n",
       "\n",
       "[195 rows x 118 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th># P/P0\\H[nm]</th>\n",
       "      <th>0.7</th>\n",
       "      <th>0.7125</th>\n",
       "      <th>0.725</th>\n",
       "      <th>0.7375</th>\n",
       "      <th>0.75</th>\n",
       "      <th>0.7625</th>\n",
       "      <th>0.775</th>\n",
       "      <th>0.7875</th>\n",
       "      <th>0.8</th>\n",
       "      <th>...</th>\n",
       "      <th>48.0</th>\n",
       "      <th>50.0</th>\n",
       "      <th>55.0</th>\n",
       "      <th>60.0</th>\n",
       "      <th>65.0</th>\n",
       "      <th>70.0</th>\n",
       "      <th>75.0</th>\n",
       "      <th>80.0</th>\n",
       "      <th>90.0</th>\n",
       "      <th>100.0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.268479e-09</td>\n",
       "      <td>3.952327</td>\n",
       "      <td>3.731933</td>\n",
       "      <td>3.321363</td>\n",
       "      <td>2.863562</td>\n",
       "      <td>2.381306</td>\n",
       "      <td>1.914137</td>\n",
       "      <td>1.479571</td>\n",
       "      <td>1.141257</td>\n",
       "      <td>0.876086</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000332</td>\n",
       "      <td>0.000319</td>\n",
       "      <td>0.000290</td>\n",
       "      <td>0.000266</td>\n",
       "      <td>0.000245</td>\n",
       "      <td>0.000228</td>\n",
       "      <td>0.000212</td>\n",
       "      <td>0.000199</td>\n",
       "      <td>0.000177</td>\n",
       "      <td>0.000159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.596543e-09</td>\n",
       "      <td>4.199571</td>\n",
       "      <td>3.943151</td>\n",
       "      <td>3.553391</td>\n",
       "      <td>3.067191</td>\n",
       "      <td>2.574083</td>\n",
       "      <td>2.027395</td>\n",
       "      <td>1.598272</td>\n",
       "      <td>1.233990</td>\n",
       "      <td>0.946521</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000383</td>\n",
       "      <td>0.000367</td>\n",
       "      <td>0.000334</td>\n",
       "      <td>0.000306</td>\n",
       "      <td>0.000282</td>\n",
       "      <td>0.000262</td>\n",
       "      <td>0.000245</td>\n",
       "      <td>0.000229</td>\n",
       "      <td>0.000204</td>\n",
       "      <td>0.000183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.009431e-09</td>\n",
       "      <td>4.423346</td>\n",
       "      <td>4.192714</td>\n",
       "      <td>3.780905</td>\n",
       "      <td>3.265074</td>\n",
       "      <td>2.716655</td>\n",
       "      <td>2.194424</td>\n",
       "      <td>1.744386</td>\n",
       "      <td>1.338909</td>\n",
       "      <td>1.023296</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000482</td>\n",
       "      <td>0.000463</td>\n",
       "      <td>0.000420</td>\n",
       "      <td>0.000385</td>\n",
       "      <td>0.000356</td>\n",
       "      <td>0.000330</td>\n",
       "      <td>0.000308</td>\n",
       "      <td>0.000289</td>\n",
       "      <td>0.000257</td>\n",
       "      <td>0.000231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.529027e-09</td>\n",
       "      <td>4.691274</td>\n",
       "      <td>4.406331</td>\n",
       "      <td>3.926953</td>\n",
       "      <td>3.426152</td>\n",
       "      <td>2.883507</td>\n",
       "      <td>2.367929</td>\n",
       "      <td>1.866774</td>\n",
       "      <td>1.438704</td>\n",
       "      <td>1.099539</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000562</td>\n",
       "      <td>0.000539</td>\n",
       "      <td>0.000490</td>\n",
       "      <td>0.000449</td>\n",
       "      <td>0.000414</td>\n",
       "      <td>0.000385</td>\n",
       "      <td>0.000359</td>\n",
       "      <td>0.000336</td>\n",
       "      <td>0.000299</td>\n",
       "      <td>0.000269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3.183027e-09</td>\n",
       "      <td>4.942917</td>\n",
       "      <td>4.662738</td>\n",
       "      <td>4.235102</td>\n",
       "      <td>3.658560</td>\n",
       "      <td>3.120700</td>\n",
       "      <td>2.524314</td>\n",
       "      <td>2.017452</td>\n",
       "      <td>1.560537</td>\n",
       "      <td>1.210825</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000677</td>\n",
       "      <td>0.000650</td>\n",
       "      <td>0.000591</td>\n",
       "      <td>0.000541</td>\n",
       "      <td>0.000500</td>\n",
       "      <td>0.000464</td>\n",
       "      <td>0.000433</td>\n",
       "      <td>0.000406</td>\n",
       "      <td>0.000360</td>\n",
       "      <td>0.000324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190</th>\n",
       "      <td>9.925630e-01</td>\n",
       "      <td>15.390863</td>\n",
       "      <td>14.933137</td>\n",
       "      <td>14.587888</td>\n",
       "      <td>14.248162</td>\n",
       "      <td>13.888123</td>\n",
       "      <td>13.531749</td>\n",
       "      <td>13.305604</td>\n",
       "      <td>12.982052</td>\n",
       "      <td>12.763135</td>\n",
       "      <td>...</td>\n",
       "      <td>17.311887</td>\n",
       "      <td>17.311887</td>\n",
       "      <td>17.311887</td>\n",
       "      <td>17.311887</td>\n",
       "      <td>17.311887</td>\n",
       "      <td>17.311887</td>\n",
       "      <td>17.311887</td>\n",
       "      <td>17.311887</td>\n",
       "      <td>17.311887</td>\n",
       "      <td>17.311887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>191</th>\n",
       "      <td>9.941513e-01</td>\n",
       "      <td>15.379747</td>\n",
       "      <td>14.929881</td>\n",
       "      <td>14.587508</td>\n",
       "      <td>14.257587</td>\n",
       "      <td>13.900280</td>\n",
       "      <td>13.541826</td>\n",
       "      <td>13.308271</td>\n",
       "      <td>13.003020</td>\n",
       "      <td>12.756473</td>\n",
       "      <td>...</td>\n",
       "      <td>17.295862</td>\n",
       "      <td>17.295862</td>\n",
       "      <td>17.295862</td>\n",
       "      <td>17.295862</td>\n",
       "      <td>17.295862</td>\n",
       "      <td>17.295862</td>\n",
       "      <td>17.295862</td>\n",
       "      <td>17.295862</td>\n",
       "      <td>17.295862</td>\n",
       "      <td>17.295862</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192</th>\n",
       "      <td>9.957396e-01</td>\n",
       "      <td>15.391051</td>\n",
       "      <td>14.943118</td>\n",
       "      <td>14.597172</td>\n",
       "      <td>14.266703</td>\n",
       "      <td>13.898679</td>\n",
       "      <td>13.542538</td>\n",
       "      <td>13.311109</td>\n",
       "      <td>12.992426</td>\n",
       "      <td>12.759832</td>\n",
       "      <td>...</td>\n",
       "      <td>17.294825</td>\n",
       "      <td>17.294825</td>\n",
       "      <td>17.294825</td>\n",
       "      <td>17.294825</td>\n",
       "      <td>17.294825</td>\n",
       "      <td>17.294825</td>\n",
       "      <td>17.294825</td>\n",
       "      <td>17.294825</td>\n",
       "      <td>17.294825</td>\n",
       "      <td>17.294825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193</th>\n",
       "      <td>9.973292e-01</td>\n",
       "      <td>15.399291</td>\n",
       "      <td>14.961349</td>\n",
       "      <td>14.586886</td>\n",
       "      <td>14.267135</td>\n",
       "      <td>13.899183</td>\n",
       "      <td>13.522256</td>\n",
       "      <td>13.298693</td>\n",
       "      <td>13.010123</td>\n",
       "      <td>12.778168</td>\n",
       "      <td>...</td>\n",
       "      <td>17.309949</td>\n",
       "      <td>17.309949</td>\n",
       "      <td>17.309949</td>\n",
       "      <td>17.309949</td>\n",
       "      <td>17.309949</td>\n",
       "      <td>17.309949</td>\n",
       "      <td>17.309949</td>\n",
       "      <td>17.309949</td>\n",
       "      <td>17.309949</td>\n",
       "      <td>17.309949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>194</th>\n",
       "      <td>9.989173e-01</td>\n",
       "      <td>15.399970</td>\n",
       "      <td>14.958560</td>\n",
       "      <td>14.574360</td>\n",
       "      <td>14.264961</td>\n",
       "      <td>13.893692</td>\n",
       "      <td>13.529513</td>\n",
       "      <td>13.316876</td>\n",
       "      <td>12.998196</td>\n",
       "      <td>12.775074</td>\n",
       "      <td>...</td>\n",
       "      <td>17.311599</td>\n",
       "      <td>17.311599</td>\n",
       "      <td>17.311599</td>\n",
       "      <td>17.311599</td>\n",
       "      <td>17.311599</td>\n",
       "      <td>17.311599</td>\n",
       "      <td>17.311599</td>\n",
       "      <td>17.311599</td>\n",
       "      <td>17.311599</td>\n",
       "      <td>17.311599</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>195 rows × 118 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 138
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "46e86efa1df2af42"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
